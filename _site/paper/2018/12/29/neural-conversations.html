<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Neural approaches to conversational AI | Neal Lathia</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Neural approaches to conversational AI" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a a six chapter review! It alternates between giving overview-level information for specific areas of conversational AI, and diving into details of deep or reinforcement-based learning algorithms as applied to conversational settings." />
<meta property="og:description" content="This is a a six chapter review! It alternates between giving overview-level information for specific areas of conversational AI, and diving into details of deep or reinforcement-based learning algorithms as applied to conversational settings." />
<link rel="canonical" href="http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html" />
<meta property="og:url" content="http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html" />
<meta property="og:site_name" content="Neal Lathia" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-29T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"This is a a six chapter review! It alternates between giving overview-level information for specific areas of conversational AI, and diving into details of deep or reinforcement-based learning algorithms as applied to conversational settings.","headline":"Neural approaches to conversational AI","dateModified":"2018-12-29T00:00:00-06:00","datePublished":"2018-12-29T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html"},"url":"http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="Neal Lathia" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Neural approaches to conversational AI | Neal Lathia</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Neural approaches to conversational AI" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a a six chapter review! It alternates between giving overview-level information for specific areas of conversational AI, and diving into details of deep or reinforcement-based learning algorithms as applied to conversational settings." />
<meta property="og:description" content="This is a a six chapter review! It alternates between giving overview-level information for specific areas of conversational AI, and diving into details of deep or reinforcement-based learning algorithms as applied to conversational settings." />
<link rel="canonical" href="http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html" />
<meta property="og:url" content="http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html" />
<meta property="og:site_name" content="Neal Lathia" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-29T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"This is a a six chapter review! It alternates between giving overview-level information for specific areas of conversational AI, and diving into details of deep or reinforcement-based learning algorithms as applied to conversational settings.","headline":"Neural approaches to conversational AI","dateModified":"2018-12-29T00:00:00-06:00","datePublished":"2018-12-29T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html"},"url":"http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="Neal Lathia" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Neal Lathia</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/research/">Research</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural approaches to conversational AI</h1><p class="page-description">This is a a six chapter review! It alternates between giving overview-level information for specific areas of conversational AI, and diving into details of deep or reinforcement-based learning algorithms as applied to conversational settings.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-12-29T00:00:00-06:00" itemprop="datePublished">
        Dec 29, 2018
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#paper">paper</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Available: <a href="https://arxiv.org/pdf/1809.08267.pdf">https://arxiv.org/pdf/1809.08267.pdf</a></p>

<p>The stand-out points (for me) were about problem formulation (e.g., single vs multi turn dialog) and that nearly all sections discussed that robust evaluation is a core problem in dialog systems: we don’t have generalised, automated, scalable, robust ways to determine that a conversation with a computer has gone well.</p>

<h2 id="chapter-1-introduction">Chapter 1. Introduction</h2>

<p>Intelligent dialogue systems are on the rise thanks to breakthroughs in deep and reinforcement learning. The research field is growing too, attracting folk from the NLP, IR, and ML communities. A number of tutorials and surveys have been published - this chapter is another survey, based on tutorials given at SIGIR and ACL 2018 - that hopes to provide a unified view of conversational / dialog systems.</p>

<p>The kinds of problems that dialog systems are expected to solve include:</p>

<ul>
  <li><strong>Question answering</strong>: providing a concise, direct answer to user queries based on rich knowledge drawn from various data sources.</li>
  <li><strong>Task completion</strong>: accomplishing a task for the user (e.g., restaurant reservation, business trip planning).</li>
  <li><strong>Social chat</strong>: conversing seamlessly with the user and providing useful recommendations.</li>
</ul>

<p>Bots can be grouped into two categories: “task oriented” and “chitchat.” All of the personal assistants on the market today (e.g., Alexa) are task oriented and handle simple, one off tasks (e.g., reporting the weather). They are typically composed of four modules: (1) Natural Language Understanding, (2) state tracking, (3) dialog policy, (4) Natural Language Generation – although there is now a trend to develop all four of these together using a neural network. Most of these will have access to external databases that supporting completing the task.</p>

<p>An example of a chat-oriented bot is XiaoIce - there’s an overview of it in the recent <a href="https://jack-clark.net/2018/12/25/import-ai-126-what-makes-microsofts-biggest-chatbot-work-europe-tries-to-craft-ai-ethics-and-why-you-should-take-ai-risk-seriously/">Import AI newsletter</a>. Their primary goal is often to be “companions to humans with an emotional connection.”</p>

<p>The unified view casts dialog as an <em>optimal (hierarchical) decision making</em> process. The top level is about picking the right agent (e.g., question answering vs. meeting scheduling), the lower level is about picking the right action. These can be thought of as options over a Markov Decision Process, which fits well with reinforcement learning.</p>

<p>A common metric to use as a reward function is CPS: <strong>Conversation-turns Per Session</strong>. There are two views on this metric: it sometimes needs to be minimised (complete a task quickly) and other times needs to be maximised (engage in long/meaningful conversations). The can be combined by reasoning over multiple sessions: being able to complete a task quickly (small short-term CPS) means that users will find the dialog system useful and will use it often (large long-term CPS).</p>

<p>Neural-based NLP tasks are usually performed in three steps: (1) encoding user input (words) into vectors, (2) reasoning in the vector space to generate a response, (3) decoding the response into a symbolic space (back to words). End-to-end training of neural nets results in a tighter coupling between the architecture and the application – the focus shifts away from having the right components for language processing (e.g., parsing, context reasoning, etc.) towards having the right architecture.</p>

<h2 id="chapter-2-machine-learning-background">Chapter 2. Machine Learning Background</h2>

<p>This chapter is an overview of ML. I’m only going to write notes about bits that are specific to conversational systems.</p>

<p>Widely used neural network layers for text classification:</p>

<ul>
  <li>Embedding layers: mapping each word to an m-dimensional vector.</li>
  <li>Fully connected layers: performing linear projections</li>
  <li>Convolutional-pooling layers: forms a local feature vector of a word and its adjacent words (within a fixed sized window).</li>
  <li>Recurrent layers: map the text to a dense/low dimensional vector by sequentially and recurrently processing each word.</li>
</ul>

<p>Two examples: <a href="https://www.microsoft.com/en-us/research/project/dssm/">Deep Structured Semantic Model</a> and <a href="https://google.github.io/seq2seq/">Seq2Seq</a>.</p>

<h2 id="chapter-3-question-answering-and-machine-reading-comprehension">Chapter 3. Question Answering and Machine Reading Comprehension</h2>

<p>Question Answering (QA) allows a user to query a large scale knowledge base/document collection in natural language. They differ from search engines in that they aim to provide a concise and direct answer rather than a ranked list of results. QA is split into single-turn and multi-turn, with the latter being an emerging research topic.</p>

<p>Knowledge base (KB) question answering has been developed based on symbolic approaches – which has been hard to scale. KBs are sometimes known as knowledge graphs - entities are nodes and relations are directed edges between them. Finding an answer to a question is based on semantic parsing: mapping a question to a “meaningful” representation, finding the paths in the graph that match the query, and then retrieving the nodes. This approach has two challenges:</p>
<ul>
  <li>Paraphrasing: there are many ways to ask the same question. Embeddings have been a useful way to tackle this.</li>
  <li>Search complexity: number of candidate paths grows exponentially with the path length.</li>
</ul>

<p>Embeddings map entities and relations to continuous vectors in a (neural) space: this space can be viewed as a hidden semantic space where various expressions with the same semantic meaning map to the same vector. There are various ways (e.g., ReasoNet using DL and DeepPath using RL) to perform multi-step knowledge based reasoning using these vectors.</p>

<p>For multi-turn QA, users need to interactively compose (complex) queries, where the agent can ask clarifying questions. The Entropy Minimization Dialogue Management strategy picks the next question to ask based on the attribute that has the most entry in the remaining KB entries.</p>

<p>Machine Reading Comprehension is the task of answering questions given a set of text passages. Answers are defined as a span of text in (one of the) passage(s). There are many datasets (e.g., SQuAD) that are fueling research in this area. As with approaches above, this is usually done in 3 steps: encoding, reasoning, and decoding.</p>
<ul>
  <li>Encoding: Popular pre-trained embeddings include word2vec and GloVe, and can be enhanced with part of speech tags. Contextual embeddings (e.g., ELMo and BERT) uses contextual cues from surrounding words to refine the embeddings.</li>
  <li>Reasoning: can be done in a single-step (match the question and document only once) or multi-step.</li>
</ul>

<p>Many of these systems are developed as single-turn. To make this conversational, the system also needs to include (encode) the conversation history.</p>

<h2 id="chapter-4-task-oriented-dialogue-systems">Chapter 4. Task-oriented Dialogue Systems</h2>

<p>Completing a task is different from question answering – they are typically domain dependent (e.g., book a hotel room). There are a variety of types, including slot-filling dialogue: where the machine needs to collect information from the user in order to complete the task.</p>

<p>The interaction between a dialog agent and a user mirrors the interaction between an RL agent and the environment. An appropriate reward function in this domain would give a high reward for completing the task and a small penalty for each required intermediate turn. In practice, reward functions are often a linear combination of a subset of the following:</p>
<ul>
  <li>Task completion success</li>
  <li>Time elapsed (number of turns)</li>
  <li>Coherence, diversity, personal style(s)</li>
</ul>

<p>For RL, researchers have developed user simulators for RL agents to converse with – since conversing with real users is expensive. The simulators could be based on a user agenda or based on models/data. Ultimately, many evaluations rely on recruited users (small scale) or A/B tests.</p>

<p>Slot filling problems also rely on dialogue state tracking – containing all information about what the user is looking for at the current turn of the conversation. This is the input information to the dialogue policy which decides what action to take next. For example, see <a href="https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/">this challenge</a>.</p>

<p>An example dialogue policy is DQN, which takes as input an encoding of the current dialogue state. It outputs a vector who entries correspond to all the possible dialogue-act/slot pairs that can be chosen by the system. Learning the policy from scratch takes a lot of data, but can be sped up by using expert-generated dialogues, imitation learning, or replay buffer spiking (warm start). In online settings, an agent has to balance between exploit (maximise reward) and explore (discover better alternatives) – exploration when neural nets are used is an active research topic. This is made even more challenging when needing to extend the agent’s domain while serving users (adding more intents/slots).</p>

<p>Dialog can often be decomposed into a sequence of related sub-dialogues that each focus on a single topic. For example: booking a flight, hotel and car rental for a single trip. These are referred to as composite-task dialogues, where subtasks may have interdependencies. On the other hand, if a dialog can have more than one broad subject matter (e.g., hotels and restaurants), this is referred to as multi-domain dialogue.</p>

<p>Natural Language Generation (NLG) aims to convert a communication goal into natural language form. Rule and template based approaches are the most common ways to achieve this. Newer approaches (corpus-based) aim to optimise generation from a corpora of text. There are a number of LSTM based approaches that have been explored here.</p>

<p>All of these modules in a dialog system are often optimised separately. If all of them are differentiable, then the whole system can be optimised by backprop. Supervised and reinforcement learning have both been applied to this scenario.</p>

<p>Beyond slot-filling dialogue:</p>
<ul>
  <li>Information seeking dialog</li>
  <li>Navigation dialogue</li>
  <li>Multimodal dialogue with non-verbal inputs</li>
  <li>Mixed initiative and negotiation</li>
  <li>Multiple-parties</li>
</ul>

<h2 id="chapter-5-fully-data-driven-conversation-models-and-social-bots">Chapter 5. Fully Data-Driven Conversation Models and Social Bots</h2>

<p>End to end systems do not rely on expert knowledge and do not have the traditional components described in the previous chapter. These are used more often for chit chat scenarios rather than task completion. Many of these systems are inspired by statistical machine translation.</p>

<p>Early systems represented the data as query/response pairs, which limited the context of the responses. Researchers have since explored RNN/LSTM based approaches that exploited a longer context. Other approaches such as the Hierarchical Recurrent Encoder-Decoder (HRED) aim to exploit longer-term context. Attention based models allow for conditioning on parts of the sentence that are relevant for prediction.</p>

<p>Challenges in these systems:</p>
<ul>
  <li>Response blandness - responses are often bland, deflective, uninformative (e.g. “I don’t know”). A variety of methods, including GANs, have been explored to increase the diversity of responses.</li>
  <li>Speaker consistency - responses are incoherent, where the system contradicts itself. This is often a result of a one-to-many of examples in training data (e.g., there are many answers to “how old are you?”).</li>
  <li>Word repetitions - since it is not clear how often a specific word or phrase should be repeated in the output.</li>
  <li>Response appropriateness - struggling to produce names and facts that has appropriate semantic content.</li>
</ul>

<p>Grounded conversation models aim to help to effectively converse about a user’s environment, by factoring in additional information (e.g. using images from the environment).</p>

<p>Supervised learning in end to end conversation training is hard because human-human conversation training data is very different from online human-computer scenarios. These approaches also optimise for an immediate reward rather than a long-term one. Reinforcement learning in this domain is hard because of the difficulty of coming up with the right reward function.</p>

<p>Researchers have tried using social media data (e.g., twitter or reddit) to train these systems. They usually need to reconstruct dialogues from the posts, and sometimes run into trouble with API limits. Evaluation is just as challenging as previous sections – it’s common to use human raters, alongside metrics like BLEU, ROUGE, and METEOR. There is research that shows that human ratings and these metrics do not correlate.</p>

<h2 id="chapter-6-conversational-ai-in-industry">Chapter 6. Conversational AI in Industry</h2>

<p>Gives a number of examples, e.g. Bing QA, Siri, Google Assistance, Alexa, Cortana, etc.</p>

<p>Most notably, this section highlights differences between research and application:</p>
<ul>
  <li>Scale and quality of text</li>
  <li>Latency</li>
  <li>User experience</li>
</ul>

<p>Many of the actual chatbot examples do not use any AI at all, and fully rely on handcrafted rules.</p>


  </div><a class="u-url" href="/paper/2018/12/29/neural-conversations.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nlathia" title="nlathia"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/neal_lathia" title="neal_lathia"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
