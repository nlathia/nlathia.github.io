{
  
    
        "post0": {
            "title": "Shadow mode deployments: somewhere between "on" and "off"",
            "content": "Shipping changes to a system that makes a complex decision is rife with uncertainty. You may be adding a rule to a rule engine that already has 10s of rules, you may be changing a machine learning model that eats up 10s of features and spits out a probability, or (more likely) you may be doing a combination of both. . The reason this is often difficult is because the most pressing question that comes up is no longer just ‚Äúwill it work?‚Äù (as in, will the system make a decision?) but, rather, ‚Äúwill it work well?‚Äù (i.e., will the system make a good decision?). . I‚Äôve seen these types of cases lead to a lot of worry and analysis paralysis: teams do not ship because they are worried that their system will make bad decisions, and instead opt to try and map out ‚Äòwhat if‚Äô scenarios on paper or reconstruct a lot of possible scenarios using historical data. . To get around this, I‚Äôm a vocal proponent of shipping systems in one of three states: off, on, and shadow mode. Shadow mode, specifically, is the best tool I‚Äôve found to answer the ‚Äòhow good could this system be?‚Äô question. So, what are these three states? . ‚ùå Off . A system that is ‚Äúoff‚Äù doesn‚Äôt do anything except for integrate well into the rest of the system. If called, it doesn‚Äôt execute anything and returns a default value without breaking. . It seems odd to think about shipping code that doesn‚Äôt do anything. As Stephen‚Äôs post on High Growth Engineering explains, this is a useful way to define interfaces between different systems and enable each subsystem to be iterated on separately. . I‚Äôve seen this approach used most often when wanting to ship a feature that impacts many teams and one team (or group of stakeholders) isn‚Äôt ready. Instead of waiting for them, ship it and turn it off for them! Having the ability to quickly flip the switch and turn things off is also a life saver if you are dealing with an incident. . ‚úÖ On . Systems that are ‚Äúon‚Äù are just that: when called, they do some work, they log some data, and they return a value. . It is common to add conditions here that quantify how on some systems are. This could be for two reasons: . Staged roll out: you may be unsure of the performance of a system, so you just turn it on for a small fraction of traffic (e.g., 5%). As nothing blows up, you continue to turn up the dial until you reach 100% and the system is generally available for all your customers. | Experiment: you may want to compare two variants of a system, and so you redirect a percentage of traffic down one decision path, and the remainder goes down another. This is different from a staged roll out because it ends with the winning variant being turned on for everyone. | Both of these typically manifest in the same way‚Äìusing flags of some sort in the code‚Äìbut have very different intents and outcomes (safely scale up vs. compare and decide). One of my bugbears is when teams call something ‚Äúan experiment,‚Äù when it‚Äôs actually a staged roll out (but I will leave that rant for another day). . ‚ö†Ô∏è Shadow mode . Finally, systems that are in ‚Äúshadow mode‚Äù are somewhere in between: when called, they do some work, they log data about all of their decisions, and then return a default value as if they were off. All of the work is done, but the decision is not acted on. . You end up with is a boat load of data that you can use to pragmatically answer the question: what if this system had been on? (because, technically, it was on!) without requiring Data Scientists to try and reverse engineer answers to this out of historical data. Shadow mode balances between insight (you get all of the data) and risk (you don‚Äôt need to act on the outcomes); critically, it tests your system‚Äôs performance on real, recent examples. . Last year, we were shipping some text classifiers. We had trained them using historical data that had been manually tagged (and we knew the tags were sometimes unreliable); we were unsure of how well classifiers trained on old data would perform on more recent conversations. Given that these classifiers were powering a system that sent automated answers to customers, we were not willing to turn them on without some reassurances that they would work well. . We had a couple of options: (1) wait and collect new data, and then manually evaluate the classifiers using it, or (2) ship the classifiers in shadow mode. . We did the latter; we shipped a system that would query the classifiers for their decision, log the result, and then carry on as if it never had. In this case, the automated answer was not sent to customers even when when the classifiers said that one should be sent. Instead, for a sample of these decisions, the system created validation tasks which would appear in our internal tooling. We used them to manually evaluate those decisions. The result? Within days, we had precision and recall values for the classifiers‚Äìbased on the live data from production. This enabled us to make quicker decisions: to either confidently turn them on, or know that we needed to do more work and retrain the models. . üí≠ How does this compare to A/B tests? . The one bit that you may be thinking about is how shadow mode compares to A/B testing. There are different problems that shadow mode deployments enable solving: shadow mode allows you to know whether a system inherently works before you go ahead and experiment with whether it impacts a business metric. . For the text classifier example, above, we wanted to be sure that the system worked well (it gave precise answers) before testing whether it impacted customer behaviour as we hoped it would (sending automated support answers would encourage self-service). . There are other scenarios where regulatory guidance may impede running A/B tests (I imagine this is the case with credit scoring models), or where you have low-volume/high-impact traffic (as is the case in fraud monitoring) where you may not want to send a percentage of traffic down a path that you don‚Äôt know anything about. . I‚Äôm writing this post as it has become one of the most recurrent things that I‚Äôve talked about with Engineers when we are shipping systems that make a decision. Many times, those decisions are powered by a machine learning model; but the principle applies equally if you remove machine learning from the equation. .",
            "url": "http://0.0.0.0:4000/data-science/tech-lead/2020/07/04/Shadow-mode-deployments.html",
            "relUrl": "/data-science/tech-lead/2020/07/04/Shadow-mode-deployments.html",
            "date": " ‚Ä¢ Jul 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Customer service is full of machine learning problems",
            "content": "For many folks, the words ‚Äúcustomer service‚Äù do not bring to mind images of a high-tech problem space - it certainly was not the case for me. The only public discourse that I‚Äôd read about customer service and tech is about how bots were promised to replace human agents, but have mostly been disappointing - and limited to giving poor answers to simple questions. . After working closely with a bunch of customer service squads at Monzo, I‚Äôve changed my perspective on this. I believe that building systems that enable and optimise customer service operations is going to become an entire industry soon - specifically, one that can be very profitable for companies that apply NLP and machine learning in their offerings to tackle the specific problems that prevent customer service functions from scaling. . What are these problems? I‚Äôve written this post - echoing an internal document that I shared inside of Monzo over a year ago - to describe how much more there is to it than just bots. This post is broadly a list of ideas; it includes some things that my team has built and other ideas that have yet to be prioritised. . ‚ÑπÔ∏è Customer service as information asymmetry . To get us started and to give a little bit of background, here is a framing of the broad problem that customer service is trying to solve that I pitched. . Customer service can be thought of as a problem of information asymmetry: customers have questions and companies have answers. Questions and answers can be broken down further: . There are two types of questions: questions seeking information (‚Äúdo you do refunds?‚Äù) and questions seeking an action (‚Äúmay I have a refund?‚Äù). | There are two types of answers: answers that companies may be happy for customers to find by themselves (‚Äúhere is our refund policy‚Äù), and the answers that they may want to control manually (‚Äúyes, I‚Äôve initiated the refund‚Äù). | . This creates a delicate balancing act for a business: providing excellent service is known to boost brand loyalty and makes customers happy, but maintaining an organisation where agents can reply instantly to everyone is inefficient and infeasibly expensive. . Machine learning systems have a great history of helping with problems that boil down to information relevance and discovery. With this lens, customer service can be broken down into two non-mutually exclusive systems: (1) systems that enable self-service and (2) systems that enable and support agent productivity. . üîç What problems could machine learning help with? . With that background, let‚Äôs dive into the list. For each area, I‚Äôve covered the why? (the business problem), the how (the ML problem formuation), and the so what? (how having this type of system helps). . üïµÔ∏è‚Äç‚ôÄÔ∏è Helping customers discover answers . Why? Many companies manage a set of frequently asked questions (FAQ) pages for customers; these are broadly useless without having a way for customers to discover answers in them. . How? Search is one of the most classic machine learning problems. In the customer service context, it translates to: . How easily can customers find the right FAQ content? . This can either be framed as a ranking problem (sort FAQ content from most to least relevant), a recommendation problem (suggest articles that are relevant to what the custommer is currently doing) or as a question-answering problem (find the content within FAQ pages that directly answers a question). . So what? The best possible outcome - for the customer and the company - is for customers to not need to get in touch, because the information they need is readily available and discoverable. . üè¶ Helping companies categorise known topics . Why? From a macro-level, companies can use an understanding of what customers are talking about to decide how to react as the landscape of queries changes. . How? Many companies tackle this by asking agents to manually categorise conversations, by using tags: topic categorisation is the ML equivalent: . Given a new query from a customer, what is the customer asking about? . This is a classification problem: given a set of N topics (that have typically been defined by the business), we need a system that can accurately recognise whether a new query is about one of those topics. . Categorisation can go beyond query topics; there are a whole host of non-topic based categorisations (e.g., urgency, sentiment, language detection) that could help. . So what? Knowing what a customer is chatting about enables a host of analytics, reporting, and automation. If anything, having an agent confirm a recommended tag rather than tag manually may save a lot of time! . üè¶ Helping companies discover unknown topics . Why? When you build a product, there are likely to be many things you expect customers to ask you about (e.g., refunds, orders, faulty items, etc.). However, there are also a lot of unexpected topics. Discovering these early could enable the business to jump onto problems before they impact all customers. . How? The main problem that topic discovery could try to solve is: . What new (or unusual) topics are customers talking to us about? . This is a unsupervised learning problem: given a set of conversations that have occurred recently, can we discover new topics that are coming in at high volume that the company does not have a process to support? . So what? One thing that wasn‚Äôt intuitive for me is how changes in customer service queries could have nothing to do with your business (or global affairs). For example, when Ticketmaster‚Äôs systems were breached, there was a rise in customer support queries at Monzo. . üëÆ‚Äç‚ôÄÔ∏è Helping agents get up to speed quickly . Why? When agents pick up a new support ticket, the first thing they probably need to do is to understand what is going on - what the customer is asking, who the customer is, what (if anything) has already happened or been done, and so on. . How? The main problem that case summarisation could try to solve is: . What is this relevant information about this case? . For long conversations, this could be treated as a text summarisation problem; it could also be framed as a question extraction problem. Thinking about how to surface the customer information (what‚Äôs the status on which order? what segment of customer?) that is relevant to resolving the question at hand could be treated separately. . So what? Many customer service experiences today are frustrating because the only way that agents have to get this information is to ask the customer (sometimes, more than once). By having ways for agents to see the most relevant information about a customer, they can get to work faster. . üëÆ‚Äç‚ôÄÔ∏è Helping agents say &amp; do the right thing . Why? Companies will have policies and procedures that dictate how different cases should be handled - for example, refunding a customer may be a multi-step process. Many will also have responses with pre-formatted replies to specific types of questions. In some domains, there is a very limited set of actions that an agent could do (e.g., issue a refund). In others (cough banking cough), this range of actions can quickly become huge. . How? The main problem that task-content matching is trying to solve is: . Given a query from a customer, what content do we have that is similar? . This is a ranking problem: given a set of N pieces of content (e.g., stored responses or procedures), we need to build a system that can sort that content from most to least relevant, given the customer query. . So what? Handling a customer query swiftly and effectively is the epitomy of good service: if agents need to spend majority of their time trawling through manuals and typing out every single word of a reply, their responses can be frustratingly slow. . üëÆ‚Äç‚ôÄÔ∏è Helping agents to be productive . This is one of the ideas that my team worked on - with mixed success - and I will be blogging about it separately. . Why? Context switching is a well-known productivity killer for engineers and data scientists. With customer support agents, it‚Äôs no different: if every single query they attend to is completely different from the last, they are likely to spend majority of their time context switching! . How? The main problem that task matching is trying to solve is: . When an agent needs a new task, which of the available ones should be assigned? . This is a ranking problem: given a set of N unassigned tasks that are waiting for an agent, and an agent (who may have specific skills and permissions) who needs a new task, a system that sorts the available tasks from ‚Äúbest match‚Äù to ‚Äúworst match‚Äù can help to maintain the agent‚Äôs productivity throughout the day. . In this case, ‚Äúbest match‚Äù could be framed in many different ways, such as similar to tasks that the agent has successfully resolved recently. . So what? The traditional formulation of support systems is that customers create or submit ‚Äútickets‚Äù which need to be ‚Äúrouted‚Äù to agents. Switching to a ranking mindset can spare the organisation from building and maintaining specialist teams that work needs to be ‚Äúsent‚Äù to. . üëÆ‚Äç‚ôÄÔ∏èüëÆ‚Äç‚ôÄÔ∏è Spreading knowledge across agents . Why? Expert knowledge in customer service is typically manually curated (in processes and knowledge management systems) or inbuilt into the company hierarchy (which manifests as ‚ÄúI‚Äôm going to escalate your query to a specialist team/to my manager‚Äù). However, the questions that customers ask are rarely unique: it is very likely that a similar query from another customer has been resolved before. . How? The main problem that case similarity could try to solve is: . Given a chat case, what (resolved) cases are the most similar? . This is a ranking problem: given a case that has just been assigned to an agent, sort the resolved cases (perhaps with some conditions, e.g.: resolved quickly and with a high customer satisfaction) from ‚Äúbest match‚Äù to ‚Äúworst match‚Äù and show the agent the top-3 most similar cases. . So what? I believe that this approach could be useful to help agents see how other cases have been solved by others ‚Äî potentially increasing knowledge discovery and reducing the need to escalate things to other teams. . üè¶ Monitoring outcomes as they happen . Why? There are two common ways that companies track the quality of their customer support: by asking customers (with a customer satisfaction survey) or by reviewing cases (using a quality assurance team). Both of these are lagging metrics that only happen after-the-fact. . How? The main problem that case monitoring could try to solve is: . Given a case that is ongoing, what outcome is likely to happen? . This could cover a variety of regression problems: given a case, we could try to predict how long it will take to handle, what the likelihood of complaint is, what the customer‚Äôs satisfaction rating will be, etc. . So what? This approach could be useful if we wanted to intervene in specific cases (e.g., detect that a conversation is likely to end up in a complaint so that we can do something about it earlier). . üìú The end! . This list of ideas switches customer service from ‚Äúthis will be replaced in its entirety by bots‚Äù towards ‚Äúautomation can help human service scale.‚Äù .",
            "url": "http://0.0.0.0:4000/machine-learning/monzo/2020/06/27/Customer-service-machine-learning.html",
            "relUrl": "/machine-learning/monzo/2020/06/27/Customer-service-machine-learning.html",
            "date": " ‚Ä¢ Jun 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "How to over engineer a sound classifier",
            "content": "üè° Hack day idea . I recently moved our washing machine out to the garage, which meant that I couldn‚Äôt hear it beep when it was finished. I would have some ‚Äúfalse positive‚Äù trips out there (through the drizzly British rain!) when timers went off, only to find that the thing was still going. How horrendous. . It had also been a while since I wrote some code just for the sake of building something and I had never done anything with software and sound (recording, cleaning, classification). So, solving this first world problem for myself became my small üí° idea for a hack project I could work on as the world slowly started locking itself down. . This post is an overview of what I built. It does not cover the day that I spent dusting off the old laptop and generally waiting for all of the updates to download (goodbye, Python 2.7!). It also does not give appropriate credit to the infinite StackOverflow posts that I read along the way. . All of the code for this is on Github in my sound-detection repo. . üéß Collecting training data . The first thing I needed was some data: I used the PyAudio library for this. . PyAudio is a library that allows you to record audio by reading from a stream, in a similar way that you would read from a file. There was a bit of faff with figuring out sampling and frame rates - I ended up using default values that I found on different examples. . I ended up with the function below that I used to record 3-second long samples of audio. The function returns as a list of arrays - in case you‚Äôre wondering, here‚Äôs a post about the difference. . Critically, I also added a counter that would tell me (approximately) what percentage of the sample was ‚Äúsilent.‚Äù All I did was check what the max value of each array was; if it was less than a totally arbitrary value of 300, I counted it as silent. I tested this by recording a few samples and shouting at my computer. . def record_sample(stream, rate, seconds=NUM_SECONDS): frames = [] count_silent = 0 for i in range(int(rate / NUM_FRAMES * NUM_SECONDS)): sound_data = array(&#39;h&#39;, stream.read(NUM_FRAMES, exception_on_overflow=False)) if max(sound_data) &lt; SOUND_THRESHOLD: count_silent += 1 frames.append(sound_data) percent_silent = float(count_silent / len(frames)) logger.info(f&quot;‚ÑπÔ∏è Finished recording {seconds} seconds: {(percent_silent * 100):.2f}% silent.&quot;) return frames, percent_silent . I saved any 3-second sample that wasn‚Äôt silent as a wav file, using the wave library. Somehow, I recall seeing some error when I tried to write this function using a with statement, so I ended up opening and closing the file directly: . def save_sample(frames, rate, sample_size): file_path = os.path.join( &quot;data&quot;, &quot;live&quot;, f&quot;Sound-{str(datetime.now())}.wav&quot; ) logger.info(f&quot;‚§µÔ∏è Storing audio to: {file_path}...&quot;) wf = wave.open(file_path, &quot;wb&quot;) wf.setnchannels(NUM_CHANNELS) wf.setsampwidth(sample_size) wf.setframerate(rate) wf.writeframes(b&#39;&#39;.join(frames)) wf.close() return file_path . With this minimal setup, I did a couple rounds of laundry and ended up with a bunch of wav files! That was the end of day 1 of the hack project. . üè∑ Labels . For day 2, I started by having to label the data I had previously recorded. I did this manually, by listening to all of the recordings ‚Äì well, at least the first second of each one. Luckily, these washing machines don‚Äôt tend to beep or spin at random, so all I had to do was find when it started and stopped doing one of those things, and bulk move all of those files into a directory. At this point I was thinking of making a classifier that could tell me about different things that the machine was doing, so I created four groups: beeps, spinning, washing, and ‚Äúhuman‚Äù (which was usually me coming in and out of the room). . I‚Äôm used to regularly labelling text for our classifiers at work, but usually do things like listen to music while doing this. Stepping through and listening to audio files needs your eyes, ears, and hands - this was all encompassing. It is also a prime way to annoy other people in your household. . In summary: this was super boring, so I had a gin &amp; tonic while I was doing this. I ended up with 43 samples of beeps, 588 samples of the machine making noises as part of the wash cycle, 38 samples that were sounds from me, and 748 samples of the machine spinning. I would later come back to this and change it to two classes: beeping and not beeping - which is what I ended up using. . Once that data was sorted into different directories, I loaded up the file paths (and corresponding label/directory) into a Pandas data frame and then used scikit learn for what it is best known for: train test split. . ü§ñ 1D Convolution . Okay, finally! Time for some machine learning. I fired up my browser to figure out how to even begin on this. . This is the bit of the hack that is intentionally over-engineered. I am very aware that all I really wanted to do was detect a high-pitched sound among mostly background noise, and so could have gone down the audio analysis route. But that‚Äôs not fun, so I didn‚Äôt do that. . At work, we primarily use PyTorch, and so that was my first port of call. I found this tutorial which points to a paper called ‚ÄúVery Deep Convolutional Neural Networks for Raw Waveforms‚Äù (here‚Äôs the PDF). I skimmed the paper - it looks like it‚Äôs based on some dataset of urban sounds called UrbanSound8k, which has 10 classes of sounds like horns, children playing, and dogs barking. The tutorial also links to this Colab notebook. . I first tried swapping my dataset into this notebook, but soon hit all sorts of errors. I think this boils down the fact that the tutorial was written for PyTorch 1.1.0, and I was running 1.4.0. Everything was broken. . I ended up going back to first principles. By that, I mean that it had been so long since I had worked at this level of detail in PyTorch that my first few attempts didn‚Äôt work at all, and I had to go and re-learn about 1-D convolutional layers. Here‚Äôs a really good YouTube video that helped me. . In the end, I made a neural net with a dimensionality reduction step (1-D convolution, batch norm, and max pool), and then a classifier (linear and softmax): . class BeepNet(nn.Module): def __init__(self): super(BeepNet, self).__init__() self.main = nn.Sequential( nn.Conv1d( in_channels=NUM_CHANNELS, out_channels=2, kernel_size=KERNEL_SIZE, stride=STRIDE ), nn.BatchNorm1d(num_features=NUM_CHANNELS), nn.MaxPool1d(kernel_size=2), ) self.classifier = nn.Sequential( nn.Linear(in_features=298, out_features=NUM_LABELS), nn.Softmax(dim=1) ) def forward(self, x): batch_size = x.size(0) hidden = self.main(x) return self.classifier(hidden.view(batch_size, -1)) . Once this seemed to be working, I looked into using a GPU to train the model. I spent a while moving the data into Google Drive and reading about how I could load it all into a Colab notebook. In the end, this was another unnecessary rabbit hole and I trained the whole thing in minutes on my laptop. . I trained the model for a few epochs and it converged pretty fast. I then looked at the examples of what it was doing, and the results seemed legit. I hear you asking - what did you do about overfitting? The answer is that I did absolutely nothing. A model that was overfit on these beeps (that all sound exactly the same) was fine. . You can see the notebook that I used here. . ‚è≠ Deploying to production . The final stage was to make something that could use this model to detect the beeps, and somehow let me know. . At this stage, I had two different components: a PyAudio thing that would record samples and save them to a wav file, and a PyTorch model that would use torchaudio to load data from a file and give it to the model. Instead of figuring out a way for the PyAudio data to go directly to the model, I decided to keep what I already had and use the disk as an intermediary. . Here‚Äôs how I made this unnecessarily complicated: I decided that it would be unacceptably slow if all of this happened in a single process. So I turned back to an old friend, the multiprocessing library - and found out how multiprocessing has a neat bug where Python crashes on macOS; setting some weird flag export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES before running it fixed this ü§∑‚Äç‚ôÇÔ∏è. . The main process in my pipeline records a sample of audio and (if it is not silent) saves it to a file; it then pops the path to the file onto a queue. On the other side, a classifier process reads from that queue and loads up and classifies any file that is popped onto it. . What does it do if it detects a beep? I hunted around for different options here. One of the first options I thought of was to send myself an email; the problem is that I‚Äôve turned off gmail notifications (and my life has been much better since). I then went down a rabbit hole of options - Signal, WhatsApp, SMS gateways, paid services, and all of that. . I settled on using Telegram, because I stumbled onto a Medium post about setting up a bot and sending it a message with Python, and it looked do-able. But, what if the model was wrong? How could I avoid that short walk out to check? I decided that the pipeline should also send me the actual audio that it thought was a beep. Sending a snippet of audio via telegram was not something that looked super straightforward, until I ran into the Python Telegram Bot library. The main problem I ran into was that this library would only send sound files that were formatted as mp3s. Instead of re-writing everything to always use mp3s, I found an mp3 encoder called lame that could be installed via brew. I found that before finding any Python library that I could use directly, so I just called this function from Python: . def convert_to_mp3(file_path): path_fields = os.path.split(file_path) file_name = path_fields[1].replace(&quot;.wav&quot;, &quot;.mp3&quot;) result_file = os.path.join(path_fields[0], file_name) logger.info(f&quot;üéß Converting: {file_path} -&gt; {result_file}&quot;) command = f&quot;lame --preset standard &quot;{file_path} &quot; &quot;{result_file} &quot;&quot; result = os.system(command) logger.info(f&quot;üéß Converted to mp3 with result={result}&quot;) return result_file . üéâ That‚Äôs it! ‚Ä¶Or was it? . All of this means that I now take my super old laptop and fire up the pipeline after I‚Äôve started the machine. I then go and hang out, anxiously waiting for a message. Here‚Äôs how I tweeted when it started working! . . The first time it worked, I was overloaded with messages. I had forgotten to add a way for it to not send me a message every time it detected a beep (which was happening in multiple 3-second interval successions), so I had to add in a way for it to be rate-limited to one message every X minutes. . There were also a couple of times that it didn‚Äôt seem to work: the classifier process would die, the laptop‚Äôs wifi would have briefly gone down, or other such oddities. So I added in a bunch of logging, a time out (it would message me if it hadn‚Äôt detected a beep in more than X minutes), and I added in the tenacity library‚Äôs annotations so that it would retry message sending. . üíª What‚Äôs next? . All of the code for this is on Github in my sound-detection repo. Feel free to use it, find bugs, and tell me what‚Äôs wrong with it. . The silliest thing about all of this is that I now have to fire up my old laptop when I want it to monitor a laundry cycle. Some day in the future I‚Äôll think about spending some time getting this to work on a Raspberry Pi or some other device that I can leave out there. .",
            "url": "http://0.0.0.0:4000/machine-learning/2020/04/02/How-to-overengineer-a-sound-classifier.html",
            "relUrl": "/machine-learning/2020/04/02/How-to-overengineer-a-sound-classifier.html",
            "date": " ‚Ä¢ Apr 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Testing SQL the hard way",
            "content": "üìä Context . It‚Äôs not uncommon for Data Science workflows to have large chunks of SQL. Maybe you have a sequence of queries that you run every day to produce dashboards, or maybe you have a bunch of queries that spit out features that you feed into machine learning algorithms. If you‚Äôre a Data Scientist, you‚Äôre bound to have written a SELECT statement at some point of your day! . There‚Äôs an online wiki for programming languages called progopedia which lists SQL as ‚Äúnot a programming language:‚Äù this perfectly characterises everything that is strange about these words that we write, execute, and rely on to get our insights right every single day. SQL doesn‚Äôt have all the sensible things that real programming languages have to help us feel confident that they are doing what we think they‚Äôre doing. Primarily, it lacks a straightforward abstraction for ‚Äúunit‚Äù testing. . About a year and half ago, a colleague Jack and I were wrangling with a query that had to be right‚Äìwe are, after all, working in a regulated industry. We were dealing with a bunch of queries that were orchestrated together with Airflow. The main problem we had was that we could not rely on the input to our query being right and needed to prevent this query from quietly succeeding if anything in its results was wrong. . We went the entire journey from manually validating entries in our table to automating the detection and reporting of inconsistencies so that we could flag or fix errors that were upstream of our query: this post is a heavily simplified example of what we did. In case you‚Äôre wondering, I‚Äôve called the post ‚Äútesting SQL the hard way‚Äù because we have now abandoned this approach (and replaced it with dbt). But it had some fun lessons, so here goes! . üëØ‚Äç‚ôÇÔ∏è A table of users . Let‚Äôs look at a toy example: let‚Äôs say that we want to make a table that has some key information about all of a company‚Äôs customers. I‚Äôve written all of the snippets below in BigQuery SQL, so that you can copy/paste it into the console and play around with it yourself. . We start by building up some mock data. Imagine you have a JSON payload (an ‚Äúevent‚Äù) each time a user creates an account, with the user_id, the timestamp of when the account was created, and a bunch of other critical fields. Something like this: . WITH account_created_events AS ( SELECT * FROM UNNEST([ &quot;{ &quot;user_id &quot;: &quot;user_1 &quot;, &quot;account_created &quot;: &quot;2020-01-01 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_2 &quot;, &quot;account_created &quot;: &quot;2020-01-02 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_3 &quot;, &quot;account_created &quot;: &quot;2020-01-03 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_4 &quot;, &quot;account_created &quot;: &quot;2020-01-04 &quot;}&quot; ]) AS event ), . When users finish some set of actions, we get another event to tell us that the user has completed their signup. Here‚Äôs some more mock data, for those same users: . signup_completed_events AS ( SELECT * FROM UNNEST([ &quot;{ &quot;user_id &quot;: &quot;user_1 &quot;, &quot;signup_completed &quot;: &quot;2020-01-02 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_2 &quot;, &quot;signup_completed &quot;: &quot;2020-01-03 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_3 &quot;, &quot;signup_completed &quot;: &quot;2020-01-04 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_4 &quot;, &quot;signup_completed &quot;: &quot;2020-01-05 &quot;}&quot; ]) AS event ), . We can take those two events and create a table with one row per user: . -- Extract the account creation columns WITH accounts_created AS ( SELECT JSON_EXTRACT_SCALAR(event, &quot;$.user_id&quot;) AS user_id, TIMESTAMP(JSON_EXTRACT_SCALAR(event, &quot;$.account_created&quot;)) AS account_created FROM account_created_events ), -- Extract the signup completion columns signups_completed AS ( SELECT JSON_EXTRACT_SCALAR(event, &quot;$.user_id&quot;) AS user_id, TIMESTAMP(JSON_EXTRACT_SCALAR(event, &quot;$.signup_completed&quot;)) AS signup_completed FROM signup_completed_events ) -- Join them together SELECT user_id, account_created, signup_completed, TIMESTAMP_DIFF(signup_completed, account_created, HOUR) AS signup_time FROM accounts_created LEFT JOIN signups_completed USING (user_id) . And here are the results: . . Perfect! This is the ideal analytics table to answer a ton of different basic analytic questions: How many users do we have? How many of them signed up today? How long does it take users, on average, to complete signup? . This is also the type of table that could be extended with tens (or even hundreds) of columns, each representing unique pieces of information about each user. It is the type of canonical data table that could grow to having tens of contributors and many tens of use cases. As long as this table maintains its core structural assumption (one row per user), we‚Äôll be good to go. . How could this go wrong? . üôÉ Someone completes the signup flow twice . Let‚Äôs imagine that a customer, due to whatever reason, completes the signup flow twice. Maybe there‚Äôs a bug in the app and the customer uninstalls &amp; reinstalls the app, maybe there was a bug or failure in the backend service that is emitting those events (whatever, it doesn‚Äôt matter!). . Nothing has changed in the analytics code base, but now we have two signup completed events for that user: . signup_completed_events AS ( SELECT * FROM UNNEST([ &quot;{ &quot;user_id &quot;: &quot;user_1 &quot;, &quot;signup_completed &quot;: &quot;2020-01-02 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_2 &quot;, &quot;signup_completed &quot;: &quot;2020-01-03 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_3 &quot;, &quot;signup_completed &quot;: &quot;2020-01-04 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_4 &quot;, &quot;signup_completed &quot;: &quot;2020-01-05 &quot;}&quot;, &quot;{ &quot;user_id &quot;: &quot;user_3 &quot;, &quot;signup_completed &quot;: &quot;2020-02-04 &quot;}&quot; -- Yikes! ]) AS event ), . This tiniest of errors ‚Äì something that is largely invisible to the person who is writing the SQL, breaks the table: user_3 appears twice. Not only that, but your stats on signup completion rates have shot through the roof! . . Herein lies the problem: a tiny issue in the data has propagated itself through the LEFT JOINs in the analytics code base, and has completely skewed some metrics that you are using. . Of course, you could argue that this is fairly trivial to fix: we‚Äôd need to add in a pre-processing step on the signup completed events to pick the one we care about more, and spit out one signup event per user. But what if this table has tens or hundreds of columns? What if the person contributing a new column is not the person who added the signup stats column, so doesn‚Äôt know this? What if we have two of these types of problems manifest at the same time? What if this table is input to some downstream queries, and all they know is that their table is now wrong? . These problems are more of a headache to discover and diagnose than they are to fix. Indeed, they may first manifest as a question about the metrics (‚Äúwhy has our average sign up time gone through the roof?‚Äù) and not about the data. . üêõ ‚ÄúUnit‚Äù testing a table in two steps . This post is not about fixing those bugs: it is only about an approach that we used to use to detect these types of problems with some sort of meaningful error message. At its core, this method relies on enumerating your assumptions about the structure of the table, and then triggering an error in the query execution if you find any. . In our example, our key assumption was that the table has one row per user. . Step 1. Separately from the query above, we wrote a query that should create an empty users_table_errors table if the users table‚Äôs assumptions were not broken: . WITH user_validation AS ( SELECT user_id, COUNT(*) AS num_rows FROM users GROUP BY 1 HAVING num_rows &gt; 1 -- Should never be the case, amirite? ) SELECT &quot;Duplicated user_ids&quot; AS failure_type, COUNT(DISTINCT user_id) AS num_failures FROM user_validation -- Union all with any other validations (e.g. validating time stamps) . These types of queries are an extremely useful way of (a) documenting what you expect, and (b) creating a table of all of the (in this case) user ids that don‚Äôt match your expectations ‚Äì and how many times each one is duplicated. . . Step 2. Now that we have a way to identify errors, we need a way to stop our query from completing successfully if any errors are found. The first way we did this was to force BigQuery to compute something it couldn‚Äôt if it found errors: we would literally encode a one divided by zero. Shortly after, we found a debugging function buried in the BigQuery documentation. . If a validations table is not empty, we used this ERROR() function to stop the query! It looks like this: . SELECT ERROR(CONCAT(&quot;ERROR: &quot;, num_failures, &quot; &quot;, failure_type)) AS error FROM users_table_errors WHERE num_failures != 0 . If you run this in the BigQuery console, it pops up with this kind of alert: . . The final workflow would run (1) the original query, (2) the validation query, and then (3) the error query. This approach really accelerated our ability to diagnose and fix errors; it de-coupled the code SQL that did the work from the SQL that did the validation, and it automatically documented all of our assumptions about a given table. . For our example, above: we‚Äôd see an error message that reads ERROR: 1 Duplicated user_ids; we‚Äôd pop open the users_table_errors and see that it has 1 entry with user_3; and voil√†, we‚Äôd already be on track to finding and resolving the problem. . üèö Why is this the ‚Äúhard way‚Äù of testing? . We got a lot of value out of the approach above; the major trade-off was that we had to write (nearly) double the amount of SQL. This is not too dissimilar from what happens in other programming languages, where we can write unit tests. . However, these are definitely not unit tests: we would primarily discover errors in the data when the analytics pipeline was running, and had to make choices as to whether we wanted our (fairly huge) graph of queries to halt running completely on every error, or continue with warnings. We had started designing ways to quantify the severity of errors (both in terms of columns and in terms of number of affected rows), to run queries on samples of data, and more. . By the end of 2019, though, the Data Science team at Monzo migrated to using data build tool, or dbt. If you want to read about why we think that was a good choice, Stephen‚Äôs post here describes things that we like about it. It helped us to automate a lot of the work flow that I‚Äôve described above, and more. . Does this solve everything we need to ensure reliable, consistent, and scalable analytics queries? Absolutely not. The experience of writing this kind of ‚Äúcode‚Äù is still very much characterised by the ‚Äútry running it and see if the results look sensible,‚Äù which is a far cry from where it could be. But it‚Äôs a great step in the right direction. .",
            "url": "http://0.0.0.0:4000/data-science/monzo/2020/03/15/Testing-SQL-the-hard-way.html",
            "relUrl": "/data-science/monzo/2020/03/15/Testing-SQL-the-hard-way.html",
            "date": " ‚Ä¢ Mar 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "When is a neural net too big for production?",
            "content": "üí¨ Background . Over the last couple of years, there have been a ton of exciting developments in natural language processing (NLP). In case you haven‚Äôt been working in this area, here‚Äôs the crash course: the development of deep pre-trained language models has taken the field by storm. In particular, transformer architectures are everywhere, and were popularised by Google‚Äôs release of the Bidirectional Encoder Representations from Transformers (BERT) model, OpenAI‚Äôs release of the GPT(-2) models, and other similar releases. . Various research teams are continuing to compete to train better language models; if you look at the General Language Understanding Evaluation (GLUE) benchmark leaderboard, you‚Äôll find a host of other approaches (many of them also named BERT: ALBERT, RoBERTa, etc.). The overarching trend in this research has been to train bigger models with more data - growing to the extent that researchers have investigated the costly carbon footprint of training these large networks. . For practicioners, the main selling point of pre-trained language models is that you do not need to start from scratch when developing a new text classifier: you can fine tune a pre-trained model and often get state-of-the-art results with a fraction of the work. But as these models continue to grow in size, folks have started to question how useful they are in practice. For example, this quote in a blog post from the explosion.ai team caught my eye (emphasis mine): . In a recent talk at Google Berlin, Jacob Devlin described how Google are using his BERT architectures internally. The models are too large to serve in production, but they can be used to supervise a smaller production model. . Unfortunately, it looks like this talk was not recorded, and so all of the context around this claim was lost. In light of that, this post gives an overview of how these models can and have been put into production. . üö¢ Patterns for models in production . Let‚Äôs begin with a very pragmatic question: what is ‚Äúproduction?‚Äù . For our purposes, production is the environment where we put software that we have written (and models we have trained) so that it can enable features in a product to work without any manual intervention. By this definition, we exclude any code that we use for analytics or ad-hoc purposes, even though there may be potential applications of NLP in those domains (e.g., sentiment analysis on historical data). . I‚Äôm also assuming that this environment is designed using microservices ‚Äì which just so happens to be how the Monzo backend is designed. . There are three main ways that models can be used in production: . RESTful Services. This is the first (and sometimes only thing) that comes to mind when folks talk about ‚Äúproduction.‚Äù The idea is to build some kind of microservice with an API which can receive requests, do some work (i.e., get predictions from a model), and return a result. For example, when a customer types a query into the Monzo app‚Äôs help screen, we have a service that receives that request and returns relevant help articles (disclaimer: that has been simplified a bit. We have quite a few services that are involved in this work, but the idea is the same). . Consumer Services. The second approach is to build a service which listens out for certain events and requests for some work to be done when they happen. For example, when a customer starts chatting with our customer support team, we have a service that is listening for particular events in order to (a) generate embeddings of the chat‚Äôs first turn, and (b) trigger recommendations that are shown to the agent for saved responses that may be relevant to the current query. . Cron Jobs. These are batches of work that need to be done on a regular basis. For example, we store all of the help articles and agent responses in a content management system - and these are regularly edited and updated with new content. Our search and recommendation services use the embeddings of this content: we therefore have cron jobs that regularly run, encode all of the text and store the embeddings so that they can be used when needed. . In practice, building an end-to-end system is likely to involve more than one of the above. I‚Äôve already mentioned the system which gives our agents response recommendations: this system has a cron job (to encode all of the response text), a consumer service (which decides when recommendations should be triggered), and a RESTful service (which is, effectively, a k-Nearest Neighbour between the encoded customer text and the encoded responses). . üôÖ‚Äç‚ôÇÔ∏è When is a model too big? . Now that I‚Äôve described three generic ways that models are shipped, let‚Äôs tackle the main question: when is a model too big? There are two scenarios to consider: (1) a model is too big to ship at all, and (2) a model‚Äôs size is making it inefficient. . Too big to ship at all? The main question that may prevent shipping a model at all is about reconciling the hardware (where you want to run a model) with the size of the model. In practice, current models‚Äô sizes are not a big problem in cloud-based backend systems, which have a variety of different instance sizes available - the hardware we have in the cloud can ship a model like BERT. It may eat up a ton of memory - but it will work. . This could change if you want to want to ship a model elsewhere (for any other reason). For example, this ICML workshop talk by Andrej Karpathy describes a large multi-task neural net that needs to run in a car, or folks at Google are investigating federated learning in order to distribute model training across mobile phones. This is part of a broader movement that is pushing machine learning towards ‚Äúedge‚Äù devices, which are generally resource and energy constrained. . Too big to be efficient? Models are often trained using GPUs, but shipped on non-GPU instances, where inference will be slower. As models get bigger, inference time often continues to grow. There may be a point where this slow down makes it infeasible. This is going to be a very application-specific decision: for example, a chat bot responding within a few seconds may still be ‚Äúfast‚Äù in the customers‚Äô eyes, while if it took a similar time to get search results on Google, something would seem odd. To dig deeper, let‚Äôs reference the three patterns above. . In cron job settings, inference time is usually not such a big deal - predictions can be batched, and need to be completed on a schedule. Performance will become more of an issue as the amount of data grows, and we can then consider parallelising the problem to make it faster. . In the other two patterns, things become even more application specific. Consider, for example, the system I mentioned above that is consuming chat events to decide to push saved response recommendations to our agents. In this case, the time the system needs to generate those recommendations should be (broadly) less than the time that it takes an agent to read through what has been written so far - this use case is measured in the order of multiple seconds; i.e., nearly an eternity for computers. . Finally, we have services that we are experimenting with to try and improve the app help screen - some of these are using BERT. In our first experiment, we saw that some of these services were struggling under the load they were receiving - but the first port of call is to scale them horizontally rather than pull the handbrake and not deploy them at all. This means that we are trading off between how many instances we need (or want) to spin up and the performance we want to achieve, much like what happens when these same models are trained on large clusters. . ü§ó Example: serving BERT predictions . At Monzo, we have decided for our Python microservices to be as lightweight as possible: they are effectively a nice wrapper around a model‚Äôs predict() function, and we write the rest in Go - the main language that is used throughout the Monzo backend. . We have built a cookiecutter Python microservice template that uses Sanic. When one of these services is starting up, it needs to do two, fairly slow, things: (1) find and retrieve the model that it wants to serve (I‚Äôll blog about that problem separately), and (2) load the model (in eval() mode) and the tokenizer as global variables in memory. . (Side note: I originally intended to share some code snippets here, but this jekyll theme stubbornly refused to be mobile friendly. So I‚Äôve removed it - but it‚Äôs not far off from the available examples online.) . I did a small test on my own laptop using this approach which uses psutil to measure the ‚ÄúResident Set Size‚Äù memory usage (is this the right way? ü§∑‚Äç‚ôÇÔ∏è). Before loading the model, memory usage was about 79 MB: after the call to load_model(), it shot up to just over 957. A huge jump, yes (and 100s of times bigger than what you would expect in non-machine learning services) - but still well below what decent cloud instances provide. . Once these steps have finished, the service will start serving requests. Each of these services will have an endpoint (or Sanic route) to get the model‚Äôs predictions for a given input. . The async and await syntax in Sanic routes is the key here: handlers are an async co-routine, and all model predictions are run in the asyncio event loop, rather than as blocking functions. There are many blog posts that describe the details of the Python event loop and the async / await syntax much better than I ever could; if you‚Äôre interested, I‚Äôd recommend searching for this topic (here‚Äôs one). . The model_predict() runs model predictions with torch.no_grad(): this ensures that the autograd engine is not used. According to this thread, this reduces memory usage and speeds up computation. Naturally, you can‚Äôt then backpropagate (needing to do so while serving is an entirely different type of problem!). . There was one tiny trick that two folks on the team discovered which helped us to make these types of services even faster. They discovered the issue that it seems others have also found regarding threading performance and the OMP_NUM_THREADS and MKL_NUM_THREADS environment variables; the one difference was that they also had to factor in how all of this plays with Sanic worker threads. . üîç Reflections . The main disclaimer that I‚Äôll add to the above is that we are currently in the stage of development where we‚Äôre iterating on and validating new product features, and not squeezing performance gains out of existing ones. Perhaps, once we reach that stage (where every megabyte of memory, instance we spin up, and microsecond matters), I‚Äôll change my mind about BERT being suitable for production üòä. . So, after writing an entire blog post about how we can (and do) but large models like BERT into production, I‚Äôll close with two thoughts. . First, the research on distilling and compressing these models into smaller ones that retain similar levels of accuracy is still very valuable. It will unlock our ability to (a) run these models more efficiently, and (b) run them on edge devices at all. Just as training larger models with more data is showing impressive feats of engineering and distributed model training, doing more with less seems to push researchers towards hard, unsolved problems of understanding how neural nets learn at all. . Second: the main (hidden) assumption that I‚Äôve had throughout this whole post is that an entire neural net should be shipped as a single service. Maybe this doesn‚Äôt have to be the case: a network could be broken up into multiple microservices (e.g., imagine freezing an entire pre-trained network and then shipping multiple different fine-tuned heads into different services). I haven‚Äôt been able to find a lot of blog posts about machine learning in production - if you find (or write!) one, do send it my way. . Acknowledgements: thanks to Stephen Whitworth for feedback on a draft of this post. .",
            "url": "http://0.0.0.0:4000/opinion/monzo/2019/09/29/Large-NLP-in-prod.html",
            "relUrl": "/opinion/monzo/2019/09/29/Large-NLP-in-prod.html",
            "date": " ‚Ä¢ Sep 29, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Machine learning, faster",
            "content": "Speed is not a word that is regularly associated with machine learning teams. When we talk and write about accomplishments in machine learning, there is often a focus on the problem, the algorithmic approach, and the results - but no mention of the time that it took to get there. . Does speed matter in machine learning? . I remember once speaking with a machine learning researcher who worked at a large company. He told me that a product team had approached him with a very exciting idea that had to do with text summarisation. He started looking into the problem and made some very significant contributions over the course of 12 months - going so far as publishing papers in top-tier conferences about the topic. I asked him if his ideas made it into the product in question. Unfortunately, the answer was no: by the time his research was completed, the product team had moved on from this problem, and weren‚Äôt interested in having the solution anymore üò≠. . I‚Äôve heard many variants of this story: they all capture a misaligned pace of work between product and machine learning teams. Ultimately, this leads to machine learning research never making it out of the lab. And yet, the best measure of impact for machine learning, if you work in a non-research institution, is whether you can use it to help your customers - and that means getting it out of the door. . Speed matters. This goes beyond thinking about minimum viable products (and the ML equivalent of ‚Äúuse a logistic regression before you use a neural network‚Äù); this is about the speed of the entire lifecycle for building machine learning systems. . I covered four angles to this topic, when I gave talks about it recently: . üö¢ Quickly deploying models to production . Quickly deploying models to production is one of the biggest roadblocks for impactful machine learning. In many companies, this boils down to who is trusted to do this work; often, ‚ÄòScientists‚Äô design and train a model, and then hand it over to ‚ÄòEngineers‚Äô to put it into production. This implicitly develops a ‚Äúthrow it over the wall‚Äù mentality: people who train models do not have to think about how complex it would be to ship, and folks who ship models can throw a model back over the wall if it‚Äôs not behaving how it should. The most common complaint I‚Äôve had from Scientists is along the lines of ‚ÄúI trained this model months ago and I‚Äôm just sitting here waiting for it to be shipped,‚Äù and Engineers retort that they get no recognition for doing all of the hard work of enabling production inference to actually happen. A frustrating experience all around. . At Monzo, Machine Learning Scientists deploy their own models. We are enabling that by focusing on our tooling. The goal that we set ourselves is that if you can write a predict() function for a machine learning model, you should also be able to safely deploy a model to production. To enable this, we‚Äôve built a cookiecutter microservice template that Machine Learning Scientists use: it guides them through a small set of options (e.g., ‚Äúwhich ML library are you using?‚Äù) and then creates everything they need, minus small bits, like the predict() function itself. We can now build and deploy a service in a matter of hours - right after we finish training a model. A Scientist does not have to wait around for others to finish the work, and can take an idea all the way from validation to deployment. . üîç Quickly validating misbehaviours . Last year, after a re-launch of our help screen search system, a very common question that we would get from product teams is ‚Äúwhen I search for X, why doesn‚Äôt article Y show up?‚Äù Trying to explain machine learning algorithms is hard enough - diagnosing minor misbehaviours felt even more challenging. Has something gone wrong with our data pipeline, our model, or how we are post-processing the results down stream? . This is a type of problem that could grind us to a halt: it‚Äôs not the sort of thing we could write unit (or integration) tests for, and neither is it something where the problem is immediately clear. To tackle this quickly, we started writing validation tests: these are a set of fairly simple inputs (e.g. ‚Äúmoving home‚Äù) that have obvious outputs (the updating my home address article), which we expect our production machine learning system to be able to return when making predictions. These run in production: they are integrated into how we update the embeddings that are stored in the system, and prevent updates from happening if expected predictions fail (also, we get notified on Slack ‚ö†Ô∏è ). . Keeping track of the online performance of machine learning models is going beyond what we traditionally do when deploying software - writing validation tests is another small improvement that has helped us to keep on top of how these systems operate online. . ‚ôªÔ∏è Quickly repurposing models for new problems . Last year, Monzo went through a challenging period where non-urgent response times for customer support was in the order of days rather than hours. The entire company rallied behind this: engineers, designers, and lawyers all dropped what they were doing to respond to customers. . As part of enabling the entire company to answer customers, a set of topic-based queues were set up; the idea was that it is easier to train someone to answer one type of question rather than answer any kind of question. Our team was asked whether we could set up a way to detect what customers were talking about and automatically assign queries to queues accordingly. Typically, machine learning teams would start here with the usual questions (e.g., what data do we have?) and methods (e.g., training a classifier). However, this wasn‚Äôt a request to kick-start a project looking into text classification: this was a request to get something working and deployed, now. . One of the systems that we were already running at the time served recommendations to customer support agents - it looked at what customers said and returned recommendations for which of our many different saved responses may be appropriate to use. For example, if a customer says ‚ÄúI‚Äôve forgotten my PIN,‚Äù the system recommends a response that contains all of the details of how to recover it. This system is not explicitly categorising queries; it is mapping questions to recommended answers based on a similarity metric. However, the recommended answers could easily be mapped to categories! . We therefore quickly repurposed this system‚Äôs recommendations to solve for the queue classification problem: we wrapped the recommendations in if statements to redirect queries to queues based on what reply was recommended. This was deployed this in less than a day! . This lesson has stuck with us since then: combining rule engines with machine learning models can be used to quickly create, contextualise, or repurpose a decision system. . ‚è∞ Measuring time-to-results, not results . The systems mentioned above use an encoder architecture that was published in 2017 (based on the Attention is all you need paper). While working on it, we felt that, overall, we were taking advantage of latest research when building our systems. Then, in 2018, deep pre-trained language models (like ELMo, ULMFit, BERT) appeared and started to take the top spot in a variety of research challenges - and many of them were open sourced. Every few months the state-of-the-art was changing. As a non-research team, that focuses on building systems, how could we keep up with this pace of research? . We decided to set ourselves a challenge: to explore how these techniques could work on as many different NLP problems as we could find, in a really short time (a couple of weeks). A typical approach here, which I have used before, is to embark on a new project for each idea: analyse the data, understand/refine the problem, train and evaluate models, and tweak until something starts looking promising. There was no way that, using this approach, we could evaluate tens of ideas in a fortnight. Instead, rather than trying to answer the question ‚Äúwhat is the best result I can get?,‚Äù we designed our work around ‚Äúhow long will it take to get a result?‚Äù . Switching our perspective led us towards taking a highly modular approach to our NLP work. We built a pipeline that creates supervised learning datasets from all of our chat data: we just point it to what we want to predict, and give it some additional arguments to tweak how we want the text to be processed. This tool is completely separate from anything that does any machine learning; for that, we built a number of Colab notebook templates, which take (as input) a dataset, and then run it through a specific algorithm (e.g., ULMFit). Our guiding principle was that anyone should be able to point a notebook at their dataset, and then run all the cells to get to a result. . Many of the experiments that we ran had poor results - and we have thrown them away. However, the pipelines that we used to get them have stayed and evolved, and are regularly used to generate the data that trains all of our models. This experience changed my perspective on research time: it is time well spent if we learn about new things and develop tools that allow us to apply it quickly (if we also get some good results, that‚Äôs a bonus!). . ‚¨áÔ∏è Conclusions . There‚Äôs a famous quote that I‚Äôve often heard: ‚Äúto increase your success rate, double your failure rate‚Äù (it looks like Thomas Watson said this). This is as true in machine learning as it is anywhere else. . In the machine learning team at Monzo, we‚Äôve been thinking about this in terms of the speed of our work. This is not about cutting corners, being less rigorous, or skipping important steps of the work. Speed is about setting ourselves up to get results quickly, repurposing systems quickly, validate online behaviour quickly, and deploy to production quickly. . This post covers a few stories that shaped our views on speed - but this is always a work in progress. More posts to come! It is based on these two talks: . Speeding up Machine Learning Development April 2019, Xcede Data Science Networking Event. London. | Using Deep Learning to Support Customer Operations March 2019, ReWork Deep Learning in Finance Summit. London. | . You can find the slides from my talk here; reach out on twitter if you have any thoughts! .",
            "url": "http://0.0.0.0:4000/opinion/monzo/2019/08/13/Machine-learning-faster.html",
            "relUrl": "/opinion/monzo/2019/08/13/Machine-learning-faster.html",
            "date": " ‚Ä¢ Aug 13, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Neural approaches to conversational AI",
            "content": "Available: https://arxiv.org/pdf/1809.08267.pdf . The stand-out points (for me) were about problem formulation (e.g., single vs multi turn dialog) and that nearly all sections discussed that robust evaluation is a core problem in dialog systems: we don‚Äôt have generalised, automated, scalable, robust ways to determine that a conversation with a computer has gone well. . Chapter 1. Introduction . Intelligent dialogue systems are on the rise thanks to breakthroughs in deep and reinforcement learning. The research field is growing too, attracting folk from the NLP, IR, and ML communities. A number of tutorials and surveys have been published - this chapter is another survey, based on tutorials given at SIGIR and ACL 2018 - that hopes to provide a unified view of conversational / dialog systems. . The kinds of problems that dialog systems are expected to solve include: . Question answering: providing a concise, direct answer to user queries based on rich knowledge drawn from various data sources. | Task completion: accomplishing a task for the user (e.g., restaurant reservation, business trip planning). | Social chat: conversing seamlessly with the user and providing useful recommendations. | . Bots can be grouped into two categories: ‚Äútask oriented‚Äù and ‚Äúchitchat.‚Äù All of the personal assistants on the market today (e.g., Alexa) are task oriented and handle simple, one off tasks (e.g., reporting the weather). They are typically composed of four modules: (1) Natural Language Understanding, (2) state tracking, (3) dialog policy, (4) Natural Language Generation ‚Äì although there is now a trend to develop all four of these together using a neural network. Most of these will have access to external databases that supporting completing the task. . An example of a chat-oriented bot is XiaoIce - there‚Äôs an overview of it in the recent Import AI newsletter. Their primary goal is often to be ‚Äúcompanions to humans with an emotional connection.‚Äù . The unified view casts dialog as an optimal (hierarchical) decision making process. The top level is about picking the right agent (e.g., question answering vs. meeting scheduling), the lower level is about picking the right action. These can be thought of as options over a Markov Decision Process, which fits well with reinforcement learning. . A common metric to use as a reward function is CPS: Conversation-turns Per Session. There are two views on this metric: it sometimes needs to be minimised (complete a task quickly) and other times needs to be maximised (engage in long/meaningful conversations). The can be combined by reasoning over multiple sessions: being able to complete a task quickly (small short-term CPS) means that users will find the dialog system useful and will use it often (large long-term CPS). . Neural-based NLP tasks are usually performed in three steps: (1) encoding user input (words) into vectors, (2) reasoning in the vector space to generate a response, (3) decoding the response into a symbolic space (back to words). End-to-end training of neural nets results in a tighter coupling between the architecture and the application ‚Äì the focus shifts away from having the right components for language processing (e.g., parsing, context reasoning, etc.) towards having the right architecture. . Chapter 2. Machine Learning Background . This chapter is an overview of ML. I‚Äôm only going to write notes about bits that are specific to conversational systems. . Widely used neural network layers for text classification: . Embedding layers: mapping each word to an m-dimensional vector. | Fully connected layers: performing linear projections | Convolutional-pooling layers: forms a local feature vector of a word and its adjacent words (within a fixed sized window). | Recurrent layers: map the text to a dense/low dimensional vector by sequentially and recurrently processing each word. | . Two examples: Deep Structured Semantic Model and Seq2Seq. . Chapter 3. Question Answering and Machine Reading Comprehension . Question Answering (QA) allows a user to query a large scale knowledge base/document collection in natural language. They differ from search engines in that they aim to provide a concise and direct answer rather than a ranked list of results. QA is split into single-turn and multi-turn, with the latter being an emerging research topic. . Knowledge base (KB) question answering has been developed based on symbolic approaches ‚Äì which has been hard to scale. KBs are sometimes known as knowledge graphs - entities are nodes and relations are directed edges between them. Finding an answer to a question is based on semantic parsing: mapping a question to a ‚Äúmeaningful‚Äù representation, finding the paths in the graph that match the query, and then retrieving the nodes. This approach has two challenges: . Paraphrasing: there are many ways to ask the same question. Embeddings have been a useful way to tackle this. | Search complexity: number of candidate paths grows exponentially with the path length. | . Embeddings map entities and relations to continuous vectors in a (neural) space: this space can be viewed as a hidden semantic space where various expressions with the same semantic meaning map to the same vector. There are various ways (e.g., ReasoNet using DL and DeepPath using RL) to perform multi-step knowledge based reasoning using these vectors. . For multi-turn QA, users need to interactively compose (complex) queries, where the agent can ask clarifying questions. The Entropy Minimization Dialogue Management strategy picks the next question to ask based on the attribute that has the most entry in the remaining KB entries. . Machine Reading Comprehension is the task of answering questions given a set of text passages. Answers are defined as a span of text in (one of the) passage(s). There are many datasets (e.g., SQuAD) that are fueling research in this area. As with approaches above, this is usually done in 3 steps: encoding, reasoning, and decoding. . Encoding: Popular pre-trained embeddings include word2vec and GloVe, and can be enhanced with part of speech tags. Contextual embeddings (e.g., ELMo and BERT) uses contextual cues from surrounding words to refine the embeddings. | Reasoning: can be done in a single-step (match the question and document only once) or multi-step. | . Many of these systems are developed as single-turn. To make this conversational, the system also needs to include (encode) the conversation history. . Chapter 4. Task-oriented Dialogue Systems . Completing a task is different from question answering ‚Äì they are typically domain dependent (e.g., book a hotel room). There are a variety of types, including slot-filling dialogue: where the machine needs to collect information from the user in order to complete the task. . The interaction between a dialog agent and a user mirrors the interaction between an RL agent and the environment. An appropriate reward function in this domain would give a high reward for completing the task and a small penalty for each required intermediate turn. In practice, reward functions are often a linear combination of a subset of the following: . Task completion success | Time elapsed (number of turns) | Coherence, diversity, personal style(s) | . For RL, researchers have developed user simulators for RL agents to converse with ‚Äì since conversing with real users is expensive. The simulators could be based on a user agenda or based on models/data. Ultimately, many evaluations rely on recruited users (small scale) or A/B tests. . Slot filling problems also rely on dialogue state tracking ‚Äì containing all information about what the user is looking for at the current turn of the conversation. This is the input information to the dialogue policy which decides what action to take next. For example, see this challenge. . An example dialogue policy is DQN, which takes as input an encoding of the current dialogue state. It outputs a vector who entries correspond to all the possible dialogue-act/slot pairs that can be chosen by the system. Learning the policy from scratch takes a lot of data, but can be sped up by using expert-generated dialogues, imitation learning, or replay buffer spiking (warm start). In online settings, an agent has to balance between exploit (maximise reward) and explore (discover better alternatives) ‚Äì exploration when neural nets are used is an active research topic. This is made even more challenging when needing to extend the agent‚Äôs domain while serving users (adding more intents/slots). . Dialog can often be decomposed into a sequence of related sub-dialogues that each focus on a single topic. For example: booking a flight, hotel and car rental for a single trip. These are referred to as composite-task dialogues, where subtasks may have interdependencies. On the other hand, if a dialog can have more than one broad subject matter (e.g., hotels and restaurants), this is referred to as multi-domain dialogue. . Natural Language Generation (NLG) aims to convert a communication goal into natural language form. Rule and template based approaches are the most common ways to achieve this. Newer approaches (corpus-based) aim to optimise generation from a corpora of text. There are a number of LSTM based approaches that have been explored here. . All of these modules in a dialog system are often optimised separately. If all of them are differentiable, then the whole system can be optimised by backprop. Supervised and reinforcement learning have both been applied to this scenario. . Beyond slot-filling dialogue: . Information seeking dialog | Navigation dialogue | Multimodal dialogue with non-verbal inputs | Mixed initiative and negotiation | Multiple-parties | . Chapter 5. Fully Data-Driven Conversation Models and Social Bots . End to end systems do not rely on expert knowledge and do not have the traditional components described in the previous chapter. These are used more often for chit chat scenarios rather than task completion. Many of these systems are inspired by statistical machine translation. . Early systems represented the data as query/response pairs, which limited the context of the responses. Researchers have since explored RNN/LSTM based approaches that exploited a longer context. Other approaches such as the Hierarchical Recurrent Encoder-Decoder (HRED) aim to exploit longer-term context. Attention based models allow for conditioning on parts of the sentence that are relevant for prediction. . Challenges in these systems: . Response blandness - responses are often bland, deflective, uninformative (e.g. ‚ÄúI don‚Äôt know‚Äù). A variety of methods, including GANs, have been explored to increase the diversity of responses. | Speaker consistency - responses are incoherent, where the system contradicts itself. This is often a result of a one-to-many of examples in training data (e.g., there are many answers to ‚Äúhow old are you?‚Äù). | Word repetitions - since it is not clear how often a specific word or phrase should be repeated in the output. | Response appropriateness - struggling to produce names and facts that has appropriate semantic content. | . Grounded conversation models aim to help to effectively converse about a user‚Äôs environment, by factoring in additional information (e.g. using images from the environment). . Supervised learning in end to end conversation training is hard because human-human conversation training data is very different from online human-computer scenarios. These approaches also optimise for an immediate reward rather than a long-term one. Reinforcement learning in this domain is hard because of the difficulty of coming up with the right reward function. . Researchers have tried using social media data (e.g., twitter or reddit) to train these systems. They usually need to reconstruct dialogues from the posts, and sometimes run into trouble with API limits. Evaluation is just as challenging as previous sections ‚Äì it‚Äôs common to use human raters, alongside metrics like BLEU, ROUGE, and METEOR. There is research that shows that human ratings and these metrics do not correlate. . Chapter 6. Conversational AI in Industry . Gives a number of examples, e.g. Bing QA, Siri, Google Assistance, Alexa, Cortana, etc. . Most notably, this section highlights differences between research and application: . Scale and quality of text | Latency | User experience | . Many of the actual chatbot examples do not use any AI at all, and fully rely on handcrafted rules. .",
            "url": "http://0.0.0.0:4000/paper/2018/12/29/neural-conversations.html",
            "relUrl": "/paper/2018/12/29/neural-conversations.html",
            "date": " ‚Ä¢ Dec 29, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Using data to build a better help screen",
            "content": "Originally published on monzo.com. . Our customer support team are always on hand if you need to chat with a human, but that isn‚Äôt the only way to find answers: the help screen now has over 400 pages that answer common questions, from activating your card to travelling abroad. . As we work to make sure we can keep giving you world-class support as we scale, we‚Äôve been using data to make changes to the help screen so you can find your own answers faster. . Measuring how helpful we¬†are . If you scroll to the bottom of any page in the help screen, you‚Äôll see a button that says ‚ÄòI can‚Äôt find what I‚Äôm looking for.‚Äô If the page you‚Äôre reading didn‚Äôt have the right answer, clicking it will put you in touch with someone from our customer support team. . When you do that, it also gives us a powerful signal that the page you were reading wasn‚Äôt good enough to answer your question. If you don‚Äôt click the button, that‚Äôs another useful signal, that maybe you did find all the information that you needed. . We use these signals to prioritise the help pages we need to improve: if lots of people have to start a chat after reading a certain article, we know we need to make it clearer or add some extra information. We‚Äôve set up a dashboard that tracks each page‚Äôs performance in terms of ‚Äòconversion to chat,‚Äô which lets us quickly identify pages that we need to improve, and measure any changes after we update a page. . We also measure how people behave when they use the help screen. We call a set of interactions with the help screen a ‚Äòsession,‚Äô and each one can include one or more of the following actions: searching, seeing a suggested help page, navigating the categories, and starting a conversation through in-app chat. . From tracking this data, we found: . Customers who used our search functionality were much less likely to get in touch with customer support. | If people opened an article but didn‚Äôt find what they needed, they were very unlikely to go back and search again. Most people opted to chat to someone in our customer support team straight away. | . These insights told us that searching and navigating help pages isn‚Äôt as seamless as it should be. So we‚Äôve been using them to make the help screen better! . Easier navigation with related¬†articles . Many of our help pages are about very similar topics, but each one‚Äôs relevant in a slightly different scenario. For example, we have separate articles for ‚ÄúI forgot my PIN,‚Äù ‚ÄúMy PIN is blocked,‚Äù ‚ÄúI don‚Äôt know my PIN,‚Äù and ‚ÄúI want to change my PIN.‚Äù But if you search for ‚ÄúPIN,‚Äù we‚Äôll show you most of these results. . In the past, if you clicked on one of these search results but didn‚Äôt find the answer you were looking for, you‚Äôd need to go back into your search results to browse the other answers, or return to the help screen and start all over again. . Because this process wasn‚Äôt easy, people were more likely to tap ‚ÄòI can‚Äôt find what I‚Äôm looking for‚Äô and get in touch with customer support instead. . What we built . So, to make it easier to find the right answer after landing on the wrong search result, we‚Äôve added ‚ÄòRelated articles.‚Äô We now list other pages that are semantically related to the one you‚Äôre currently reading. . . To work out whether two pages should be related, we‚Äôve built a system that makes it easy for us to run A/B tests. It means we can show different customers related articles that had been computed in different ways, and work out which method is most successful. . So far, we‚Äôve tried some relatively simple methods: . Manually categorising articles: We tried relating articles using the categories we set manually when we write content for the help screen. Two articles are related if they also appear in the same categories that you can navigate in the help screen. | Using machine learning: We also tried relating articles using an open source machine learning model that was trained on millions of words from a Google News dataset. | . Does it work? . In a short series of experiments, we learned that showing customers related articles significantly lowered conversion to chat across the board: people were able to find the answer they needed, even if they landed on the wrong one at first, without needing to chat with someone in customer support. . Interestingly, using machine learning showed no significant improvement compared to grouping articles manually by category. There are a couple of reasons why this may be the case. First, we invested a lot of time to make sure that pages were grouped together properly. Second, we used an open source machine learning model to compute related articles, rather than using our own data. We think training them on our own data might produce better results. . Integrating search throughout the help¬†screen . Our analysis of help screen sessions also showed that customers who use search were approximately 20% more likely to find the answer that they needed, without having to get in touch with customer support. . Because search seems to be the most effective way for people to find the answers that they need, we decided to find out how we could help customers benefit from our search results, without forcing them to actually search. . What we built . We‚Äôve been running some experiments and making changes to the help screen, like putting the search bar at the centre to make searching more prominent. . We also ran an experiment that showed you search results as you typed a message to our customer support team. For example, if your started telling us that you forgot your PIN, we‚Äôd suggest a series of relevant articles. Under the hood, these suggestions use the same machine learning service that powers search on the help screen. And we didn‚Äôt add any additional friction to the process of getting in touch, so you‚Äôd still be able to start a new conversation. . . Does it work? . We found that suggesting relevant articles as you typed helped you find the answer you needed as effectively as searching yourself, so we added the feature to the apps! . What‚Äôs next? . Understanding what you talk to us about . We‚Äôve recently begun another round of analysis, to find out what customers talk to us about when the help screen hasn‚Äôt been helpful. We‚Äôll use this analysis to help us identify areas that the help screen doesn‚Äôt cover at all, and understand how we can improve specific help pages even more. . More nuanced machine learning . We‚Äôve started prototyping various ways that we can use the data we collect to tweak the existing machine learning features on the help screen. . Right now a lot of our machine learning focuses on the content of each page, and doesn‚Äôt factor in the implicit feedback that we get as you view and click on articles. The actions you take when you use our search results are powerful indicators of their quality. . For example, imagine if every time a customer searched to find out what to do if they forgot their pin, they always ignored the first two results and picked the third answer on the list. This would tell us that we should promote the third result to the top of the list, so everyone could find it right away. . Surfacing other kinds of content . Right now our help search algorithm surfaces help articles with text. But in the future, we want it to be able to show you other types of results as well. For example, if your question‚Äôs been answered by someone in the community, we‚Äôd like to link you to their post. . Making better suggestions . We‚Äôre also looking to experiment with machine learning in other areas altogether. For example, the suggested articles on the help screen are currently computed using a number of rules, and we have a hunch that personalising these results with machine learning could help us provide better suggestions. . Preempting questions about specific transactions . We‚Äôve noticed that lots of conversations with customer support are about specific transactions, which is why the help screen now shows your last three transactions. . But we‚Äôve also started to explore how we can identify which transactions you‚Äôre likely to have questions about, and link them with useful answers. . All week we‚Äôll be sharing an insight into how we‚Äôre working to make sure Monzo‚Äôs ready to support one billion customers! Join the discussion on the community forum and keep an eye out on the blog and Twitter for updates üëÄ. .",
            "url": "http://0.0.0.0:4000/monzo/data-science/2018/08/01/Using-data-to-build-a-better-help-screen.html",
            "relUrl": "/monzo/data-science/2018/08/01/Using-data-to-build-a-better-help-screen.html",
            "date": " ‚Ä¢ Aug 1, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "The first 90 days (book review)",
            "content": "Most of the blog posts that I read about tech show how thorough typical workflows are: we take a systematic approach when trying to solve problems in software development, data science, and product management. So why not be just as systematic when thinking about how we work within our team and company? . I recently had the opportunity to think about this in some time that I had between jobs. As part of this, I read a book called The First 90 Days. There are a few key ideas in this book that captured a very systematic approach to joining a new team and organisation; I feel that being aware and mindful of these points helped me in the first 90 days of my new role. In this post, I‚Äôm going to summarise five broad ideas that I picked up while reading it. . Understand what kinds of problems you gravitate towards . The premise of the book is that you are either joining a new organisation or have been promoted into a new role that has a wider scope than the one you had before. Both of these situations are often met with great enthusiasm‚Ää‚Äî‚Ääand a desire to immediately get to work and start ‚Äòdoing stuff.‚Äô Who doesn‚Äôt want to roll up their sleeves and dive right in? . Different parts of the book, however, describe that ‚Äògetting things done‚Äô‚Ää‚Äî‚Ääsay, creating a new product feature or changing a process in an organisation, requires solving different types of problems at the same time. These include technical (e.g., building systems), political (e.g., prioritising conflicting strategies), and cultural (e.g., maintaining team dynamics) problems. A key point that I picked up through some of the early exercises in the book is that we all naturally gravitate towards a certain type of problem over others‚Ää‚Äî‚Ääperhaps because it is the type of problem that we have worked on in our previous role. . The issue with naturally gravitating towards a certain type of problem is that (a) you are only working on one third of what it really takes to ‚Äòget things done‚Äô in an organisation, (b) you can blindside yourself to the real problems that need solving, and (c) you can alienate your new colleagues who do not gravitate towards the same kind of action that you do. . A spurious scenario that would capture this would be: imagine learning about a system that your team maintains, and immediately seeing that there are some huge problems‚Ää‚Äî‚Ääbugs or technical debt‚Ää‚Äî‚Ääthat you know how to fix. These are technical problems, and perhaps, as an individual, you gravitate towards these. However, these technical problems may have originated from how the team made decisions or how they worked together before you joined. By immediately diving in to squash bugs, you may not only fail to solve the root cause of the technical problem, but you may also fail to make a positive contribution to other efforts at resolving it that focus on a different angle of the problem. . Meet stakeholders, natural historians, and cultural interpreters . Joining a new organisation naturally involves meeting a lot of new people. In most cases, we‚Äôll be quickly introduced to people that the book characterises as vertical relationships: people that will report to you and stakeholders that you will report to. . The First 90 Days discusses other types of people to look out for. An obvious starting point are people who may be characterised as horizontal relationships‚Ää‚Äî‚Ääyour neighbours. If I recall correctly, there‚Äôs a mention of something along the lines of ‚Äúyou don‚Äôt want to be meeting your neighbours for the first time when your house is on fire‚Äù: horizontal relationships may not help on a day to day basis, but they may be critical at some point in the future when things are going wrong. . There are two other types of colleagues that we should also be on the lookout for, that no one will explicitly introduce you to. . The first group are natural historians. These people will be able to give you the background to a team or project‚Ää‚Äî‚Ääthey broadly answer the question ‚Äúhow did we get to where we are today?‚Äù I recall some quote that said that joining a company will be a new chapter in your life, but it will not be the first chapter in that company‚Äôs story. There‚Äôs a common saying that captures the importance of meeting natural historians‚Ää‚Äî‚Ää‚Äúthose who cannot learn from history are doomed to repeat it‚Äù‚Ää‚Äî‚Ääwithout having the right historical context, your initial work in the team may be repeating mistakes from the past. . The second group are cultural interpreters. These people will be able to onboard you into the cultural practices of your new organisation. Where are decisions made? What amount of open conflict is expected or tolerated, and how is it resolved? Why do certain teams work well together (or not)? Having someone who can openly give you some insight into these can let you adapt quicker to your new environment. For example, your previous company may have used meetings as a forum to fiercely debate the pros and cons of a particular decision before making it. Your new company, instead, may use meetings to announce decisions that have already been made. If you come into your new job expecting (and trying to) fiercely debate a decision that has already been made, then your efforts to steer that decision are coming too late and will have less impact. . Learn systematically by asking the same questions many¬†times . The First 90 Days has a huge focus on learning ‚Äîone of the biggest take-aways from the book is that we should set time to learn proactively and systematically during this transition phase. . In many companies, there will be an onboarding process or set of welcome/introduction-to sessions. These sessions tend to focus on business as usual (‚Äòwhat problems are we solving?‚Äô), perhaps broken down by team. The other perspectives that are less frequently covered‚Ää‚Äî‚Ääand therefore are up to you to proactively figure out‚Ää‚Äî‚Ääare about connections (‚Äòwho will I work on solving these problems with?‚Äô), expectations (‚Äòwhat part of this problem will I be working on?‚Äô) and culture (‚Äòhow do we collaboratively solve problems?‚Äô). While all four of these are equally important, the book places the responsibility of learning about these on you‚Ää‚Äî‚Ääit‚Äôs your job to learn about how the pieces of your organisation fit together, rather than their job to tell you. . One-to-one meetings are emphasised as a massive opportunity to learn and get quick, varied insight into a company‚Äôs dynamics. I used to treat these kinds of meetings as a way to get to know a person (and still believe that this is an important part of a 1:1 meeting), but I would never really come prepared with specific questions. The book suggests different ways of approaching this. For example, you could focus on challenges and opportunities: what are our biggest challenges? Why are they challenges for us? What are our most promising opportunities, and what do we need to change to achieve them? What would you focus on and why? Alternatively, you could focus on the past, present and future: Why was this done this way? How are things going now? Where would we like to be in the future? . The benefit that I found from having a structured set of questions that I asked everybody in my first 1:1 with them went beyond the information that I learned from their answers: I got insight into the variance between answers to the same question. This variability has more information about the company‚Äôs structure than anything else that I‚Äôve come across to date‚Ää‚Äî‚Ääit encodes where teams are aligned and focused, and where frictions may be emerging. . The other question that I tried to ask everybody at the end of each meeting was ‚Äúwho do you think I should meet next?‚Äù Keeping track of this was a very interesting experiment; my hypothesis is that people will frequently point you towards others who they feel have a large amount of influence or insight. . Diagnose the organisation as a¬†system . The previous points have covered (a) knowing yourself, (b) meeting others, and (c) structuring how you learn. Based on these, the book talks about making an assessment of what ‚Äòstate‚Äô your new team or company is currently in, which is a key part of understanding how you fit in and what you should do next. To do so, the book talks about the STARS model. . This acronym stands for different scenarios that a team can find itself in, and captures what kind of action is needed: . Start-up: a team that is assembling the initial capabilities (people, technology, etc.) in order to get a new project off the ground. You will be dealing with _bootstrapping-_type of problems and will need to work on maintaining focus. | Turnaround: a team that is trying to save a project that is widely acknowledged to be in deep trouble (a ‚Äòburning platform‚Äô situation). Efforts will need to focus on re-energising a demoralised team and making effective changes quickly. | Accelerated growth: a team that is scaling extremely quickly or managing a rapidly expanding business. Onboarding many new team members at the same time is a challenge to maintaining productivity and culture, particularly when structures that were previously unnecessary become essential. | Realignment: a team that faces problems which will drift it into danger. It is not quite yet at the _turnaround _stage, but complacency will take it there; the main difference may even be that the team doesn‚Äôt recognise that changes are needed. | Sustaining success: a team that needs to preserve the vitality of recent successes and take it to the next level. While this is generally a great situation to be in, your arrival marks a change‚Ää‚Äî‚Ääand so finding ways to maintain success given that change become a major area of focus. | . An interesting detail that is mentioned is that within the same organisation, different teams may be in a different ‚Äòstate‚Äô of the STARS model. Having a broad assessment of where your and other teams are gives you insight into how you can start working, both within your team and with other teams. . Avoid common pitfalls in your first contributions . Finally, now that we have learned about ourselves, met others in a structured way, learned about the company, and made an assessment about where things are, we can start thinking about what we will do first. . The First 90 Days has plenty of anecdotes about people coming into a new position and falling prey to some kind of pitfall which tarnishes how their colleagues view them‚Ää‚Äî‚Ääand even ultimately defines their success or failure in that role. Many of these failures are attributed to the points above, i.e. not spending time learning about your new oganisation and role. . I noted down a few specific pitfalls. These include: . Coming in with ‚Äúthe‚Äù answer. Approaching a problem in your new role with a specific answer can be a really negative experience for your colleagues. For example, imagine that you leave a large tech company to join a small tech start-up, and every time that a problem arises, you say: ‚Äúin [large tech company], we did X, and it worked.‚Äù In these cases, you are not only failing to adapt to where you currently are (a small start-up), but could also be implicitly undermining the opinions of colleagues who do not have experience at an equivalent large tech company. | Change for change‚Äôs sake. It‚Äôs often tempting to do something‚Ää‚Äî‚Ääafter all, it is easier to feel that we are being productive when we are doing something rather than do nothing. Making a change in order to satisfy our ego, rather than solve a problem, is dangerous. Instead, the book hints at looking for early wins in areas that those who you will work closely with care deeply about. | How you contribute matters as much as what you do. When we come into a new role, our ‚Äúfresh‚Äù eyes may quickly allow us to identify some quick wins‚Ää‚Äî‚Ääsay, a product feature change that may significantly shift a metric that everyone cares about. However, if you achieve this early win but, in order to do so, you break cultural norms and practices in your team or undermine others, your quick win may not be so useful after all. | . Conclusion . I was initially fairly sceptical that I would find value from what seems to be a popular book for business leaders while moving from one data team to another. I therefore treated the book like a textbook‚Ää‚Äî‚Ääreferencing different sections, jumping back and forth between them, and skipping others. . Now that I have spent more than 90 days in my new role, a lot of the issues that this book highlighted feel useful. It‚Äôs clearly too early to say that I have succeeded or failed by using them‚Ää‚Äî‚Ääbut equally, they nearly seem obvious in retrospect, and have become a reference point that I go back to from time to time. . Starting a new job is always an exciting time. Equally, it is an opportunity to proactively shape how you work with others, and how they see you. Thinking about an organisation as a system that can be characterised around various principles, qualities, and structures, changed how I viewed what it means to work‚Ää‚Äî‚Ääand I hope that these notes may help you do the same. .",
            "url": "http://0.0.0.0:4000/book/2018/05/27/The-first-90-days.html",
            "relUrl": "/book/2018/05/27/The-first-90-days.html",
            "date": " ‚Ä¢ May 27, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Industry stories about machine learning",
            "content": "Last week, I went to the PAPIs.io Europe 2018 conference, which was held in Canary Wharf Tower in London. The conference describes itself as a ‚Äúseries of international conferences dedicated to real-world Machine Learning applications, and the innovations, techniques and tools that power them‚Äù (and, from what I gather, the name papis comes from ‚ÄúPredictive APIs‚Äù). I went down on the Thursday, the day that was dedicated to ‚ÄúIndustry and Startups,‚Äù and took some notes on what I saw. Here‚Äôs a quick summary! . . ML infrastructure with Kubernetes, Dask, and¬†Jupyter . The morning keynote was by Olivier Grisel, who is probably best known for his immense contributions to scikit-learn‚Ää‚Äî‚Ääand therefore anyone who does machine learning in Python is indebted to him! His slides are online here . In this video, starting around 23 minutes in, he shows how to set up your own machine learning infrastructure using three main open source components: Kubernetes (a cluster orchestration system based on containers), Dask (a tool to parallelize python jobs that is deeply integrated with existing python libraries like pandas/numpy), and Jupyter Notebooks (the well-known web application for interactive development). . Specifically, he was using minikube to run kubernetes locally and jupyter hub to manage multiple users on a single jupyter server. The first example that he showed was somewhat trivial (e.g., incrementing a counter), but this allowed him to describe, in depth, how the computation was being distributed and executed. The second example showed how to run a grid search to find the best parameters for a Support Vector Machine, using dask-joblib to run this on the cluster. . One of my favourite lines from the Q&amp;A was an off-hand comment that touches on developing ML systems: ‚Äúyou shouldn‚Äôt do everything in a Jupyter notebook‚Äù (because that‚Äôs not great for maintenance). . Prediction at the edge with¬†AWS . The second talk was by Julien Simon (who blogs here), an AI/ML evangelist from Amazon. Starting at about minute 59 in this video, his talk focused on running machine learning predictions outside of data centers (‚Äòat the edge‚Äô‚Ää‚Äî‚Ääon cameras, on sensors, etc.). Achieving this entailed, perhaps unsurprisingly, going through a whirlwind tour of various AWS services that are available for machine learning systems. These included: . Defining and manipulating models with Gluon and MXNet; | Building and training models with SageMaker; | Using Lambda to write on-demand prediction functions; | Deploying the code to edge devices using Greengrass. | . His talk closed with a demo of DeepLens, the ‚Äúworld‚Äôs first deep learning enabled video camera for developers‚Äù which was recently launched, showing real-time object detection in action. . Managing the gap between the engineer and data scientist role . One of the talks that touched on very interesting topics was by Beth Logan from dataxu, a data-driven advertising company. She described how they go about developing and automating the deployment of a machine learning pipeline (hence the title ‚Äòchanging tires while driving;‚Äô the talk is online here) to support various applications in the advertising domain. . Moving away from the ML itself, there were some interesting points made about how to manage what a ‚Äòdata scientist‚Äô does vs. what an ‚Äòengineer‚Äô does, in order for each role to play to their strengths. In effect, this was about letting data scientists develop and iterate on models, while leaving all of the job of productionising and scaling them to engineers‚Ää‚Äî‚Ääwho also had to demonstrate that the production implementation performed as expected. . The intersection of data science and engineering is a topic that I could probably write an entire blog post about; suffice to say, we had a discussion at the end about whether such a divide is the ‚Äòright‚Äô way to do this, and how each discipline can upskill the other while collaborating. . Pipeline jungles in machine¬†learning . The next talk was by Moussa Taifi from Appnexus, another company that deals with digital advertising. He discussed building various kinds of pipelines for click prediction, a common task in online advertising. . Moussa touched on a number of practical aspects of developing pipelines while going back and forth between research and production. These included getting into trouble with reproducing results once pipelines are overly complex (‚Äòjungles‚Äô), model versioning for experiments, avoiding common issues like time travel (training on data that was created after the data in the test set), and whether it is better to go for systems with just-in-time data transformations and feature extraction vs. building models from a fixed set of features that are precomputed, regardless of the task at hand . Building a culture of machine¬†learning . Lars Trieloff gave a high-level talk about nurturing a culture of AI inside of Adobe‚Ää‚Äî‚Ääfocusing specifically on Adobe Sensei. His talk spanned three broad areas: brand, vision, and technology, and how the three needed to gel in order to foster a culture of machine learning within an organisation. Interestingly, he also touched on responsibility‚Ää‚Äî‚Ääand how all employees at the company needed to go through training and approval process when developing new machine learning tools. . Feasibility vs return on investment in machine¬†learning . Poul Petersen from BigML gave a talk about how the company predicted 6 out of 6 of the 2018 Oscar winners‚Ää‚Äî‚Ääsee this blog post which has some similar content. Oscars aside, he made an interesting observation about how to prioritise machine learning projects, based on comparing their feasibility and projected return on investment. If both are low, this is clearly a no-go area; if both are high, this is a no-brainer that you should already be working on. The remaining two categories were ‚Äòpostponable‚Äô (low ROI, highly feasible) and ‚Äòbrainers‚Äô (high ROI, not currently feasible). . He gave a similar analogy for the progression of which algorithms were his go-tos, depending on what stage of development a particular system was at: early stage, requiring rapid prototyping (logistic regression), mid stage, where you have a proven application (random forests), and finally late stage, where tweaking performance becomes critical (neural networks). . Startup pitches &amp; panel‚Ää‚Äî‚Ääthe European AI landscape . The startup pitches were dispersed across the day. The ones I saw here: . Logical Clocks: who have an enterprise machine learning platform called Hops that aims to improve the productivity of data scientists. | Antiverse: aim to enable antibody drug discovery in one day using AI. | Tensorflight: automate property inspections by analysing satellite and aerial data using machine learning. | Teebly: offers a single point of contact for a business‚Äô clients, automating all of the various ways that they can get in touch with you. | . Some of these startups participated in a startup battle at the end of the day, which was judged by an AI. While I was somewhat sceptical when I first heard about this, it was actually very entertaining. Each startup took turns being asked questions by an Alexa, with questions ranging from the size, experience, and structure of the team, and were scored on a variety of factors. The winners took home ¬£100k! . The startup panel, instead, took a retrospective perspective‚Ää‚Äî‚Äälooking back at what worked for Twizoo (which was acquired by Skyscanner, shortly before I left), prediction.io (which was acquired by Salesforce after being founded in London), and Seedcamp. The recurring theme was the importance of focusing on the customer, rather than the machine learning: technology is an enabler to solve a customer‚Äôs pain, and the abstract machine learning problems that need to be solved along the way are nearly superfluous compared to the customer‚Äôs need. . There are many different take-aways from this day. One that stands out is that the European startup landscape in the machine learning space is still thriving and growing. And, indeed, Libby from Project Juno AI announced that they are starting another round of mapping this landscape‚Ää‚Äî‚Ääa project that‚Äôs definitely worth checking out and contributing to. .",
            "url": "http://0.0.0.0:4000/conference/2018/04/15/Industry-Stories-about-Machine-Learning.html",
            "relUrl": "/conference/2018/04/15/Industry-Stories-about-Machine-Learning.html",
            "date": " ‚Ä¢ Apr 15, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Five lessons from building machine learning systems",
            "content": "In January 2017 I left Skyscanner, where I was a Senior Data Scientist. Like I did with a previous post a couple of years ago, I thought I‚Äôd summarise my journey by collecting some thoughts and lessons learned from the projects that I was involved in‚Ää‚Äî‚Ääwhich all focused on building different machine learning features for the Skyscanner app. . This post does not have any breakthrough insights on specific algorithms or technologies. Instead, I focus on a few broader aspects of data science: definining methodologies, dealing with uncertainty, building pipelines, and fostering a culture of machine learning. . 5. . My first squad designed, built, and ran some experiments with a pipeline that uses different kinds of unsupervised ranking in order to compute localised destination recommendations. We set out to build a new system, and so we didn‚Äôt know a lot of things: what data should we use? What kind of ranking algorithm? Where will the feature appear for users, and how will they interact with it? . As a data scientist working in a new problem space, there‚Äôs certainly an initial urge to reach for the latest algorithm right away. Similarly, as engineers working on a new system, there‚Äôs an instinct to automate and scale everything within the problem‚Äôs scope. However, that is a far cry from what we needed in order to get the initial system up and running: what we really needed was a foundation that we could use to run experiments. . Our focus shifted towards designing a pipeline that would allow us to run all kinds of experiments: for example, experiments that changed input data type or size, parameters, or switch algorithms ‚Äîall aimed at demonstrating how much we were incrementally making a difference, rather than proving that we had solved the problem outright. You can read more about the pipeline‚Äôs initial architecture here. . The process itself encompassed many small details and tasks that the data science commmunity does not tend to celebrate (at least, not as much as the newest algorithm!). The algorithm itself was a very small piece of the whole ecosystem‚Ää‚Äî‚Äämuch like the little black box in this image: . . When I presented this work in the industry session at the ACM Recommender Systems conference (the abstract pdf is here), I recall feeling a form of impostor syndrome‚Ää‚Äî‚Ääfrom an algorithmic perspective, this system is very simple compared to the typical academic papers that are presented at the conference. However, it works! You can see this feature today in the iOS app if you navigate to the ‚Äòexplore‚Äô tab, where there are various groups of destinations. . Lesson: Getting a data pipeline into production for the first time entails dealing with many different moving parts ‚Äîin these cases, spend more time thinking about the pipeline itself, and how it will enable experimentation, and less about its first algorithm. . 4. . A squad that I spent many recent months working with focuses on improving the search experience in the app. As part of this, we ran experiments that aimed to explore how learning to rank flight itineraries can do better than sorting itineraries by price. . This work threw us into the tricky domain of evaluating supervised machine learning models from a cost/benefit perspective. What sort of offline evaluation improvement warrants running an online experiment? Is an improvement of x% in precision worth the work (and time) that it will take to run an A/B test? What is ‚Äúwrong‚Äù with this process if many models with better offline scores don‚Äôt perform better in online experiments? These are problems that I‚Äôve mentioned in a previous post; to date, I still don‚Äôt think that there are any definitive answers. When developing a model offline, we need to be able to measure its performance with particular metrics (like precision); however, the actual user behaviour that the model aims to support is often not captured by the same metrics. Similarly, online (A/B) testing everything is infeasible‚Äî the best way we have to prioritise what to test next, in supervised learning, is to run offline evaluations and then discuss their results with other stakeholders. . Lesson: This highlights the importance of considering the big picture and debating the pros and cons of running a specific machine learning experiment with various stakeholders. Accuracy is not enough! . 3. . The itineraries mentioned above are only one part of the app‚Äôs search experience; indeed, the app‚Äôs search result interface is going beyond itineraries and towards being designed around the idea of widgets. For example, this tweet shows a screen shot of the app giving users suggestions to change their search dates. . An interesting problem that emerged from this is how to design the ‚Äúbest‚Äù layout for a given user‚Äôs search when various widgets are available. As I described in the keynote I gave at the 2017 AI Tech World conference (slides here), the main approach that we were exploring is the use of multi-armed bandits to learn to create layouts for different searches. Although this was the first time that I had worked with multi-armed bandits, I felt confident that the research literature and resources available online about this approach would be there to inform each step we needed to take. I was wrong! . One of the fascinating aspects of this work was that there are many practical decisions that needed to be made (e.g., what variability in user experience should be allowed) that are often taken for granted or altogether ignored in the research literature. . Lesson: the devil is in the detail. While machine learning does provide useful abstractions, there are many practical decisions that need to be made in a product that is driven by machine learning that govern how it works. . 2. . I‚Äôve seen the ‚Äúdata science is 80% data cleaning‚Äù phrase in all different shapes and sizes. It often comes up at Data Science meetups and conferences. However, one topic that I feel isn‚Äôt discussed as much as it should be is about working, as a Data Scientist, within and across multi-disciplinary teams. . A recurring conversation that I had with engineers, product managers, and designers started with: what is machine learning? Venturing into this hyped and alchemist-filled field can be both daunting and overwhelming‚Ää‚Äî‚Ääand it is unclear how to get the best result when intersecting with other discipline‚Äôs methods. How should data scientists factor in qualitative research results into their workflow? How does machine learning fit into experience design? . Naturally, a lot of practical questions emerge that do not have much to do with machine learning itself, but about how to understand and make use of the benefits that this tool could provide. My main take-away was yet another 80/20 rule: data science is 80% education and 20% implementation. . I mean this as a two-way street: I learned a lot by having regular 1‚Äì1s with product owners to understand how they do things: for example, here‚Äôs a great post by Andrea Sipos on the development of one of the app‚Äôs features. Similarly, I spent time trying to communicate how machine learning works and fits into this picture. After a few of these conversations, I wrote a blog post on machine learning for product managers; here‚Äôs a similar post by Fabien Girardin that discusses the implications of machine learning for designers. . 1. . There‚Äôs a great post by Monica Rogati that discusses the ‚ÄúAI hierarchy of needs;‚Äù the foundations that a company needs to build before it can reach the peak of advanced machine learning in production. The post describes how you need to be able to crawl (collect, load, move, transform data) before you can walk (experimentation, simple machine learning), and walk before you can run (use state of the art machine learning). . The projects that I‚Äôve described above all started and progressed to different stages of this hierarchy. However, just as software is never finished, it‚Äôs very challenging to claim that you are ‚Äúfinished‚Äù with the one part of the AI journey and will move up the hierarchy to the next step. . In other words, you won‚Äôt stop caring about being able to crawl well (count, load, transform data) when you have started caring about walking (experimentation). As I described in this twitter thread, building app features that use machine learning relied on standing on the shoulders of giants‚Ää‚Äî‚Ääfor example, the other teams that develop the infrastructure and experimentation services. . Wrapping Up¬†üåç . When I joined Skyscanner, I was the first data scientist based in the London office, at WeWork in Moorgate. Since I joined, Skyscanner was acquired by CTrip, it acquired Twizoo, and Trip.com joined forces with Skyscanner. Skyscanner moved into new offices near Tottenham Court Road, and there are now 5 London-based data scientists, more than 20 across all of the Skyscanner offices, and the team is continuing to grow. . Naturally, the projects I‚Äôve mentioned above are a very thin slice of the various places where data scientists at Skyscanner are working. In April 2017, we organised and hosted a series of public lightning talks as part of the London Data Science Festival that gave a glimpse into some other projects. At this event, Dima Karamshuk gave a talk on optimising content caching (here), Ruth Garcia gave a talk on evaluating ad quality (here), and Ryszard T. Kaleta gave a talk about machine learning with social data (here). . Suffice to say, if you follow Skyscanner Engineering you‚Äôre likely to run into more data science blog posts in the near future. For example, here‚Äôs a recent one from Sri¬≤ about multi-touch attribution. . Do you have any thoughts or want to share your own insights? Feel free to reach out to on twitter. . Thanks: . Ewan Nicholson, | Sri¬≤ Perangur, | Dima Karamshuk, | Ruth Garcia | Andrea Sipos | . for the feedback on this post, and for being generally awesome people. .",
            "url": "http://0.0.0.0:4000/opinion/machine-learning/skyscanner/2018/01/21/Five-lessons-from-building-machine-learning-systems.html",
            "relUrl": "/opinion/machine-learning/skyscanner/2018/01/21/Five-lessons-from-building-machine-learning-systems.html",
            "date": " ‚Ä¢ Jan 21, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "Deep learning & medical diagnosis",
            "content": "Over the last few months, there have been a number of announcements of research findings that claim that deep learning has been applied to, and often times immediately outperforms doctors in, a particular area of diagnosis. . I originally started this blog post to keep track of them‚Ää‚Äî‚ÄäI‚Äôm going to publish it as a draft that I expect to update on a regular basis. . What is deep learning in medical image diagnosis trying to¬†do? . Before diving into the specific results, I‚Äôd like to highlight that the approaches (so far) below share the same common pattern. I‚Äôve tried to summarize this into the following sentence: . Diagnosis via machine learning works when the condition can be reduced to a classification task on physiological data, in areas where we currently rely on the clinician to be able to visually identify patterns that indicate the presence or type of the condition. . To break this down into details: . Classification. The results are areas of medical diagnosis that can be reduced to a classification problem: given some data, diagnosis can be reduced to the problem of mapping that data to one of N different outcomes. In some cases, N = 2: the task is just to identify whether the data (e.g., an x-ray) displays the condition or not. Note that there are other problems (e.g., image segmentation) that can be tackled with deep learning, but I haven‚Äôt yet seen them used in isolation for diagnosis (as opposed to, say, analysis only). | Physiological data. The results below tend to be using medical imaging data or data from other kinds of sensors. The explosion of results in these areas is, in large part, ascribed to the creation of data sets (e.g., these ones) that are far larger than those that were previously available. A common approach to annotate a data set (e.g., label whether an x-ray contains a tumor) is to have a set of clinicians give their opinion and to collate the responses. | We rely on visually identifying patterns. The alternative to an automated diagnosis system would be to have an expert clinician look at your data (perhaps discuss it with some fellow experts) to determine the outcome. This point captures why deep learning should be successful in this area: deep learning automates the entire process of extracting patterns and learning relationships in this kind of ‚Äòunstructured‚Äô data. There are many non-medical applications of deep learning (e.g., face recognition) that have similar requirements; because of this, the tech is quite mature. Indeed, even models trained on medical images are now being open sourced. | . For great reviews of the area, check out the review papers and blog posts that I‚Äôve added in the references section at the bottom of this post. . Where should this approach not¬†work? . The pattern above gives some insight into areas where this approach should not currently work. Like this article mentions, deep learning does not tell us how patients should be treated or how well they will fare when being treated. However, specifically related to the points above, there are domains that: . Aren‚Äôt classification problems. If we don‚Äôt understand the disease well enough, we can‚Äôt create data to train any algorithm. For example, there are conditions that do not have a well-understood progression that can be enumerated into a set of stages. In these cases, it would be very challenging to build a reliable model to tell us what stage of progression a patient is at‚Ää‚Äî‚Ääbecause we don‚Äôt know what the stages should be. | Lack (or have subjective) data. If there is little or no data, we cannot train models. Granted, this is starting to change‚Ää‚Äî‚Ääthere are deep learning experiments that demonstrate learning from extremely small data sets. Where there is data, but it and/or patterns within it are subjective (e.g., the momentary experience of pain or stress), then I‚Äôd imagine that the approaches below would need to be re-imagined. | Do not rely on medical devices. Similarly, domains where a diagnosis cannot be derived by attaching the patient to some kind of machine and collecting a single ‚Äúsample‚Äù of data (e.g., requiring long-term tracking or diagnosis by exclusion). This could be because (a) we have not developed a means of detecting a disease‚Ää‚Äî‚Ääso, like above, more fundamental research is required, or (b) we have not developed viable products for long-term, non-invasive monitoring to collect the data that enables machine learning. | . List of Medical Conditions . My criteria for adding to this list: (a) a data set has been released, (b) research has been published,(c) a company or research group has written about work in progress, or (d) there are blog posts that describe tackling the problem. I‚Äôve sorted the conditions alphabetically. . Have I missed something? You can @ me on twitter and I‚Äôll add it. . Alzheimer‚Äôs . ‚Ä¶ ‚Äúis a chronic neurodegenerative disease that usually starts slowly and worsens over time.‚Äù Researchers in London have published a paper that reports using data from the ADNI to train a 3 layer neural network with a single convolutional layer that can predict whether an MRI scan is a healthy brain, a brain with mild cognitive impairment, and a brain with Alzheimer‚Äôs disease. . Arrhythmia . ‚Ä¶ ‚Äúis a group of conditions in which the heartbeat is irregular.‚Äù Researchers at Stanford have published a paper that reports that a 34 layer convolutional neural network they developed ‚Äúexceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor‚Äù (project page, blog post). . Autism . ‚Ä¶ ‚Äúis a neurodevelopmental disorder characterized by impaired social interaction.‚Äù A team of researchers published a paper, which reports that ‚Äúa deep learning algorithm primarily using surface area information from brain MRI at 6 and 12 months of age predicted the 24 month diagnosis of autism in children at high familial risk for autism‚Äù(via @datarequena on twitter). . Breast Cancer . ‚Ä¶ ‚Äúis a cancer that develops from breast tissue.‚Äù DeepMind Health has published a blog post where they announced that they have teamed up with Cancer Research UK to analyse and apply machine learning on anonymised mammograms from 7,500 women. . Dental Cavities . ‚Ä¶ ‚Äúis a break down of teeth due to acids made by bacteria.‚Äù Researchers at ParallelDots have published a paper that reports that a 100+ layer convolutional network that performs pixel-level binary classification of teeth radiographs (has caries/does not have caries). . Diabetic Retinopathy . ‚Ä¶ ‚Äúis a medical condition in which damage occurs to the retina due to diabetes.‚Äù Over two years ago, there was a kaggle competition that sought to classify images of eyes into one of 5 classes (from no diabetic retinopathy, to mild, moderate, severe, and proliferative). The winning solution used a combination of sparse convolutional networks and a random forest to make a prediction from a pair of images (left and right eye) to the outcome. . Gram Stains . ‚Ä¶ are a ‚Äúmethod of staining used to distinguish and classify bacterial species into two large groups.‚Äù It is a laboratory technique that is performed on bodily fluids (e.g., blood) when an infection is suspected. Researchers have published a paper (cited in this article) that describes using a convolutional neural network to classify microscope images into gram-positive, gram-negative, and background (no cells). In the paper, they describe that they did not train a CNN from scratch; they fine-tuned Inception v3 for this task. . Lung Cancer . ‚Ä¶ ‚Äúis a malignant tumor characterized by uncontrolled cell growth in tissues of the lung.‚Äù This 2017 kaggle competition included a data set of CT scans, and the goal was to predict the likelihood of lung cancer. There were some interesting challenges here, including the fact that the data is in 3-dimensions‚Ää‚Äî‚Ääthe write-ups of winning solutions describe some interesting ways to tackle this. This blog post outlines some of the limitations of this competition, from a clinical perspective. Separately, it looks like Enlitic is also working on a lung cancer screening solution. . Onychomycosis . ‚Ä¶ ‚Äúis a fungal infection of the nail.‚Äù As this tweet pointed out, researchers in Korea have published a paper that reports using CNNs (VGG-19, ResNet-152) to both create a training dataset (e.g., extracting hand and foot images from a clinical photographs) as well as classify nails into one of six classes (as follows: (onychomycosis, nail dystrophy, onycholysis, melanonychia, normal, and others) to achieve ‚Äúa diagnostic accuracy for onychomycosis using deep learning that was superior to that of most of the dermatologists who participated in this study.‚Äù . Pneumonia . ‚Ä¶ ‚Äúis an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.‚Äù Researchers at Stanford have published a paper that reports that a 121 layer convolutional neural network they developed ‚Äúcan detect pneumonia from chest X-rays at a level exceeding practicing radiologists‚Äù (project page). . Skin Cancer . ‚Ä¶ ‚Äúare due to the development of abnormal cells that have the ability to invade or spread to other parts of the body.‚Äù Researchers at Stanford have published a paper that reports fine-tuning Inception v3 to classify 757 disease classes, using a ‚Äúdermatologist-labelled dataset of 129,450 clinical images, including 3,374 dermoscopy images.‚Äù The results are examined across different prediction tasks and accuracy rates seem comparable to the clinicians‚Äô scores. . ‚Ä¶I‚Äôm sure that this list will grow. . References &amp; Resources . 1702.05747 A Survey on Deep Learning in Medical Image Analysis | Medium Up to Speed on Deep Learning in Medical Imaging | NCBI Deep Learning in Medical Imaging: General Overview | Science direct A survey on deep learning in medical image analysis | Github Deep-Learning-for-Medical-Applications | .",
            "url": "http://0.0.0.0:4000/machine-learning/health/2017/12/04/Deep-Learning-Medical-Diagnosis.html",
            "relUrl": "/machine-learning/health/2017/12/04/Deep-Learning-Medical-Diagnosis.html",
            "date": " ‚Ä¢ Dec 4, 2017"
        }
        
    
  
    
        ,"post12": {
            "title": "Learning to rank for flight itinerary search",
            "content": "Recently, at the Workshop on Recommenders in Tourism (part of ACM RecSys 2017), I had the opportunity to give a keynote where I shared some of our recent experiments in this domain‚Ää‚Äî‚Ääand particularly, on how we‚Äôre applying machine learning within this problem. In this post, I‚Äôll expand on some of the thoughts that I shared there. . A Complex Search Problem . Many of us will have already experienced the pain of searching for an ideal flight‚Ää‚Äî‚Ääthere are so many different factors that come into play. Is this too expensive? Could we get to the airport for this time? Should we stopover somewhere to save some money, or pay a bit more for a direct flight? Should we leave a day earlier and come back a day later? Depending on where you are flying from and trying to fly to, the possibilities may seem endless. . In a recent post, Zsombor described how the Skyscanner app‚Äôs flight search result page has evolved over time ‚Äîit is now moving away from being an infinite list of price-sorted flights and towards being a set of tools (or widgets) that help users navigate the complexity of this choice. While there are many ideas and improvements that we can envisage that will surface information and control, one of the questions that we asked was: can we help users by immediately surfacing the ‚Äòbest‚Äô flights? Learning to Rank . In the research literature, sorting ‚Äòitems‚Äô (in this case, flight itineraries) using some notion of ‚Äòbest‚Äô or ‚Äòrelevant‚Äô is known as learning to rank. Applying various forms of machine learning in this problem space has been studied extensively and is increasingly common across various products (e.g., Search at Slack, Venue Search in Foursquare, and Ranking Twitter‚Äôs Feed). . The basic premise is that if we had data about items that users think of as relevant to their query‚Ää‚Äî‚Ääpositive examples‚Ää‚Äî‚Ääand items that users think were not relevant to their query‚Ää‚Äî‚Äänegative examples‚Ää‚Äî‚Ääthen we can use these to train a machine learning model that can predict the probability that a user will find a flight relevant to their query. In doing so, we have translated the problem of ranking items into a binary regression one (note: there are other methods, such as the pairwise approach to ranking which I do not cover here). . Defining relevance in any context is tricky. Many systems rely on measuring this implicitly, by looking at what items users are clicking on. In the Skyscanner app, though, clicking on a search result is not a very strong signal that users have found what they are looking for‚Ää‚Äî‚Ääyou may simply be clicking on the itinerary to find out a little bit more. A much stronger signal of relevance is the commitment click through to the airline/travel agent‚Äôs website to purchase it, which requires multiple actions from the user. . . There were a few stages in the journey from the idea of ranking flight search results with machine learning to experiments. We tackled these with two streams of work‚Ää‚Äî‚Ääoffline and online experiments. . Could it work? Offline Experiments . We first refined how we collected data about each user‚Äôs search experience; this data was then fed through a pipeline that took care of joining, transforming, and reshaping it into a set of features and a binary relevance score. . Having this kind of historical data allows us to ask what-if kind of questions. What if we had sorted flights in a different way‚Ää‚Äî‚Ääwould the flight that you picked appear closer to the top of your search result list? . Formally, this means conducting a number of offline evaluations. We developed a toolkit to support this work‚Ää‚Äî‚Äämuch like similar open-source tools, this toolkit took care of the basics of conducting a machine learning experiment: splitting the data into training/test and collecting a variety of metrics (e.g., Mean Average Precision and Mean Reciprocal Rank) during each test. We developed our own tool so that we could compare how any machine learning approach compares to a simple baseline: price sorting. . Unsurprisingly, many of our initial experiments did not pay off. Our baseline was proving to be incredibly difficult to beat‚Ää‚Äî‚Ääperhaps given that all of our data was sourced from users viewing search results ranked this way. We kept on iterating here until we found one set of features that seemed to be doing better than price-sorting, using an incredibly simple model to begin with: logistic regression. . Will it work? An Online Experiment . An open challenge with offline machine learning experiments is understanding how offline metrics correlate with online performance. In other words, just because an algorithm seems to do better than the baseline on historical data, it doesn‚Äôt mean that it will do better for users. To validate this, we turned to an online experiment. . To do so, we built all of the remaining parts we needed to run a production experiment. At Skyscanner, this means building components that interact with our data platform and a micro service to serve predictions. . One way to evaluate this approach in an A/B test would be to completely replace the price-sorted list with a new, relevance sorted one. While this approach is being actively explored, the UI that we initially tested was a widget that recommended flights above the price-sorted list. . In the end, we ran an experiment comparing users who were given recommendations using machine learning, users who were given recommendations using aheuristic that only took price and duration into account, and users who were not given any recommendations at all. . . Our First Results: Search Effort and Conversion . As above, we did not evaluate this by looking at clicks‚Ää‚Äî‚Ääwe were not interested in users clicking on a widget. Instead, we first quantified search effort by looking at how often users would filter/resort/re-search for an itinerary in a single session, both when they received flight recommendations and when they didn‚Äôt. In this case, we didn‚Äôt find any significant differences. . More broadly, though, we were interested in measuring conversion‚Ää‚Äî‚Äähow often users would purchase a flight that was recommended by the widget versus purchasing a flight that fell below it, in the ‚Äòtraditional‚Äô result list. We found evidence of great promise‚Ää‚Äî‚Äämuch to our surprise, this first ranking model drove more purchases into the widget than the rule-based variant. . This experiment is one of many; a lot of the process was about laying the foundation so that we can explore the thousands of hypotheses that using machine learning for flight ranking offers‚Ää‚Äî‚Ääboth offline and online. . There is much more to come in this space at Skyscanner: experiments that focus on the UI, that go across platforms, that test new machine learning models, and on the various widgets that can be built using this approach. .",
            "url": "http://0.0.0.0:4000/machine-learning/skyscanner/2017/09/26/Learning-to-rank-flights.html",
            "relUrl": "/machine-learning/skyscanner/2017/09/26/Learning-to-rank-flights.html",
            "date": " ‚Ä¢ Sep 26, 2017"
        }
        
    
  
    
        ,"post13": {
            "title": "A review of the ACM RecSys 2017 keynote on the economics of information",
            "content": "The first keynote at the 2017 ACM Conference on Recommender Systems (RecSys) was by Prof. George Loewenstein. . In this wonderful lecture, Prof. Loewenstein delved into the past and present of the economics of information - using behavioural economics to study how we interact with the information around us. In a conference like RecSys, where most of the focus tends to be on the systems (data, algorithms, interfaces) that we can build to help people navigate information, it was refreshing to hear from a different discipline, that focuses on the ‚Äòother‚Äô end of the spectrum: people. . A number of weeks on, I‚Äôm revisiting the notes that I furiously typed throughout the session. Here they are, turned into sentences, with no guarantee that there are no mistakes, omissions, or misinterpretations. . The keynote began with a simple observation: there are people online who rate toasters. Who are these people? Why are they sharing this information? It may sound commonplace to rate something online, but when is the last time that you have gone to read toaster reviews? . What followed was a whirlwind tour of the history of this research. Starting as a counter to the ‚Äòstandard‚Äô economic view of information gathering‚Ää‚Äî‚Ääi.e. that people are Bayesian, and rationally update their beliefs based on the information they have received‚Ää‚Äî‚Ääbehavioural economics research takes the view that people are motivated to hold certain beliefs because they feel good or fit into the sense they have (already) made of the world. The so-called new economics of information introduced a variety of concepts such as adverse selection, signalling, and screening. . The way we process information is biased: when faced with new information, rather than asking ‚Äúshould I believe this?‚Äù (is there valid evidence to support the information?) we tend to subconsciously ask ‚Äúmust I believe this?‚Äù (am I forced to change my beliefs) instead. We defend our beliefs in the same way that we defend possessions (Abelson, 1986). Our beliefs are not just shaped by evidence: for example, one study (Kahan 2015) showed that believing in climate change was correlated with how ‚Äòscientifically knowledgeable‚Äô a person was. However, when the data was split by political affiliation, it showed an extreme result: those who were more scientifically knowledgeable but republican were less likely to say that there is solid evidence of recent global warming due to mostly human activity. So is the evidence driving the belief, or vice versa? . . Screen shot from Kahan 2015, page¬†12. . One of the ways that Prof. Loewenstein proceeded to categorise interaction with information was via two axes: (1) whether we are engaging with (desiring) or avoiding the interaction, and (2) whether we are obtaining or revealing information. Combinations of these result in four areas of research: . 1. Desire to Obtain Information (Curiosity) . Curiosity was broadly defined as the desire to obtain information. There‚Äôs an entire review of the field in The Psychology of Curiosity (Loewenstein, 1994); broadly speaking, it‚Äôs the desire for information for its own sake, as opposed to as a means to achieve a goal. We can be captivated by an intense desire for information‚Ää‚Äî‚Ääso intense that we can even seek out information when we know that finding it will bring us misery (Hsee &amp; Ruan, 2015). People tend to become more curious when they feel they are getting closer to information, and are most curious about themselves. . One item of discussion at the end of the talk was about how this research can speak to those designing systems: curiosity was pointed out as a factor that feels currently underutilised in the design of recommendation systems. . 2. Avoiding to Obtain Information . There are many cases where we actively engage in avoiding information, even when it is ‚Äúuseful, free, and independent of strategic considerations‚Äù (Golman et al., 2016). One reason cited for this is when we expect the information to be painful. For example, the paper states ‚Äúif you know that your paper got rejected, you may still not want to read the reviewers‚Äô comments.‚Äù That one brought back some memories for me! . An interesting domain where information avoidance is of particular significance is in financial investments‚Ää‚Äî‚Ääwhere it is known as the ostrich effect. In Sicherman et al, 2016, a study is reported that looks at how investors are (not) logging into a portfolio information system as the market fluctuates. When the market declines, the log in rate drops by 9.5%; similarly, when the market was up (even if it was closed, such as on weekends), logins were higher. . This behaviour was also observed in a health domain: the higher a user‚Äôs Vitality age in an app (relative to their actual age), the more likely they were to click the ‚Äòhide‚Äô button on their Vitality age, and Loewenstein also mentioned the book Don‚Äôt Even Think About It, as discussing information avoidance in the context of climate change. . 3. Desire to Reveal Information . Behaviours that seek to share information can be strange‚Ää‚Äî‚Ääwe have websites where people can confess and share secrets, and the example used at the beginning of the talk (reviewing toasters) is just one of the many ‚Äòweird‚Äô things that people commit time to sharing information about. . Motivations to share information can be amazingly intense: it is no wonder that many metaphors (e.g. ‚Äòbursting‚Äô to tell someone) seem hydraulic in nature. One qualitative study of people who were ‚Äòdying‚Äô to share information (I missed the reference) reported that about half of them had something (mostly positive) to share about themselves, while the other half had something (mostly negative) to share about others. . One question that Prof. Loewenstein asked: why do we share so much information that is obvious? As an example, telling someone that Lake Como, the location of the conference, is beautiful. What has been gained from that revelation? . This line of research links directly into recommender systems that rely on crowdsourced information. Why are users providing reviews? Perhaps they are motivated by reciprocity with (or revenge on) providers, showing off their sophistication (bragging), they may want to influence others (e.g., based on how many people will see their review), or just want to be noticed (e.g., the prank on YouTube by the couple that wanted to boost their profile which ended up as a fatal shooting). This matters because it determines selection bias‚Ää‚Äî‚Ääwho is going to write the reviews‚Ää‚Äî‚Ääas well as providing effective strategies to solicit reviews. . (Wait, why am I writing this blog post again? Oh yeah, for the claps üëè!) . 4. Avoiding Revealing Information (Privacy) . Privacy, in the context of this talk, was defined as the desire to not share information. . . The ‚Äòstandard‚Äô economic model of privacy tended to model this as an optimisation‚Ää‚Äî‚Ääweighing the costs vs. the benefits of revealing something. However, our preferences are often ill defined, as is our ability to navigate the cost/benefit of information disclosure in different contexts. . While a large majority of people believe that privacy policies are put in place to protect them, users are often unaware as to the real implications. Case in point: this study, where users signed over their first born child when agreeing to the terms to join a free Wi-Fi network. . Our willingness to share information depends on arbitrary cues from our current context (example: strangers on a plane) rather than the net benefits of sharing information (Acquisti et al., 2011). People take cues about what to reveal from others and their own past experiences (Acquisti et al., 2012). More online control for privacy makes people more likely to reveal information about themselves (Brandimarte et al., 2013); put another way, the more you can control your privacy online, the ‚Äúmore rope you have to hang yourself with.‚Äù People exhibit default default effects and loss aversion with respect to their privacy (Acquisti et al., 2013). . The main take-aways seemed to be: people don‚Äôt seem to care much about privacy, except when you ring alarm bells; privacy preferences are context-dependent, and there are a host of research questions that remain at the intersection of privacy and information disclosure (why do people review strip clubs? Or gastroenterologists? He gave examples of both!). . In Closing . It looks like this recorded talk of his is similar if you want to dive in deeper. When announcing the best paper award this year, the award committee made a special point of calling out for more research (like this year‚Äôs best paper) that builds off of theories developed in other disciplines into practical impact in the recommender system domain. . The same can be said for most disciplines‚Ää‚Äî‚Ääthere are some wonderful things to be learned and explored if you step away from home from time to time. And this keynote was one of those times for me. .",
            "url": "http://0.0.0.0:4000/conference/2017/09/19/A-review-of-the-ACM-RecSys-2017-keynote-on-the-economics-of-information.html",
            "relUrl": "/conference/2017/09/19/A-review-of-the-ACM-RecSys-2017-keynote-on-the-economics-of-information.html",
            "date": " ‚Ä¢ Sep 19, 2017"
        }
        
    
  
    
        ,"post14": {
            "title": "Machine Intelligence Summit 2017",
            "content": "Last week, I was at the 4th Machine Intelligence Summit #reworkMI in Amsterdam. The mission of the event is: . ‚Äú‚Ä¶to bring together government officials, academics, entrepreneurs, investors, and industry professionals all in the same room to discuss the latest breakthroughs in machine learning, deep learning and AI and to explore their applications in industry and their potential impact‚Ä¶‚Äù . When I ran a quick hand-raising survey of the room on the 2nd day, it looked like the audience was mostly from an academic or startup background (I admit, I forgot to ask about government officials). . The full schedule and abstracts are available online, and I‚Äôm writing this summary by looking back there. Here‚Äôs a brief overview of the talks, grouped by themes (rather than by speaker order): . Computer Vision . Damian Borth from DFKI discussed explorations of the latent space created by Generative Adversarial Networks (GANs) in terms of adjective-noun pairs to capture subjective traits of images. The results that were presented included a way of translating images to better capture some traits; e.g., translating an image from a ‚Äúfoggy sky‚Äù to a ‚Äúpretty sky.‚Äù . Appu Shaji from EyeEM discussed how EyeEM is using computer vision to create new experiences for customers. One interesting application that he discussed was the the ‚ÄòEyeEM Selects‚Äô feature (see this blog post) which picks the best photos from your library to showcase. . Fashion &amp;¬†Beauty . Panagiotis-Alexandros Bokaris from L‚ÄôOr√©al promised us a talk that would ‚Äònot just be about lipstick.‚Äô His talked covered a wide breadth of work being done at the intersection of augmented reality and computer vision‚Ää‚Äî‚Ääwith small pointers about how these are making their way into beauty-related products, like smart hairbrushes (and lipstick). . Roland Vollgraf from Zalando Research discussed applications of deep learning to items of clothing. By learning from product images, textual descriptions, and vendor attributes, they created a ‚Äòfashion DNA,‚Äô which can be applied to cluster items as well as to generate items of clothing that don‚Äôt exist.. yet. . Search &amp; Recommendation . Aleksandr Chucklin from Google Research gave a presentation about the evaluation of search engine result pages (Arxiv paper here). This is another talk that demonstrates how nuanced the evaluation of search results has become‚Ää‚Äî‚Ääthere is research on the differences in search between mobile/web, on evaluating ‚Äògood abandonment‚Äô, and on evaluating the quality of mixed-content and non-linear search result pages. . I gave a talk about bootstrapping the destination discovery system at Skyscanner, and was perhaps one of the few not discussing an application of deep learning (at least, for now!). More on that to come in a different blog post. I think that this was my first ever appearance on Periscope. . Shopping &amp;¬†Auctions . Daniel Gleber from Picnic gave an overview of the various problems that Picnic (a Dutch online shopping service) is tackling from a machine learning perspective: they span the entire customer journey, and more. For example, this included scaling a customer feedback system, where users can request items that are not currently stocked, from being a Slack channel into an automated classification system. Another focus area was on predicting delivery times‚Ää‚Äî‚Ääthe y-axis of the graph with error metrics was in seconds! . Peter Tegelaar from Catawiki discussed a variety of applications of deep learning in Catawiki‚Äôs auction system. This ranged from using classification to support product categorisation through to estimating item‚Äôs prices. Peter also spoke in an open way about an application of deep learning that didn‚Äôt work for them (estimating customer lifetime value), and warned us about treating everything as a nail, now that deep learning is the latest hammer. . Robotics &amp;¬†Hardware . Prof Sven Behnke from the University of Bonn discussed applications of deep learning for robot cognition. There are various angles to this space. One of the many interesting applications he discussed was using these methods in the RoboCup 2016, where participants competed with robots tasked with automated picking in an unstructured environment (see Amazon Robotics Challenge). . Theo Gevers from the University of Amsterdam gave an overview of his group‚Äôs research into automatic object detection, segmentation, and reconstruction. He gave a demo of an application that can automatically infer a person‚Äôs emotion, age, and gender from a live video stream. No surprise that the first question was about privacy. . Adam Grzywaczewski of NVIDIA gave an overview of the infrastructure challenges that are arising as a result of the surge of interest in deep learning. One question he got was about how NVIDIA‚Äôs GPUs compare to Google‚Äôs TPUs. His answer? I don‚Äôt know, I can‚Äôt go out and buy Google‚Äôs board to compare it to ours. . Ben Scott-Robinson from The Small Robot Company gave a talk about three robots: Tom, Dick, and Harry, and how they are being designed to help farmers with everything from soil analysis to planting seeds. There was a fun focus on anthropomorphising the robots that are being designed and it will be interesting to see this ‚Äòfarming as a service‚Äô model evolve. . Dor Kedem from ING gave a talk about resource allocation in large computing clusters, and how using machine learning with cluster utilization patterns can save significant amounts of money, that were otherwise being spent to allocate resources that end up sitting idle. . Document Analysis &amp; Embeddings . Sotirios Fokeas from Swisscom gave a talk about using sentence embeddings to extract high coverage key-phrases in documents. Sentence embeddings are an interesting approach that has also received much recent attention. . Marcus Liwicki from the University of Kaiserslautern gave an overview of methods that have worked well in analysing documents. This includes the renown Long Short-Term Memory (LSTM) networks. . LSTMs also made an appearance in Mihai Rotaru‚Äôs talk about Textkernel; he gave a talk about using deep learning to automatically parse data from candidate‚Äôs CVs, and using this data to match applicants with jobs. This included considering language-independent models, and finding job-title similarities via embeddings. . Data Analysis with Machine¬†Learning . Vahid Moosavi from ETH Zurich gave a talk about using data to model urban phenomena, like real estate prices and air pollution flow. One of the key points that he made is that the application of data and machine learning to problem spaces that traditionally relied on domain knowledge is shifting the definition of ‚Äòexpertise‚Äô from having the answers to known questions towards knowing to ask good questions (and whether they can be answered with data). . **Ingo Waldmann from University College London gave a presentation about using deep learning to automate the analysis and discovery of extrasolar planets (see ‚ÄòDreaming of Atmospheres‚Äô), and how astronomy is the latest victim in the growing domains of research where data availability outweighs our ability to analyse it. . Augmenting Experts with Deep¬†Learning . Tijmen Blankevoort from Scyfer talked about human-in-the-loop or active learning. This included applications in medical imaging. . Larens Hogeweg from COSMONiO also discussed applications of deep learning to medical imaging. In particular, he was focusing on learning from ‚Äòsmall data‚Äô (even 1 or 2 samples), and demonstrated how approaches seem to generalise well. . Bayesian Methods . Christopher Bonnett from alpha-i presented about using Bayesian approaches in deep learning to expose a model‚Äôs uncertainty in its predictions, alongside the predictions themselves. . Laith Alkurdi from Freeletics gave a talk about how Bayesian methods are being used to model users and create a ‚Äòcoaching AI‚Äô in the freeletics app. . Panel Discussions . The first day‚Äôs panel focused on investing into AI. The panel was moderated by Matthew Bradley (Forward Partners), and included Julius R√ºssmann (Earlybird Venture Capital), Carlos Eduardo Espinal (Seedcamp), and Daniel Gleber (Picnic). Interesting snippets included assessing startup‚Äôs competitive advantage in terms of access to training data, and balancing between generalist AI startups and those that grow in a specific vertical. . The second panel was a bit of a gear-shift compared to the rest of the conference, and focused on responding to growing threats in cyber security. The panel was moderated by Eelco Stofbergen (CGI) and included Tanya Harris (Harrman Cyber) Ian Bryant, (Trustworthy Software Foundation) and Colin Williams (University of Warwick). An interesting question that the panel received was about whether the ‚Äògood‚Äô or the ‚Äòbad‚Äô guys were making better use of AI in securty attacks/defenses: the answer was along the lines of ‚Äòwe‚Äôll have to wait and see.‚Äô . Closing Thoughts . #reworkMI was an interesting conference, and I met a bunch of great people. Everyone is excited about deep learning‚Ää‚Äî‚Ääno surprises there‚Ää‚Äî‚Ääand that excitement was mostly presented from a very technical perspective. However, many of the speakers had slides about the challenges that they currently face in their deep learning systems (like the one below): these spanned the entire product life cycle. There is still much work to be done. .",
            "url": "http://0.0.0.0:4000/conference/2017/07/04/Machine-Intelligence-Summit-2017.html",
            "relUrl": "/conference/2017/07/04/Machine-Intelligence-Summit-2017.html",
            "date": " ‚Ä¢ Jul 4, 2017"
        }
        
    
  
    
        ,"post15": {
            "title": "Machine learning for product managers",
            "content": "It‚Äôs now becoming common for me to hear that product owners/managers, technical managers and designers are turning to popular online courses to learn about machine learning. I always encourage it‚Ää‚Äî‚Ääin fact, I did one of those courses myself (and blogged about it). However, it‚Äôs not always clear how much benefit someone whose goal is to design, support, manage, or plan for products that use machine learning will get from doing an online course in ML. These courses throw you into the deep end, asking you to start programming classifiers, when many non-technical team mates are only looking for sufficient knowledge to be able to work in teams that are creating an ML-driven product. It‚Äôs a bit like wanting to drive a car, and ‚Äòtherefore‚Äô signing up to a course on combustion engines‚Ää‚Äî‚Ääprobably a little bit too detailed for practical day-to-day driving! . I therefore recently ran a session at Skyscanner that aimed to cover machine learning from a non-technical, product-centric perspective. We first covered definitions, and then moved on to a number of key issues that are important to keep in mind to create successful products that go beyond ‚Äòjust‚Äô the ML. This post is a summary of that session. . Part 1: Machine learning, without the¬†math . We began by addressing the burning question on everybody‚Äôs mind: what is machine learning? I quoted the first sentence of this paper by Pedro Domingos: . Machine learning systems automatically learn programs from data. . That‚Äôs basically it. Machine learning is a way of creating a program that does something, without you having to figure out exactly how to do it. Compare that to the usual way we create programs, where we need to be able to code every step (if this happens, then do that, and then do that). In practice, the way this happens is that you give an ML algorithm a data set with examples, and the algorithm‚Äôs job is to learn from the examples. Forget the math that is happening under the hood: it is enough to know that, at the heart of every ML algorithm, is the concept of error‚Ää‚Äî‚ÄäML algorithms are all trying to minimize how many mistakes they make. . There are many families of ML algorithms; the most common two are known as supervised and unsupervised learning. How can you recognize them in products? Let‚Äôs dive into some examples. . Unsupervised learning is about identifying patterns . The examples that are usually fed into unsupervised ML algorithms are characterized by the fact that there is no deterministic ‚Äòcorrect‚Äô answer that the algorithm needs to learn to predict. Products that use unsupervised learning are typically surfacing patterns in user data. . . A simple example is ‚ÄúTrending this Week‚Äù on Foursquare. Is there any data out there that could tell us, objectively, that a venue should or should not trend this week? No. Instead, there is data that shows how Foursquare users were visiting venues that week‚Ää‚Äî‚Ääand patterns in that data create trending venues. Note that this is different from whether users think that the output of the algorithm is correct or not (‚Äúwhat? McDonald‚Äôs is trending?‚Äù). . Supervised learning is about predicting an¬†outcome . The examples that are fed into supervised ML algorithms, instead, have a deterministic outcome that the algorithm needs to try and predict. The most classic example of this in action is spam-detection. . . You give the algorithm examples of emails that are and are not spam; each example email is labelled with whether it is spam or not. Then, given a new email, you ask the algorithm: is this spam? Remember, the algorithm‚Äôs goal is to minimize how many mistakes it makes. If the algorithm says yes, you push that email into the user‚Äôs spam box. . Supervised &amp; unsupervised learning often looks similar in¬†products . While the examples above make the difference between supervised and unsupervised learning look quite straightforward, there are many cases where the difference is not so clear. Consider, for example,‚ÄúDiscover Weekly‚Äù and ‚ÄúRecommended Songs‚Äù (at the end of a playlist) by Spotify. Both are being pitched to users as a recommendation. However, they are (probably!) using different kinds of learning under the hood. . . Discover weekly looks like a supervised learning problem: the ML is given examples of songs that you have listened to, starred, etc., and is tasked with finding a list of songs that you are likely to listen to. Songs that are recommended for a playlist looks like an unsupervised learning problem: the ML algorithm is looking for co-occurrence patterns in millions of playlists, to find songs that are commonly added to playlists that contain the songs that you have already added to yours. . There are technical terms for what products are trying to¬†do . Data scientists will often use technical terms to describe the ML problem that they are working on. Are you building a product that is‚Ä¶? . Helping users find the right thing when they search? This is a ranking problem. Google, Bing, and Twitter search are all doing this‚Ää‚Äî‚Äätrying to sort things when you query so that the thing you‚Äôre looking for is at the top. | Giving users things they may be interested in, without them explicitly searching? This is a recommendation problem. Netflix, Spotify, who-to-follow on Twitter are examples‚Ää‚Äî‚Ääthese products are suggesting things to engage users. | Figuring out what kind of thing something is? This is a classification problem. Gmail spam/not spam and Facebook photos (detecting faces) are examples here. | Predicting a numerical value of a thing? This is a regression problem. Predicting how much a flight will cost in two hours is an example. | Putting similar things together? This is clustering in action. Amazon‚Äôs customers-also-bought is the most notable example, and we saw Spotify‚Äôs playlist addition recommendations above as well. | Finding uncommon things? This is usually anomaly detection. Most ‚Äútrending‚Äù products (Foursquare, Twitter, Facebook) are examples, that surface things that are being tweeted/visited/talked about more than usual. | In general, 1‚Äì4 are examples of supervised learning, and 5 &amp; 6 fall into the unsupervised learning domain. However, there are obvious cross-overs here: for example, a recommendation problem is a kind of classification problem‚Ää‚Äî‚Ääthe algorithm is trying to predict whether a user would (or would not) be interested in a thing. For practical purposes, let‚Äôs leave those cross-overs to one side for now. . Part 2: Using ML in¬†products . Technical team members who are building an ML product will be discovering and analyzing data, building data pipelines, feature engineering, selecting and optimizing algorithms, avoiding overfitting, running offline evaluations, and putting ML into production for online tests. However, as a product manager, there are a number of additional things to keep in mind that go well beyond the technicalities of ML in order to create a successful product. This part focuses on seven of those considerations. . 1. Does the ML fit the product¬†goal? . . In 2006, Netflix launched a million dollar competition to improve their recommendation system. They asked researchers to develop a supervised learning algorithm that could predict how many stars a user would give a particular movie. The idea at the time was that being able to predict how many stars a user would give a film could be used to give better recommendations. As I blogged about before, one of the major lessons from the competition, which is described in this paper and these slides, was that: . Predicting a rating accurately was no longer as important as ranking films in the right way with other sources of data. . In other words, the problem that the ML was solving was different from the problem that Netflix wanted to solve in their product. For any new product that you are developing, you should ask: is the ML solving the problem that you want it to? . 2. How does the product behave ‚Äúaround‚Äù the¬†ML? . . I‚Äôll pick on Discover Weekly again. It‚Äôs a playlist that is generated by ML. However, somebody decided that it should be finite, that it should update on Mondays, and that your previous playlists should vanish when your new one comes along. All of these are examples of product decisions, which do not rely on ML: it‚Äôs easy to imagine a version of Discover Weekly that would be an infinite playlist that updates on the fly and that stores all of the past songs somewhere for you to go back to. . In other words, while Discover Weekly is clearly a (great!) product that uses ML, there are also a number of other decisions that were made which probably contributed just as much to its success: defining the product‚Äôs behavior around the ML itself is very important. . 3. How should a product start using¬†ML? . A common question regarding ML products is where to begin: it seems like a huge, near-insurmountable task requiring months of work. However, as Martin Zinkevich‚Äôs (highly recommended) document on best practices for ML engineering describes, you should not be afraid to ship a product that does not use machine learning. Many products can start to collect useful customer feedback using simple baselines; in the document, Martin quotes an example of sorting apps in an app store by download count (or popularity). . The key word here is simple. If you need to draw a state diagram that has tens of boxes to describe what your non-ML product is doing, then you‚Äôve probably already started over-engineering an unnecessarily complex solution. One the other hand, if you can say (in 1 short sentence) what the product is doing (‚Äúwe‚Äôre sorting by cheapest,‚Äù ‚Äúwe‚Äôre showing the most popular‚Äù) then you‚Äôre off to a good start. . 4. What are you comparing with? . This next point follows on from the previous one that is about using a simple baseline. We typically think of early/MVP products in isolation: we build one thing in order to get it out there and see how customers react. . ML products are different because performance is always relative‚Ää‚Äî‚Ääeven with your first iteration. For example, if your advanced ML algorithm is 95% accurate, but your simple baseline is 94% accurate, then you‚Äôre investing a huge amount of work for 1% gain. If, on the other hand, your ML algorithm is 75% accurate, but the simple baseline was 50%, then you‚Äôve made a huge leap forward. . There are two important points here: first, performance is always relative to something: you need a baseline. Second, to be able to compare things, you need well-defined metrics. In ML products, these are often split between offline (e.g., ‚Äúhow accurate is the algorithm at predicting historical data?‚Äù) and online (e.g., ‚Äúhow much more conversion do we get when we deploy the product with this algorithm?‚Äù) metrics. . 5. How quickly should this product¬†change? . . The speed with which the ML product‚Äôs output should change has a great impact on how you build it. For example, consider Medium‚Äôs ‚Äòdaily three‚Äô email, or Quora‚Äôs digest email. There is probably some ML behind both of these‚Ää‚Äî‚Ääbut the product is an email, which does not need to adapt, in real time, to any actions a user may take. Now, instead, consider Foursquare‚Äôs location-based notifications, or Google search. Every action that a user takes (going to a new neighborhood, adding in a new query) will result in a different output. . Understanding the ‚Äòspeed‚Äô of the product not only allows you to tailor your system architecture to cater for this, but also impacts the experience that your users will have. . 6. What interactions, actions, &amp; control do users¬†have? . . Data scientists will often be looking at what data is available, and look to build ML algorithms based on that. However, when creating a new product, teams will have the opportunity to define what data will be collected when designing user interactions: part of creating a new product is identifying what data can (and should) be collected to improve the product in the future. In fact, not logging data in early products is a source of frustration for many data scientists. . For example, Instagram‚Äôs friend recommendations allow users to follow or hide the suggested contacts, and Foursquare allows users to fix the places that it automatically detected users had visited. In doing so, both of these allow users to engage with the ML algorithm‚Äôs output, and provide new data‚Ää‚Äî‚Ääboth positive (yes, follow!) and negative (no, hide!) examples. These can then be fed back to improve the algorithm. . The Instagram example also shows why users are recommended. Providing explanations for ML actions is an entire field of research, with one common theme: systems that explain how they work are often better received by users. . 7. How could the product fail catastrophically? . . In products that don‚Äôt use ML, ‚Äòfailure‚Äô is often about bugs, crashes, or confusing interfaces. Since ML is intrinsically about training an algorithm via examples, the way products can fail takes on a whole variety of new dimensions (the image here is of Microsoft‚Äôs Tay Bot, which turned racist after being put online). . One of the most cited stories in this domain is about the US-based retail store Target. They had some kind of algorithm to compute the probability that a person was pregnant, and they used that to send coupons and discounts‚Ää‚Äî‚Ääa fairly typical use of ML. This system sent coupons for baby clothes to a teenage girl, causing her father to be outraged: why was his daughter targeted with coupons for baby clothes? Shortly later, though, Target received an apology from him: the girl was indeed pregnant. . So, what happened here? Well, the ML algorithm actually got its prediction right: it learned to accurately predict pregnancy from the examples it was given. The product, though, failed. There are many, many similar examples of ML products going wrong. All of them share a common theme: the way the products are used or applied was different to how the product designers envisaged. Whether this comes down to targeting customers based on sensitive inferences, to people goading a bot, to biased data sets being used to train face-detection algorithms, the product teams had not taken into account how the product would be used in context. . Wrapping Up . Creating products that use ML is an increasingly multi-disciplinary activity. The session summarized above focused on defining ML (without the math), and highlighting seven issues that go beyond the ML when creating products‚Ää‚Äî‚Ääthere are many more. Some related reading below. . Experience design in the machine learning era | It‚Äôs ML, not magic | AI Snake Oil | .",
            "url": "http://0.0.0.0:4000/opinion/machine-learning/2017/03/14/Machine-Learning-for-Product-Managers.html",
            "relUrl": "/opinion/machine-learning/2017/03/14/Machine-Learning-for-Product-Managers.html",
            "date": " ‚Ä¢ Mar 14, 2017"
        }
        
    
  
    
        ,"post16": {
            "title": "Crowdsourcing in the London underground with wifi data",
            "content": "In November 2016, Transport for London (TfL) announced a 4-week trial where they would collect wifi connection data ‚Äúin order to better understand how London Underground passengers move through stations and interchange between lines.‚Äù . . TfL discusses some of the details of the trial in this freedom of information request. In particular: . The WiFi pilot is an internal initiative and has been developed by an in-house TfL team using our existing infrastructure. For the pilot we developed our own method and software for collecting data from our the access points. We store the pseudonymised and encrypted MAC addresses, the date and time of the connection, the device connection status and the individual access point they connected to, and therefore the London Underground station and location within a station where the device connected. . Previous Experiments with Tube¬†Crowds . Seeing TfL undertake this experiment is exciting, and probably a big step forward for the research field of urban computing. A number of years ago, when I was a postdoctoral researcher at UCL, I had been working on a project that was trying to do the same thing‚Ää‚Äî‚Ääusing smartphones. . The project started out with a focus on the Oyster card data. The most recurring frustration we had, when analysing the troves of data that TfL collects via its smartcard, was that we only saw entries/exits‚Ää‚Äî‚Ääand nothing in between. Was the trip good? Slow? Crowded? Disrupted? We could never really know. . We therefore decided to run a small experiment where we would ask participants to crowd source that information for us. We started building an Android app. After a number of iterations, we settled on a simple interaction: passengers could rate the current tube line they were on, and leave an optional message about the experience they were having. I presented the slides on the left at a conference where we reported the results of the trial. . A thematic analysis of the data we received uncovered some cute results: (1) passengers reported on a variety of topics, ranging from temperature to seat availability, (2) the ratings that passengers gave were, on the whole, mostly positive, and (3) there were a few instances where the crowdsourced reports were more timely than disruptions announced via the official TfL update feeds. . From Crowdsourced Reports to Wifi¬†Data . The biggest problem with the first Tube Star study, though, was the small amount of data that we received. Getting an app into the hands of thousands of London tube passengers was a challenge that we simply did not have the resources to face, and was seen as the major limitation to any future public transport service that would rely on crowdsourced data. . By this time, however, I had moved on to a new project that had a more distinct focus on smartphone sensors. As I learned about how to efficiently collect data from various sensors, I realised that wifi scans could be matched with known hotspot locations, and that this could be used to passively track passengers who were in the London Underground. . My first step was to send a freedom of information request to TfL: I asked them to provide me with the locations and ids of all their wifi hotspots. With those in hand, I would have been able to automatically identify which station a user was in, given the wifi hotspots surrounding them. TfL did not release the ids I needed‚Ää‚Äî‚Ääciting security concerns: ‚ÄúIn this instance the exemption has been applied as disclosure of the information requested would be likely to adversely affect the safety and security of TfL employees and members of the general public.‚Äù . I decided to go ahead anyway. I updated the Tube Star app (and the ethical approval that it had originally received) to also collect filtered wifi traces: when Android phones with the app installed would scan for wifi connectivity, they would keep a record of any Virgin Media WiFi that they came across. I got around the missing data that matched hotspot ids to stations by going back to crowdsourcing. If a hotspot did not match any tube station known to my app, the app would ping the user and directly ask ‚Äú‚Äòwhere are you?‚Äù and allow them to select a station. I re-released the app, and watched the data start flowing in. . The full results of this 2nd study have not been published to date; however, I did present them in a variety of conferences, including a workshop in 2015 that TfL hosted in its offices near Victoria. . Case 1 in the slides on the left (slides 11‚Äì30) give an overview of these results. With only 34 users who updated and consented to the new data collection, the app collected over 100,000 wifi scans that matched the London Underground. . I presented these results as a contrast between what the Oyster card tells us (entry and exit), what journey planning tools tell us should happen in between, and what we actually observe with wifi traces. There were some interesting highlights. For example, just 2% of the connection sequences that started at Pimlico and later appeared at Victoria had Green Park as an intermediary‚Ää‚Äî‚Ääthe wrong direction of travel. . The TfL trial results &amp; future advertising . The recent TfL trial was slightly different from this previous research‚Ää‚Äî‚Ääthey collected data from the hotspot devices themselves, rather than relying on passengers to install data-collecting apps. Gizmodo recently published a piece covering TfL‚Äôs trial, using data obtained via a freedom of information request. I was thrilled to see that many of the behaviours that they have observed are in line with the smaller data that I collected with Tube Star. Their results are also likely to have much higher coverage and granularity than the study that I ran. . However, one item that the piece focuses on is that this data could be used ‚Äú[‚Ä¶] to inform advertising decisions on the Tube network‚Äù. While research by @danielequercia &amp; co (‚ÄúAds &amp; the City: Considering Geographic Distance Goes a Long Way‚Äù) shows that, indeed, ‚Äú knowing the kind of people who are willing to go to, say, certain restaurants or bars [‚Ä¶] translates into low-cost marketing strategies for bars and restaurants that are willing to attract new crowds ,‚Äù it is always a bit underwhelming to announce the creation of a new dataset that has huge potential, and then say that it will be used for advertising. . Alternative uses of TfL wifi¬†data . While the TfL documents clearly show that they are going to use this data for analytic purposes (to help them understand how customers use the network), there are definitely future applications for this data that go beyond advertising‚Ää‚Äî‚Ääif they can not only collect the data, but also start building services that use it. What about‚Ä¶? . Timely &amp; relevant alerting. If TfL knew I was 2 stops away from making a potential route choice, could it tell me which route is busier, and even perhaps nudge me to go one way or another (like the game for the underground chromaroma did)? If TfL knows the routes that I regularly take, they‚Äôre likely to be able to predict when and where I travel. Could they use this to notify me about relevant disruptions, both planned and unplanned? Could TfL overcome all of the limitations that I had with the 1st Tube Star study, and also collect crowd-sourced passenger updates? | Accessibility &amp; station design. Could TfL harness wifi detection and localisation to improve accessibility, like they are doing with the wayfindr project? If they can detect crowding, could they use that to redesign stations to cater for easier walk-through routes? | (No More) Penalty fares. Right now, if you don‚Äôt touch out, you may be subject to a penalty fare for an incomplete journey‚Ää‚Äî‚Ääand we regularly hear the reminder about touching in and out. If TfL has tracked my phone, couldn‚Äôt they figure out where I forgot to touch out? | Better research into passengers‚Äô experiences. If TfL can link phones, smartcards, and passenger surveys, and give that data to its academic partners, they would enable an entirely new field of research that has been inaccessible to date. Here is an example where we only had two linked sources (Oyster cards and survey results). | . Maybe, one day, our phones will even replace the Oyster card altogether‚Ää‚Äî‚Äädoing away the need for gates at stations. In the mean time, I‚Äôm looking forward to seeing more results from this trial emerge, and for new and interesting ideas of how this data can be put to great use. .",
            "url": "http://0.0.0.0:4000/research/2017/02/19/Crowdsourcing-in-the-London-Underground-with-Wifi-Data.html",
            "relUrl": "/research/2017/02/19/Crowdsourcing-in-the-London-Underground-with-Wifi-Data.html",
            "date": " ‚Ä¢ Feb 19, 2017"
        }
        
    
  
    
        ,"post17": {
            "title": "Unexpected results in online controlled experiments",
            "content": "While discussing some A/B tests that we are running, Colin McFarland recommended ‚ÄúUnexpected Results in Online Controlled Experiments‚Äù by Kohavi and Longbotham, from SIG KDD Explorations 2010. Following on from my other research summaries, here‚Äôs a quick overview of the paper. . If a result seems unexpected it may be due to lack of understanding of user behavior or it could be due to a software or experimental design problem. . This short, well-written paper does not focus on specific ideas or experimental results, but instead covers a wide range of unexpected lessons learned across a variety of online experiments. Many of these lessons capture differences between the control (A) and treatment (B) groups that end up unduly affecting your experiment, causing you to measure a change where none should have been measured. . System Differences . The first type of differences are ‚Äúunder the hood:‚Äù system differences that you may not think about when running an experiment. There were two examples where the website‚Äôs speed was unintentionally affected in one of the two groups, resulting in performance differences, another example where a monitoring system skewed the metrics, and an example where website localisation was not taken into account properly: . Often, ‚Äúif the randomization function determines that the user should be in Control, the page is displayed; if the randomization shows that the user should be in Treatment, a browser redirect is done.‚Äù The redirect added loading time to the treatment: users in B had a slower experience. | In another example, an experiment was run where both the control and treatment website variants made use of a cache. Since the experiment was set up as 90% (A) and 10% (B), the cache was skewed towards those users/entries that were in the control‚Ää‚Äî‚Ääresulting in faster performance for A. | In a one-click conversion experiment, there was a surprisingly large number of page views in the treatment group. ‚ÄúAn investigation revealed that the experimenting site had a monitoring system that requested the page and then simulated a click on the purchase button and checked the ordering pipeline‚Äù‚Ää‚Äî‚Ääthe difference was due to the monitoring system, not the users. | In practice, you want the distribution and types of users arriving into your control and treatment groups to be the same. Do your experimentation tools take into consideration how your website may be localised across users? ‚ÄúWhen a new version of the MSN US Home Page was tested in a controlled experiment [‚Ä¶] the results were highly biased because the population of users from non-US IPs was much higher in the Treatment than in the Control.‚Äù | Experiment Execution . There were other unintended differences that seemed to originate from how the experiments were executed and measured.: . In one experiment, the percentage of users allocated to the treatment and control was ramped up over time: ‚Äúsince the Treatment had a much larger percentage on the last two days, the clicks per user on those days carried larger weight with the Treatment mean making the Treatment look worse than the Control.‚Äù This lead to an example of Simpson‚Äôs Paradox. | When evaluating email campaigns, experiments were run by sending emails to a treatment group, and withholding them from the control group. ‚ÄúThe OEC (Overall Evaluation Criterion) for the campaigns was based on purchases whose sessions were referred by the e-mails. But under this criterion, all ideas/campaigns evaluated positively.‚Äù The effect? Users started complaining about receiving too many emails. | Conclusion . The main recommendation in the paper is to run A/A tests, which tests the tool(s) and methods that you use to split users across variants. An A/A result should show no significant difference (you can expect a difference 5% of the time even in A/A tests if alpha is 0.05¬†!), but often uncovers some: ‚Äúthe slightest difference could result in significant changes to the user experience.‚Äù . The authors also share the following take-aways: . Always get information on the ultimate action you want the user to take. | Beware of classical statistical formulas that assume independence. | Experimental control is critical. Keep everything constant except the thing you want to test. | Drill-down by time to look at hourly data. Had the result not been so surprising (e.g., if the treatment were 2% worse), we might have accepted the result. We now regularly show hourly plots for sanity checks to detect such anomalies. | Use screen scrapers to save screen shots of the pages being experimented on a regular basis in order to allow debugging of surprises. We have found this to be extremely useful in other experiments. | To avoid Simpson‚Äôs Paradox, ‚Äúyou can simply require that the percentage of users in the Treatment relative to the Control not change during the experiment.‚Äù | At Skyscanner, our internal experimentation tool (Dr Jekyll) simply forces a new allocation and new scorecard anytime the experimenter makes a change. We also force buckets to be balanced: you choose your percentage of users you want in the experiment and we split all treatments equally. . Running a good online experiment is a lot more than just randomly assigning users into two groups‚Ää‚Äî‚Ääit requires careful planning and vigilance in monitoring for known and yet-to-be discovered sources of experimental bias. . Here‚Äôs another good read about experimentation: Design Like You‚Äôre Right, Test Like You‚Äôre Wrong. .",
            "url": "http://0.0.0.0:4000/paper/2017/01/09/Unexpected-Results-in-Online-Controlled-Experiments.html",
            "relUrl": "/paper/2017/01/09/Unexpected-Results-in-Online-Controlled-Experiments.html",
            "date": " ‚Ä¢ Jan 9, 2017"
        }
        
    
  
    
        ,"post18": {
            "title": "The machine learning behind farecast",
            "content": "A number of years ago, I ran into this research paper ‚ÄúTo Buy or Not to Buy: Mining Airfare Data to Minimize Ticket Purchase Price,‚Äù when I was working at UCL on a project that was researching how to help people navigate cities better using data. I had just received a large data set of Oyster card transactions from Transport for London. This paper, which looks at recommending whether to buy an airplane ticket, inspired us to apply a similar concept to help people decide what travel card to buy, with a recommender system that would learn from their travel patterns. That research resulted in this paper ‚ÄúMining Mobility Data to Minimise Travellers‚Äô Spending on Public Transport‚Äù and this BBC news story. . I resurfaced the airfare data paper for the Skyscanner reading group‚Ää‚Äî‚Ääwe‚Äôve been meeting to discuss research papers that tackle similar problems to those that we face within Skyscanner. Just like the last post, here‚Äôs a brief, non-technical overview of what we discussed. . Making a Product out of Price Prediction . Flight prices are notoriously subject to change: I‚Äôm sure that most of us have looked at a flight, thought about it for a while, and then come back to it only to find that its price had shot up (or, if we‚Äôre lucky, down). . As the authors of this paper explain, part of predicting a flight price seems to be about decoding the hidden decision engine that carriers use to set their own prices: perhaps a current route‚Äôs price is based on available seats, on demand, on competitor‚Äôs prices, on the time of year, or a multitude of other factors. Given that there are so many different reasons why flight prices may change (and we do not necessarily have access to all the data that providers are using to set prices), could we ever reasonably develop any means to predict flight prices? . This scenario is one where machine learning can come to the rescue. Using the only assumption that there will be some similarity between historical and future route prices, the authors create a model based only on data that is readily available to them on the web: the route (e.g., Seattle-Washington), the days until the departure (e.g., 10), and the historical prices. This model tries to answer the question: given a route, a departure date, and the current price, should I buy the ticket, or should I wait? . The authors throw a number of well-known machine learning methods at the problem, such as reinforcement learning and time series forecasting, and a combined model which (to no surprise), turns out to be the most accurate. The paper, though, does hold back on a number of fronts: it only gives us results for a small set of hand selected routes, only consider up to 21 days before a departure date (which means that 75% of their users would not be able to achieve any cheaper options), and does not consider any long-term effects like seasonality (to be fair it was before the big data era, hence scalability was an issue for them). . Even so, there are a number of key take-aways in this paper: . Prediction Accuracy &lt; User Utility. When faced with a problem like ‚Äúpredicting the value of X,‚Äù we‚Äôd often be naturally inclined to measure how accurately we predict that value. Instead, these authors measure how well such a prediction could support a user action, in this case ‚Äúwait‚Äù or ‚Äúbuy.‚Äù . Predicting is not the same as informing. The recent announcements from Google show that they have implemented something like this in their flights search. However, they inform users based on historical trends (‚Äúthis fare is likely to increase‚Äù rather than ‚Äúwe predict this fare will be x‚Äù). . Time ordered data does not mean a time series problem. Since the data is ordered by time, one of the ‚Äònatural‚Äô approaches to try was a model that predicts whether a price will go up or down based on its recent behaviour. However, the authors found that flight prices tend to jump‚Ää‚Äî‚Ääeither up or down‚Ää‚Äî‚Ääand so predicting the next price based on what has happened for the last few hours was one of the worst performing approaches. . The authors of this paper later founded Farecast, which was eventually acquired by Microsoft. Bing eventually dropped the Farecast‚Äôs prediction feature but other meta-search engines have developed similar products; Kayak have had a price prediction tool for more than 3 years (see their blog post announcing the feature), and Google has just launched a prediction tool in their recent blog post. . Conclusion . If you are interested in the technical side of this work, check out the paper, or get in touch on twitter. .",
            "url": "http://0.0.0.0:4000/recsys/paper/2016/10/28/The-Machine-Learning-Behind-Farecast.html",
            "relUrl": "/recsys/paper/2016/10/28/The-Machine-Learning-Behind-Farecast.html",
            "date": " ‚Ä¢ Oct 28, 2016"
        }
        
    
  
    
        ,"post19": {
            "title": "Mining large streams of user data for personalized recommendations",
            "content": "Data Scientists across Skyscanner have started meeting every fortnight to discuss research papers that tackle similar problems to those that we face within Skyscanner. The 2nd paper we read was: ‚ÄúMining Large Streams of User Data for Personalized Recommendations‚Äù (hi Xavier!). Just like the last post, we‚Äôre we‚Äôre also writing up a brief, non-technical overview the problems/opportunities we discussed. . Lessons from a Million Dollar Investment in Personalisation . Netflix famously announced a $1M prize in 2006, calling on researchers across the world to improve their movie recommender system by 10%. To create this competition, they had to make a critical decision: how could Netflix measure a 10% improvement in their system? At the time, most recommendation systems worked by predicting how many stars you would give each item (in Netflix‚Äôs case, each film): if they could predict that you would give a film 5 stars, then it would be recommended to you. The company decided that the competition would focus on making those predictions more accurate, by measuring the difference between the prediction and the actual user rating‚Ää‚Äî‚Ääthey used Root Mean Squared Error (RMSE) metric. . Just around the time when researchers started to question whether it was statistically possible to win the competition (e.g., this paper), the prize was won by a team that combined over 100 different algorithms and over 2000 hours of work to create a blend (or ensemble) of predictions that were fantastically accurate. . Many technical innovations came from the competition: matrix factorization algorithms became the state-of-the-art in recommendation systems, neural networks known as Restricted Boltzmann Machines starting coming into vogue (pre-dating the current ‚Äúdeep learning‚Äù hype), and important temporal qualities like when a user rated a film were found to be highly influential in determining how they rated it. . The most surprising result, however, was how little of the thousands of hours of work that went into this competition actually ended up affecting Netflix‚Äôs production systems. In particular: . $1M in research is not $1M in production. Netflix only implemented a handful of the hundreds of algorithms that formed the winning solution. A huge amount of engineering work had to be dedicated to productionise them: scaling the competition algorithms to handle billions of ratings rather than the competition‚Äôs 100M and making the system adaptable to new ratings rather than using the static dataset they had provided were two of the main challenges discussed. . Netflix had ‚Äòmoved on‚Äô from the proxy question they asked. As the business moved towards its online streaming model, it turned out that customer ratings (explicit data) were not the best data to source for recommendations: they now had tons more implicit data from their customers, by just logging how people were using the system (clicks, searches, views, etc.). Predicting a rating accurately was no longer as important as ranking films in the right way with other sources of data. . Netflix had started to optimise for user experience metrics: like engagement, freshness, trust, retention. They came up with a feedback loop of forming hypotheses, training offline models and experiment with them online (via A/B tests). They were able to iterate fast,reject/accept their hypotheses and reason about the results for hundreds of features. A bunch of their recent research papers demonstrate this. . The final point contains the real $1M question: how can data scientists work (offline) on an algorithm and measure improvements that will correlate with improvements in online experiments? It‚Äôs hard but an answer to this could speed up a lot of experimentation! . . Key Takeaways . In the current Netflix system, everything that you see has been, to some extent, tailored to ‚Äúyour‚Äù tastes (Netflix deals with households of customers, so they tailor to a group at a time). This includes the rows of films you see, the ordering of the rows, and the ordering of the films inside each row. . In most web services, implicit and binary feedback is much more readily available and requires no extra effort on the user side. Many algorithms have now been developed to ingest this data to build useful services. . The algorithm is only one piece of the puzzle. The metric (or multiple metrics) you are optimising for, the algorithm under the hood, and the interaction that users have with recommendations can all equally impact the business. They even mention awareness: users should know how well the service is adapting to their tastes, by providing explanations. How do we explain our algorithms to our users? . Are you aware of your users‚Äô context? Even in something as ‚Äòsimple‚Äô as recommending a film, location, time, and other contextual variables can have a very large impact. . Can you explain your algorithms‚Äô effect on our experiments? Data features have an added layer of complexity when it comes to experimentation, as the number of variables increases a lot: offline experiments, model parameters, online experiments, UI biases. We should be able to have better answers (even approximate) than yes/no‚Äî‚Ääwe should be able to answer why‚Äôs, and be able to optimise and adapt while online. . Conclusion . If you are interested in the technical side of this work, check out the paper, or get in touch on twitter,‚Ää‚Äî‚Äämore reading group summaries are on the way‚Ää‚Äî‚ÄäI‚Äôm already behind on these blog posts. .",
            "url": "http://0.0.0.0:4000/recsys/paper/2016/10/14/Mining-Large-Streams-of-User-Data-for-Personalized-Recommendations.html",
            "relUrl": "/recsys/paper/2016/10/14/Mining-Large-Streams-of-User-Data-for-Personalized-Recommendations.html",
            "date": " ‚Ä¢ Oct 14, 2016"
        }
        
    
  
    
        ,"post20": {
            "title": "Solving the continuous cold start problem",
            "content": "Background . Data Scientists across Skyscanner have started meeting every fortnight to discuss research papers that tackle similar problems to those that we face within Skyscanner. We kicked off in late August with this paper from Booking.com and NYU: ‚ÄúSolving the Continuous Cold Start Problem in E-commerce Recommendations‚Äù. As part of the reading group, we‚Äôre also writing up a brief, non-technical overview the problems/opportunities we discussed. . The Problem: ‚ÄúCold¬†Users‚Äù . In many systems across the web, services would like to recommend interesting things (movies, music, restaurants, or travel destinations). How do you recommend something to a user that you have never seen before? . Researchers call this the cold start problem: systems cannot draw any useful inferences about a user before gathering some information about them. This often leads to a vicious cycle: it is hard to gather useful data about users without engaging them, and it is hard to engage them without having some data to personalise their experience. The booking.com researchers argue that, in the travel domain, users are in a continuously ‚Äòcold‚Äô state as their needs change over time, as any data they do have (say, on that backpacking trip you took last year) may no longer be relevant to your current needs (the upcoming family-friendly trip with your children). . Opportunities and Solutions . The gist of the solution outlined in the booking.com paper is about clustering those users that we do have troves of data about, in order to create a set of stereotypical users (formally: cluster centroids). They use data from their destination finder (which asks: what is your passion? Where do you want to go?) to create these groups‚Ää‚Äî‚Ääeven going so far as to include users‚Äô device type when creating clusters. More broadly, each time a user engages with their system, they create a contextual user profile, which includes where a user is, what they look for, and what time of day it is. That way, as soon as a new (cold) user performs any action, the system can quickly identify what ‚Äòstereotype‚Äô (cluster) they most likely approximate‚Ää‚Äî‚Ääand start recommending off the back of that. Although their evaluation is somewhat under reported, they claimed that using this lead to a 20%+ rise in engagement. The main criticism that was discussed, however, was that this work was fairly (and, arguably, unnecessarily) jargon-heavy: the argument that is made about continuous cold start could be made in other domains as well. . What the booking.com researchers discuss is just one of the many solutions that have already been explored to address the cold start problem. In general, they all rely on either obtaining new data quickly, or paving a path towards personalised results by starting with non-personalised recommendations. The approach discussed in this paper, however, highlights the potential of using your company‚Äôs unique (and likely implicit) data to tackle cold-start. This data, alongside approaches that leverage instant personalisation, could quickly make use of any signal we get from users to offer them engaging and interesting experiences. . Conclusion . If you are interested in the technical side of this work, check out the paper, check out our current job openings at Skyscanner, or get in touch on twitter. .",
            "url": "http://0.0.0.0:4000/recsys/paper/2016/09/23/Solving-the-Continuous-Cold-Start-Problem.html",
            "relUrl": "/recsys/paper/2016/09/23/Solving-the-Continuous-Cold-Start-Problem.html",
            "date": " ‚Ä¢ Sep 23, 2016"
        }
        
    
  
    
        ,"post21": {
            "title": "Top-N tips for talking with non-data scientsts",
            "content": "If, like me, you‚Äôre a data scientist in a large startup, then you probably have colleagues who work on things like ‚Äòproduct,‚Äô ‚Äògrowth,‚Äô ‚Äòdesign,‚Äô or ‚Äòuser experience.‚Äô Many of them may not come from a quantitative background‚Ää‚Äî‚Ääbut you‚Äôre still likely to find yourself in meetings where you need to find a way to work with them to build great things. In fact, you‚Äôre probably not going to build great things unless you are working with them. . In terms of tips for working with non-data scientists, most of the online discussions that I‚Äôve seen have been about bridging the gap between data science and engineering (for example, see ‚ÄúLesson 7: The Two Faces of Your ML Infrastructure‚Äù in this presentation). These conversations are concerned with how to efficiently translate between exploration-oriented data science tasks and production-oriented engineering requirements. Talking with a designer, instead, is about the more abstract task of paving a path together to build a great product. So: How do you build a bridge between data science and a non-quantitative discipline? . An Analogy: From Choice Overload to Recommendation . Let‚Äôs step back and consider an analogy: researching decision making vs. building systems that help us overcome choice overload. It‚Äôs a good analogy because it is a domain that originated in the social sciences, and is now a classical data science application: it is an area that has been ‚Äòtackled‚Äô from two very different perspectives. . Psychologists and economists have been studying memory and decision-making for years. Their work predates any efforts to build automated systems that aim to support us while making choices‚Ää‚Äî‚Ääor even make them for us. This research has produced insights ranging from the magical 7 +/- 2 number to how we often make predictably irrational choices. You could argue that all of this research provides us with a conceptual model for reasoning about how we make choices; a framework that we can use to think about this domain. . And yet, none of these insights are fundamentally encoded into the statistical models that we use for recommender systems. Collaborative filtering algorithms are not hard-coded to comply with any of the findings of the psychologists and behavioral economists: if they did, they would hardly be statistical approaches anymore! Statistical models just take our data, crunch the numbers, and spit out some magic‚Ää‚Äî‚Ääthat is what makes them scalable and beautiful. . So, what gives? This analogy is not meant to highlight a mismatch between the two fields. Conceptual models taught us that we are ‚Äòbad‚Äô at making choices‚Ää‚Äî‚Ääthey motivated the need for building recommender systems in the first place. This conceptual model was then translated into a high level question: could data help us navigate choice? That question was then translated into a more specific question: how can I identify the items in a database that a user would be most interested in? Finally, we reach something that can be turned into a statistical model: how can we rank content based on signals of preference? . An Example (and more on conceptual vs statistical models) . Back to cross-disciplinary conversations‚Ää‚Äî‚Äähere‚Äôs an example scenario. I‚Äôm going to use it to continue to pick on one of the biggest causes of misunderstanding between disciplines that I have encountered to date: the word model. Let‚Äôs say your designer walks in and says something along the lines of: . ‚Äò‚Ä¶we‚Äôre working on a model of how our customers make choices in the [travel/health/music/e-commerce/your company‚Äôs] domain. We ran a workshop where participants used experience mapping to draw their journey from [A] to [B]. We figured out that we can segment our users between those who do a lot of [X] and those who do very little‚Ää‚Äî‚Ääand the overall trend of going from no [X] to a lot of [X] is mainly influenced by [Z].‚Äô . As a data scientist, a number of keywords are probably firing off in your head. Model? Probably supervised learning. We should be able to predict what kind of user this is! Segment? Sounds like clustering‚Ää‚Äî‚Ääwe can find these kinds of users in our data! Trend? Time series forecasting‚Ää‚Äî‚Ääwe can predict what a user will do next based on [Z]! . Actually, this is probably not what your design team is trying to communicate at all. More likely, they have been working on conceptual models, segments, and trends: their goal is to create a framework that we can use to explain (to humans) who the customer is and, based on that, design products for them. . This is a vastly different exercise than creating statistical models, which aims to automate (with machines) a particular, well-defined, task. Most importantly, directly converting a conceptual model into a statistical one (in this case: predicting the kind of user, clustering users based on a single behavioral trait, and time series forecasting of user actions) does not necessarily satisfy the requirements of any of the future products that you are working your way towards. . Pushing the other direction is just as fruitless. Let‚Äôs flip the meeting around, and say that you walk in and say: . ‚Äò‚Ä¶we‚Äôre working on a model of how our customers make choices in the [travel/health/music/e-commerce/your company‚Äôs] domain. We looked at 100 million log entries from users in our system who are using it to get from [A] to [B]. We figured out that we can segment the users into [X] groups¬†based¬†by modeling each one as a vector¬†with¬†500¬†features, and predict what each group is going to [click/purchase/read/share] with 99% accuracy. The most discriminant feature was [Z].‚Äô . That‚Äôs pretty dense: you‚Äôve just given a run down of how you could automate a very specific task, such as predicting that a user will click on something. So what? How does it satisfy a customer‚Äôs need, and the company‚Äôs roadmap? Your statistical model will not actually be helpful until you can translate its implications back up to the product. For example: being able to predict that a user is likely to click on something could be used to rank things in order of interest, which allows us to build a personalized experience for these users. . In summary, some tips for cross-disciplinary work . Over the last few years, I‚Äôve had the pleasure of working on a number of projects that involved a healthy dose of qualitative and quantitative methods. I‚Äôve seen variants of the example above when working with designers on speculative data-driven apps, business mentors brainstorming a startup‚Äôs strategy, and behavior-change psychologists who wanted to apply their theories into smartphone apps. There are a number of things to be mindful of: . Data Science is a ‚Äòyoung‚Äô discipline. Many of the people on the other side of the table are probably trying to get to grips with what a data scientist is (after all, so are we), what machine learning is (perhaps from a business perspective), and how data scientists can contribute. | Quantitative and qualitative methods are not adversarial. If you only speak the language of p-values/experimentation and are going to shoot down any argument that is based on design principles, diary studies, interviews, or experience mapping, then you‚Äôre not going to get anywhere. The same goes the other way around: if data-driven insights are dismissed¬†as¬†‚Äòinhuman,‚Äô then progress is unlikely‚Ää‚Äî‚Ääbut part of the onus is on you to present your work in the right way. | There is no hard-rule on whether to do qualitative or quantitative work first. ‚ÄòBig data‚Äô analysis is not an exercise in scaling qualitative research outputs‚Ää‚Äî‚Ääit is about answering different questions altogether. How can any data analysis contribute to a conceptual model? A useful heuristic to go by is that quantitative work is best suited when characterizing those behaviors that it is easier to log than to recall. | Everyone in the room is going to be speaking a different language. Specifically, everyone is likely to be using common words, such as ‚Äòdata,‚Äô ‚Äòmodel,‚Äô ‚Äòsegment,‚Äô and ‚Äòtrend‚Äô‚Ää‚Äî‚Ääbut referring to very different things. If you‚Äôre not aware of this, you‚Äôre likely to have a very frustrating time. | Both quantitative and qualitative methods have their own biases: relish that. | Make sure to clarify the difference between models aimed to explain and models aimed to automate. Don‚Äôt try to automate the model that explains; conversely, remember that the model that automates may not provide a useful explanation. | Most of the conversation is going to revolve around translation. How does their idea, or conceptual model, translate into one that you can design a statistical model around? You‚Äôre probably going to have to take a number of steps here. How does your statistical model translate back up to meeting the needs of the customer? There are interesting methods (like design fiction) that healthily intersect the two. | .",
            "url": "http://0.0.0.0:4000/opinion/2016/07/30/Top-N-tips-for-talking-with-non-Data-Scientists.html",
            "relUrl": "/opinion/2016/07/30/Top-N-tips-for-talking-with-non-Data-Scientists.html",
            "date": " ‚Ä¢ Jul 30, 2016"
        }
        
    
  
    
        ,"post22": {
            "title": "What do we mean when we talk about data-driven products?",
            "content": "I‚Äôve recently had a number of conversations about what it means to design and build data-driven products. They all started on a similar premise: data-driven is the ‚Äòright‚Äô way of doing things, it‚Äôs the future, etc., etc.; so let‚Äôs assume we‚Äôre all on the same page of the hype cycle and, instead, go a little bit deeper: ‚Äúwhat do we mean by data-driven?‚Äù . Is a network router a data-driven product? Is a social networking app data-driven? Is an app that finds the nearest taxi for you data-driven? There are probably ways that you could envisage all of these as data-driven products‚Ää‚Äî‚Ääall of them certainly work with data. However, in many cases, I am not convinced that that popular examples of these things actually are data-driven. So what is it about a thing that makes it data-driven? . From the various conversations I had, it just turns out that we are all going around with a very different understanding of ‚Äòdata-driven.‚Äô I think I‚Äôve found three different meanings to date: . Experimentation. This group of people are usually talking about designing and building products via large-scale online tests. The stereotypical example here is picking a green button for your website because conversion metrics are significantly improved over the purple button. Data-driven means making design decisions based on behavioral evidence from users. | Machine Learning. This group of people are talking about building systems that learn from data in order to provide interesting features to users: recommendation, personalized ranking, people-you-may-know, products-you-may-like, etc. While this group is very likely to be conducting online experiments (as above) as well, the ‚Äòdata-driven‚Äô part usually refers to the fact that the systems are learning from behavioral data generated by users, whether that data is explicit (ratings, reviews) implicit (clicks, views), or somewhere in between (purchases). | Databases or APIs. The final group (and also, unfortunately, the most common) are building systems that only use data; for example, they may query a database or use public APIs, such as those provided by social networking websites. To them, their system is data-driven because it uses data. | I disagree with the third group. The view that just using data equals ‚Äòdata-driven‚Äô makes the whole concept of data-driven completely superfluous and useless‚Ää‚Äî‚Ääyou may as well call it ‚Äòcomputational software‚Äô or ‚ÄòCPU-driven‚Äô and it would probably have the same depth. The first two groups, however, share a critical common theme: they are about behavioral data that was generated as people use the service. . Of course, there is a gray zone between all three of the categories above, and ways that the same product could be built to encompass all three of the groups. . Exploratory Examples . Consider a mobile app that gives you information about public transport around you. A data- based version of this app would pull data from transport operator APIs to give you the latest status updates; perhaps it then filters them to give you location based results. Nothing really data-driven about that. A data- driven version could, instead, learn what part of the transport network is relevant to you and learn to predict when it is that you travel. It could learn when cycling and walking results are relevant, and when they aren‚Äôt. It could detect that you are traveling, and preempt your queries for information about waiting times at your next interchange. Perhaps it could even do away with the need for status updates from transport APIs‚Ää‚Äî‚Ääit could detect that there are delays automagically by sourcing data from its users‚Äô phones. And how would it know that these data-driven features are working? Well, by conducting data-driven experimentation. . We could repeat this thought exercise for a number of data-rich services. . (Update: another example): Consider a website that provides blogging services to users. A data- based version of this site lets users write posts, subscribe to other users‚Äô posts, and get a feed of (perhaps temporally sorted) recent blog posts by people that have been followed. An initial data- driven version could, instead, recommend who to follow, based on the content of your own posts and the posts that you have liked. But who cares about manually following people on a blogging platform? (What if we had to ‚Äòfollow‚Äô directors on Netflix to get their movies?) A more advanced data- driven service could auto-tag your content to allow people to quickly find it, create a relevance-sorted feed of posts, and create digests of not-to-be-missed content‚Ää‚Äî‚Ääregardless of whether you follow someone or not. . What about a meta-search engine, that helps you find and compare flights? The data- based version of this product is an exercise in sourcing flight data from providers, getting you to fill out your trip‚Äôs origin, destination, and dates, and giving you a list of flights that you could take. What would the data- driven parts of this system do? Well, that‚Äôs a question I‚Äôll be exploring at Skyscanner. .",
            "url": "http://0.0.0.0:4000/opinion/2016/06/04/What-do-we-mean-when-we-talk-about-data-driven-products.html",
            "relUrl": "/opinion/2016/06/04/What-do-we-mean-when-we-talk-about-data-driven-products.html",
            "date": " ‚Ä¢ Jun 4, 2016"
        }
        
    
  
    
        ,"post23": {
            "title": "Big data: three challenges",
            "content": "I was recently asked to put together a short blog post as part of this week‚Äôs Data-Driven Economy Week, which is being run by techUK and is ‚Äúexploring the value and importance of data to organisations of all size and sector in the UK‚Äù (#datamatters). . I took the opportunity to jot down some quick thoughts, which largely reflect things that I have been reading from various thought-leaders around the world in the ongoing debate around ‚Äòbig data.‚Äô The original post is here and copied below. . ‚ÄòBig data‚Äô is a wonderfully ambiguous term. Over the last few years, it has evolved to imply business potential, research success, economic growth, and pending opportunity. Big data is forecast to get bigger: the data that we thought of as ‚Äòbig‚Äô a few years ago pales in comparison to the amount available today‚Ää‚Äî‚Ääparticularly as we transition into a world of sensors and Internet-connected things. It has now been linked across a vast range of industries, from journalism to healthcare, transport, and construction. Big data has spawned an entire family of data types: ‚Äòsmall data,‚Äô ‚Äòlifelogging data,‚Äô ‚Äòurban data,‚Äô and more. . All of this is testament to the growth of data literacy across the UK. As a Computer Scientist, it is relieving to witness these once-niche conversations enter the mainstream. However, I often encounter the opinion that simply having or opening (big) data is, in itself, a success. Herein lies one of the biggest challenges of the data movement: data is useless until it is being used to answer the right question and address the right need. Why is this a challenge? . 1. Data‚Äôs value is often serendipitous, not¬†designed . Many successful data companies are based on a serendipitous discovery of an alternative usage of data. Most notably, hyperlinks were not invented to facilitate web search, but that is where PageRank started. The accelerometers that track our activity levels were not built for that purpose; nearly everything that we use smartphones for today is an unintended consequence. The London Oyster card was built as a fare-collection system, but my research examined its potential as way to build personalised transport information systems. . Clearly, facilitating the discovery of secondary applications of data is a path toward designing the next wave of data-driven services. The most common question I am approached with is ‚Äúwe have this data: what could we do with it?‚Äù Yet, many of the discussions around protecting privacy naturally focus on scoping how data can be used. How can we resolve privacy without debilitating future innovation? . 2. Data does not exonerate . There are a number of data-centric projects that have ‚Äòfailed‚Äô spectacularly. For example, Microsoft‚Äôs AI bot started making offensive and racist remarks after learning from tweets. It is not the first system to accidentally cause offence: data and algorithms (or, more generally, automation) often blinds us to implicit biases, and masks unfairness. . Many of these examples seem to stem from an all-in approach to building a data-driven system, rather than using incremental, iterative experiments to get these systems online. They equally seem to be built without acknowledging that we often do not understand how many digital systems are or will be used. Perhaps the Tay bot‚Äôs racism could have been picked up before it had a global audience; perhaps the lessons learned from the Samaritan suicide-prevention app could have learned quicker. How can we go beyond data literacy, and instill a culture of experimentation and uncertainty into how we design and build data-driven products? . 3. Data alone is not sustainable . A flurry of devices and services‚Ää‚Äî‚Ääparticularly in the Internet-of-Things market‚Ää‚Äî‚Ääare essentially data collection engines, that promise to (one day) become sustainable, perhaps via advertising or some kind of subscription. As a data scientist, I often feel that more importance and effort is being given to designing and building data-collecting things, rather than long-term, enganging, and sustainable services. . In particular, it is interesting to see what happens when services like this smart home hub are discontinued, and what that may mean for future services that rely on data and cloud systems to operate. . No amount of data is going to replace the creativity we need to see new, sustainable business models emerge ‚Äîsuch as this project that aims to let users control and benefit from their own data‚Ää‚Äî‚Ääthat go beyond turning every screen we see into more advertising space. . Conclusion . It is an exciting time to work as a data scientist in the UK: institutions like techUK demonstrate the UK‚Äôs ambitions and enthusiasm to lead when it comes to building the next generation of data-driven services. However, when it comes to big data, we must remember that there is little value in just hoarding it: data‚Äôs true value is often unexpected, the services we build must not simply throw data at a problem, and future leaders are likely to be just as creative with their business model as they are with their data wrangling. .",
            "url": "http://0.0.0.0:4000/opinion/2016/04/15/Big-data-three-challenges.html",
            "relUrl": "/opinion/2016/04/15/Big-data-three-challenges.html",
            "date": " ‚Ä¢ Apr 15, 2016"
        }
        
    
  
    
        ,"post24": {
            "title": "Intuition in machine learning",
            "content": "I‚Äôve just finished Week 5 of the Coursera/Stanford Machine Learning course. It has been a mixture of refreshing, relearning, and new for me. I had already been using, building, and researching/evaluating machine learning algorithms for a number of years. I therefore felt like I ‚Äòknew‚Äô a lot of the concepts, particularly the introductory ones. I put ‚Äòknew‚Äô in quotes, however, since I‚Äôve always had a feeling that I don‚Äôt know them well enough, no matter how many times I‚Äôve used them. . It has therefore been particularly refreshing to hear Andrew Ng recognize that the inner workings of ML algorithms often don‚Äôt make intuitive sense. At the end of this week‚Äôs lecture videos, he talks about how, although he has been using neural networks for years (indeed, he is behind some of the biggest breakthrough technologies in this domain!), he sometimes feels he doesn‚Äôt quite grasp how they work. . What struck me when going through this course, though, is that there doesn‚Äôt seem to be a single way to learn machine learning concepts‚Äî we create an intuitive understanding of algorithms by approaching them from different perspectives. I‚Äôve put an example with four perspectives below. All of these approaches are used in the Coursera course, so why do feelings of inadequacy remain? . Example: gradient¬†descent . Here are four perspectives of the same concept, and some reasons why each of them is incomplete. . Perspective #1: Motivations . You have an ML algorithm that has certain parameters. You don‚Äôt know what those parameter values should be, and would like to learn them. What you do know is how costly (or how much error happens when) using certain values. The goal of gradient descent is to find the set of parameter values that minimise your cost function. To do that, gradient descent starts at an initial value, and then ‚Äòsteps‚Äô the parameters towards values that reduce the cost. It keeps doing that until the improvement in those steps is very small. . Pros: this perspective is a great place to start. It gives you a high-level view of what is supposed to happen. It allows you to see beyond the ML algorithm itself, and think about what you are trying to achieve. . Cons: this perspective does not actually give us enough information to use this method. Furthermore, the high-level way that we talk about a problem constrains us to think about that problem in a particular way. For example, recommender system design is constrained by the way we think about it, as a system of ‚Äòusers‚Äô and ‚Äòitems‚Äô (this metaphor does not fit many practical applications where we want to build a personalised system). . Rating: essential, but insufficient. . Perspective #2: Math and Equations . Here is the update rule for gradient descent (Credit: Week 1 coursework, Coursera/Stanford Machine¬†Learning): . . Pros: At its heart, machine learning is about understanding various mathematical concepts: from simple things like addition and multiplication to linear algebra, optimisation, and matrix operations. This is the ‚Äòmeat‚Äô of the algorithm. . Cons: If you‚Äôve been away from these concepts for some time, then diving straight into machine learning from this perspective is not ideal. There are differences in how researchers write these equations and you need to be fully versed in the notation. For example, the snippet above uses h(x) to denote a regression model‚Äôs prediction, which I have often seen as hat{y}. If you come from a Computer Science/programming background, some pseudocode may be much easier to read than an equation. . Rating: important in theory, often difficult/inaccessible in practice. . Perspective #3:¬†Pictures . Here is what you get if you plot a simple linear regression‚Äôs parameters vs. the cost, as a surface plot (left) and contour plot (right). The goal of gradient descent is to get to the lowest vertical point in the surface plot (Credit: Week 1 coursework, Coursera/Stanford Machine¬†Learning). . . Pros: visualisations are the quickest way to grasp a simple concept. It is immediately clear where the ‚Äòlowest‚Äô point of the surface plot is. For non-convex problems, a similar plot will immediately show that there are multiple ‚Äòlow‚Äô points, so understanding local minima is very easy as well. . Cons: Visualisation does not scale to high-dimensional problems, so it is limited to toy/practice scenarios. . Rating: good for simple concepts and educational examples, otherwise usually unsuitable. . Perspective #4: Code and¬†Practice . If putting machine learning theory into practice is your goal, you cannot get away from code. Here is how you fit a simple linear model in scikit-learn, which is what all of the work on gradient descent was leading up to. . from sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(X_train, y_train) predictions = model.predict(X_test) . Pros: A lot of imperative programming languages are very different from learning about the ‚Äòmaths‚Äô of ML, or knowing how things work in theory. My confidence in ‚Äòknowing‚Äô how a particular algorithm works is usually proportional to how much of it I could implement from scratch. Reading others‚Äô code is a great way to understand, step-by-step, what is happening under the hood. . Cons: There are actually quite a few here. . The Coursera/Stanford course uses Octave, which I have not had any reason to use before (and I do not know why I would use it again). I would imagine that it would take quite a bit of potentially duplicated work to design algorithms with Octave and then move them into production in some other language. | The scikit-learn example above has not taught us anything: many machine learning packages are becoming so good that you don‚Äôt need to be a mechanic to drive the car anymore. | When it comes to kaggle competitions, it is not practical to both implement algorithms from scratch (to learn) as well as actually compete (to perform)‚Äî so competing feels like more of an exercise in feature engineering and parameter tuning. | . Rating: essential in practice, but there is a trade-off between coding to learn and coding to build a useful system. . Conclusion: keep (re-)learning . All of the work that I‚Äôve done that has used machine learning started during my PhD, so there while there are some concepts I have never used (e.g., backpropagation, this week‚Äôs Coursera topic) I realized that I have never formally been taught any of them‚Ää‚Äî‚ÄäI learned by doing. It‚Äôs been great to re-learn so many things from different perspectives, and pay more attention to the details that I typically would overlook. . Quora has recently hosted a number of sessions with well-known machine learning (ML) researchers and practicioners. One of the questions that they all got was: ‚ÄúHow should you start a career in Machine Learning?‚Äù Xavier (who is a great mentor, colleague, and friend!), Andrew, and Ralf all provided interesting insights, that range from the theory to the practice of machine learning. . They suggest learning about querying large datasets, linear algebra, optimization, code/software engineering, and using online courses and machine learning competitions to hone your skills. While there are differences in the details of what they suggest, there is one common theme: learning about machine learning does not end. .",
            "url": "http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html",
            "relUrl": "/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html",
            "date": " ‚Ä¢ Mar 11, 2016"
        }
        
    
  
    
        ,"post25": {
            "title": "Archetypes of online conversations",
            "content": "I‚Äôve been reading a lot about the imminent age of smart messaging and virtual assistants. Some great reads, offering a variety of perspectives (from business through to design and more) include ‚Äú2016 will be the year of conversational commerce,‚Äù ‚ÄúClippy‚Äôs revenge‚Ää‚Äî‚Ääsmart messaging as platform shift,‚Äù ‚ÄúNo-UI is the new UI,‚Äù and ‚ÄúA Question of Agency.‚Äù . I have to admit that I‚Äôm very curious about how this ‚Äònew‚Äô way of interacting with each other and with machines will develop. In recent conversations with various friends, developers and entrepreneurs, I‚Äôve started putting together a view on how messaging fits into the broader set of motifs that appear in online communication. . I‚Äôve narrowed it down to 4 groups, with the last being a placeholder for uncategorised services. Each of them allows‚Ää‚Äî‚Ääor prevents‚Ää‚Äî‚Ääus from conversing in different ways, and is headed in slightly different directions. . This is what I have learned so far. . Group #1. Messaging: Gaining popularity . A recurring theme of messaging is that it is the ‚Äònatural‚Äô way that we talk to each other. This is probably very specific to your age. If you are part of the generation that grew up with SMS, IM clients (Hotmail, AOL, etc.), and chat forums, then yes: you‚Äôve been trained in the ways of the text. Writing short sentences this way is easy and can even be more polite. If you‚Äôre old enough, you will remember the days of the magical T9 keyboard; the days before you knew that an SMS had been read, and before replies were visually structured into conversations. . This medium is regaining popularity due to services like WhatsApp and Slack. They are services that facilitate chatting, and their simplicity is what makes them great. They don‚Äôt have to have fancy user interfaces. They are relatively history-less: the things you post do not become part of your ‚Äòprofile.‚Äô You converse 1-to-1 or in small(ish) groups of contacts. You can just write, without nesting replies. . It seems that a lot of the novelty here is that these services do not do the things that online social networks do (which is category #2, below). . So where is this headed? In short, bots. We‚Äôre starting to talk directly to algorithms‚Ää‚Äî‚Ääby issuing commands while we talk to each other. The chat screen is becoming action-oriented: you can order a cab, book a meeting, throw down a lunch order, and get things done while you type. You can also chat with a sleep expert algorithm. WhatsApp is going to help us communicate with businesses that we want to hear from. The chat window is also a naturalistic way for algorithms to talk back to us and even, perhaps, talk on behalf of us. There is a lot of scope to reimagine how smartphone (and watch) notifications fit into this group, and how smartphones may ‚Äòregress‚Äô into what they were like before (i.e., no apps) as we continue down this path. . Group #2. Broadcasting: Getting boring . The novelty of messaging is that it stands in constrast with the infamous status update. This is the primary mode of ‚Äòconversing‚Äô on Facebook, Twitter, Google+, and LinkedIn. These kinds of feeds started out as a linear list of events that were sorted by when they occurred. Everything that you post becomes part of your profile; your digital history accumulates for anyone to relive at any time. . To give credit where it is due: this category introduced the likes, +1s, shares, stars, favourites and other ways to give feedback without actually needing to say any words. This group also introduced and popularised command-like text: we were not RT‚Äôing, HT‚Äôing, MT‚Äôing, via‚Äôing, #hashtagging, and @-replying before the feed became our primary mode of online conversation. However, this group also introduced trolling, online shaming, and pull-to-refresh FOMO. . This medium is now being redesigned, with most of the efforts seeming to go into re-sorting the feed, and moving away from time-sorted and towards some kind of ‚Äòrelevance‚Äô-sorted feeds. This is the ‚Äòwhile you were away,‚Äô the ‚Äòhighlights,‚Äô the ‚Äòtop stories,‚Äô and all the other variants that broadly remind me of turning on a television news channel: ‚Äúif you‚Äôre just joining us right now, our top stories this hour are‚Ä¶‚Äù . The recurring difficulty that I‚Äôve heard about the broadcast category is that you don‚Äôt really ‚Äòconverse‚Äô with status updates. You announce. A friend (note: who does not do anything machine learning related) was the first to point out that re-ranking his newsfeed is surfacing the content of contacts who post the most, which also tends to be the most boring stuff (albeit perhaps the most ‚Äòliked‚Äô). Muting them just surfaces the 2nd most boring contact‚Äôs onslaught of content, and he is having to resort to manually searching for people he cares about. In doing so, this is pushing him away from these services altogether. . The relationship between ranking, time, and status remains unsolved, and something needs to be done to keep it relevant. . Group #3.¬†Digests: Quietly growing. . Introverts, rejoice! The third group is about one-way conversations. It is about content that takes time to consume. These used to be the editorials, the magazines, the books that we would sit down with‚Ää‚Äî‚Ääand become fully immersed. They are not real-time, they are you-time. . I came across this category when I noticed how many newsletters/digests I had subscribed to: coaching tips about Brazilian Jiu-Jitsu, reading digests from investment firms, data science curiosities, and digests of curated web content. These are emails that I‚Äôm happy to leave sitting somewhere until I have the time and focus to read them. . This category is part of the interesting conversation around slowing down media consumption to a human pace. There are interesting examples emerging that highlight how this category is evolving towards algorithmically curated content. Both Medium and Quora now send their personalised digests, which I find broadly interesting. Spotify‚Äôs Discover Weekly (emphasis mine) is a great example of distilling what could otherwise be a constant, fast, real-time, ‚Äòkeep-going‚Äô experience of music discovery into a weekly event (that I look forward to). . Group #4: Unclassified &amp;¬†Missing . Where do these¬†fit? The three groups above don‚Äôt cater to a number of big names, which mostly seem to overlap between different categories. YouTube comes to mind immediately as a place where there is a lot of great content that would fit the Digest, Broadcast, and Messaging groups. Instagram splits the crowd‚Ää‚Äî‚Ääbetween a Broadcast, visual Message, and Digest of memories. Productivity tools like Trello allow us to converse in a thematic, structured way, and digest work done while we plan ahead. Tinder has popularised the swipe-feed, which is sorted like a stack rather than a list: it looks like a variant of the Broadcast group, freed of temporal constraints and limited to a binary yes/no feedback. And, yesterday I received a Message-type email (‚ÄúI‚Äôm here‚Äù) as well as a Digest-type one (1,000+ words). Good ol‚Äô email keeps eluding any kind of consistency. . Finally, a missing group. How do we introspect‚Ää‚Äî‚Ääconverse with our own data, whether this be about our fitness, health, or other? I‚Äôve ranted a few times about the inadequacies of dashboards, which are not fit for purpose. The analogy I tend to use is that if Netflix took this approach, it would be telling you that 58% of the time, you watch action movies, rather than recommending the next one you may be interested in. . Do you have any thoughts? Let me know. (Find a way to converse with me.) .",
            "url": "http://0.0.0.0:4000/opinion/2016/02/09/Archetypes-of-online-conversations.html",
            "relUrl": "/opinion/2016/02/09/Archetypes-of-online-conversations.html",
            "date": " ‚Ä¢ Feb 9, 2016"
        }
        
    
  
    
        ,"post26": {
            "title": "What's next?",
            "content": ". The past four years have been an incredible experience. The Systems Research Group in the Computer Lab is a playground of intelligent, fun, open, curious, and driven researchers. I‚Äôve had the chance to work with many people across the entire spectrum of the University, build systems used by thousands of people worldwide, and create incredible data sets which are likely to yield many awesome papers in the coming years. . Amazing Mentors . A highlight of the recent months was working with the team of coaches and mentors at Accelerate Cambridge, in the Judge Business School. I cannot speak highly enough of this family of driven, enthusiastic, and incredibly supportive people. Working under the tutelage of coaches like Hanadi, Kaye, Chris, and Charles has been an empowering experience, and every time I walk out of the JBS I‚Äôm filled with newfound motivation for building, doing, and pursuing opportunities that lie ahead. . I‚Äôm looking forward to continuing to work with them. If you‚Äôre looking to cross paths with me in Cambridge, it will most likely be there. . . Starting, not¬†ending . Research life doesn‚Äôt end abruptly, if it even ever ends at all. Asking questions (and buildings tools to try and answer them) is probably more of a state of mind, rather than a state of employment. . On a practical note, I‚Äôm still involved with a number of conferences and workshops; I‚Äôm still involved in papers that are under submission, or being written/revised, and I‚Äôm still collaborating with a number of researchers who are using Easy M for Android. You can check out Easy M‚Äôs mailing list here (update: this mailing list no longer exists). . Let‚Äôs connect . In the coming weeks I‚Äôll be exploring all of the different opportunities that London has to offer, and savour my transition into the ‚Äúreal world.‚Äù I‚Äôm looking forward to the unknown unknowns that lie ahead, and will be using this blog to recount some highlights and tell some stories. . If you have any exciting data projects, get in touch with me by email or on Twitter. . .",
            "url": "http://0.0.0.0:4000/life/2016/02/01/What-s-next.html",
            "relUrl": "/life/2016/02/01/What-s-next.html",
            "date": " ‚Ä¢ Feb 1, 2016"
        }
        
    
  
    
        ,"post27": {
            "title": "Share data or perish",
            "content": "The International Committee of Medical Journal Editors is seeking feedback on their proposal (pdf) to require authors to share the deidentified individual-patient data, or IPD, that was used to generate research results ‚Äúas a condition for consideration for publication.‚Äù The motivation behind this is to foster a culture of reproducibility: . ‚ÄúThe data underlying the results are defined as the IPD required to re-produce the article‚Äôs findings, including necessary metadata.‚Äù . I was interested in this proposal since data sharing is already widely practiced in Computer Science research. For example, see the ICWSM datasets, the CRAWDAD repository, or the annual ACM KDD Cup. . In general, data sharing is something that I strongly agree with. In fact, my PhD research was made possible by the data shared as part of the Netflix Prize and the MovieLens project. . So how could clinical trial data be any different? This post shares the feedback that I submitted. . 1. Sharing data as a condition for publication . tl;dr: My main concern is with the term ‚Äúde-identified individual‚Äù data. . In August 2006, AOL shared a large dataset of ‚Äúde-identified individual‚Äù query logs for research purposes. Although people were identified in the data by an anonymous identifier, individuals could be located by cross-referencing queries with public phonebook listings. In October 2006, Netflix released a large dataset of ‚Äúde-identified individual‚Äù movie ratings as part of a research competition. Again, people were identified by an anonymous identifier, but individuals could be identified by cross-referencing queries with public IMDB ratings (which led to a law suit). . It is clear that the utility of sharing a dataset is for other researchers to be able to replicate and build on findings. While there are techniques for anonymizing data to a certain degree (e.g., k-anonymity) it is not clear that using these will still allow research findings to be replicated. Further, sharing data as a ‚Äúrequirement‚Äù for publication seems to shift the incentive towards sharing data quickly, rather than sharing data safely. . 2. Sharing data within 6¬†months . tl;dr: 6 months is an incentive for sharing quickly, not safely. . As above, a possible concern is that the time frame requirement will push researchers to share the data, rather than take the necessary steps to share it safely. . 3. Planning to share data when registering a¬†trial . This point seems obvious. I didn‚Äôt leave any feedback here (also, I‚Äôve been prevented from sharing data from systems I‚Äôve created since I didn‚Äôt get consent about data sharing at the outset). . 4. Providing credit to those who share¬†data . tl;dr: This should not be a problem. The academic community operates on citations. . Computer Scientists very frequently collect research data from public sources (e.g., social media) and/or write software that they use in their research. Many of them publish a finding and then release some data‚Ää‚Äî‚Ääwith an open (unenforced) request that should the data be used for further research, their original publication should be cited. . More details: See how the hugely popular scikit-learn project asks for credit here. Also, ‚Äúresearch parasites‚Äù is one of the most ridiculous terms I‚Äôve come across recently. . 5. Other¬†Comments . tl;dr: What about the edge cases? Do participants opt-in, or opt-out? . There are some cases that are worth considering. . Is the data still useful if, once appropriately ‚Äúde-identified,‚Äù research findings cannot be replicated? | What if an insufficient number of trial participants give consent for their data to be shared (i.e., the data that can be shared does not allow for research findings to be replicated)? | What will happen to trial recruitment when prospective participants are told that their data will be shared? | Perhaps most importantly, should researchers be denied publication if they cannot share their data? | . And I clicked submit. Some final thoughts. . I don‚Äôt think that the data sharing debate is going to be concluded by putting strict data sharing rules in place: the way this proposal is currently framed sounds like a recipe for disaster. If sharing users‚Äô 1‚Äì5 stars put Netflix in court, what will happen when we share clinical trial data? . More broadly, those advocating for sharing data seem to be doing so because that is the only way they see that certain things (e.g. reproducing findings) will be allowed to happen. Would data sharing practices still be enforced if we had the right tools that allowed us to reproduce findings without having to download a dataset? . Kaggle, for example, already has a tool (called scripts) that allows data scientists to run ‚Äúone-click analyses‚Äù without any data download. This is the right direction to go: What we really need (and what some smart researchers are working on) are tools that allow for data to be safely queried. Until those tools see the light of day, all data sharing enforcement is going to do is put us (as researchers) and trial participants/system users (as data points) into murky and dangerous waters. .",
            "url": "http://0.0.0.0:4000/opinion/2016/01/29/Share-data-or-perish.html",
            "relUrl": "/opinion/2016/01/29/Share-data-or-perish.html",
            "date": " ‚Ä¢ Jan 29, 2016"
        }
        
    
  
    
        ,"post28": {
            "title": "Playing with char-rnn and the NIPS 2015 data",
            "content": "Since samim published all those awesome and fun posts on using a Recurrent Neural Network to generate text (see: Zen-RNN, TED-RNN, Obama-RNN), I‚Äôve been looking for an opportunity to try the char-nn library myself. . An opportunity came up after all of the papers at this year‚Äôs Neural Information Processing Systems Conference (NIPS 2015) appeared online. What is more suitable to play around with* an RNN than a bunch of papers that talk a lot about RNNs? . The NIPS 2015¬†dataset . The dataset was provided as part of this (kudos-only, curiosity-driven) kaggle competition. It includes a CSV file that contains all of the papers‚Äô text, extracted from the papers accepted this year. . Preprocessing . The text data (extracted from the PDFs using the tool pdftotext) is incredibly noisy: pdftotext does not extract page numbers, carriage-return broken words, equations, section headers, variables, figure captions, tables, footnotes, and all the other complexity the researchers can put together with various LaTeX commands into simple text. The result is text that we clearly do not want to train anything with. Here‚Äôs a relatively common looking example: . pŒ≤ T xpiq ¬¥ Œ≤ T y pjq q2 Mij ‚Äú max&lt;br&gt;min Œ≤ T WM Œ≤&lt;br&gt;(3)&lt;br&gt;Œ≤PB&lt;br&gt;||Œ≤||0 ƒèk . This text was ‚Äúgenerated‚Äù by a human. For this experiment, I was looking to train using full sentences, which are actually a rarity in technical/algorithmic papers. They tend to (from my experience) be found in the Abstract, Introduction, Related Work, Discussion/Conclusion sections. I therefore filtered each paper‚Ää‚Äî‚Äämanually removing everything after the introduction and before the conclusion (but this varied between papers)‚Äî and yes, in case you were wondering, this was a huge pain and wasn‚Äôt done perfectly. . Note: I did try a bunch of regexs first (and used some to remove things like references), but could only get so far: there was a lot of variance in section headings, and in general too many cases/outliers/exceptions to automate this sensibly. . Training . The char-rnn library makes life very easy. Once Torch (etc.) has been installed, and all the papers have been put into a single input.txt file, training the model is one line of code away. . I used the standard setting with a higher dropout value (0.5) to minimise overfitting‚Ää‚Äî‚Ääespecially since the data remains quite noisy. . Comparative, Nonsense¬†Results . So here are some results. Example human entries come from picking a random line out of the input data that contains the seed text; machine text is then generated with the same seed. . Seed: ‚ÄúThe task we aim to solve‚Äù . Example Human: ‚ÄúThe task we aim to solve is to fill in gaps of multiple consecutive data points in high-dimensional binary time series data.‚Äù . Machine: ‚ÄúThe task we aim to solve the proof of the inference learning problem provided by which and algorithms in the method methods are possible to the posterior and computationally of the state time to the demonstration are expected in constraints, in the local minimax.‚Äù . Seed: ‚ÄúIn this paper, we‚Äù . Example Human: ‚ÄúIn this paper, we propose a novel parameter estimator for models on discrete space, which does not require calculation of the normalization constant.‚Äù . Machine: ‚ÄúIn this paper, we propose the input information of f is increased for each prediction and the minimax method is better than the following distributions and can be robust to analyze the tree image mint discretization to the computational experiments of s vret and only possible to be the acceleration rime in the linear regret in the possible data size.‚Äù . Seed: ‚ÄúTo conclude, ‚Äù . Example Human: To conclude, we show a set of optimized pseudo-inputs Z in black in Figure 1. We also superimpose the marginal of any single point in the realization, which is available through the spectral decomposition of L here¬†.‚Äù . Machine: ‚ÄúTo conclude, the distribution constraints of each relative possible development decomposition of the submodular scale goal is the expected probability of the true of the recovery information in the prediction for parameters are step is the algorithm vore an analyzed method that this analysis after random nature by the wide procedure of maximum complexity sets.‚Äù . Playful Seeds . Seed: ‚ÄúThe meaning of life is ‚Äù . Machine: ‚ÄúThe meaning of life is shown to extend both similar state of our linear expectation in a computer variance of the level of produced controlled distributed the minimum truanding eigenvalue.‚Äù . Seed: ‚ÄúDinner tonight is ‚Äù . Machine: ‚ÄúDinner tonight is shown to extend a learning interest of size point of a function because then one been respectively.‚Äù . Seed: ‚ÄúMachine learning research should ‚Äù . ‚ÄúMachine learning research should be generalized in networks of the case of the only quadratic shared images.‚Äù . Seed: ‚ÄúMy predictions are ‚Äù . ‚ÄúMy predictions are related to the prior of the linear and non-not gradient function both from the model is to also consider the MDM of the posterior and define the true of the state that with large optimal analysis defined as the computation p.‚Äù . Some thoughts . Machines aren‚Äôt going to be taking over anytime soon! . Extracting high-quality text from a PDF is harder than training an RNN (a.k.a. ‚Äúmachine learning,‚Äù as this NIPS paper points out, is a small part of ‚Äúmachine learning systems.‚Äù). . This is the first time that I play with the char-rnn library. It‚Äôs likely that people who know it a bit better could get better results. I was left wondering whether some further preprocessing could have made for a better training dataset. For example, some kind of clustering could have been used to identify papers that are similar (e.g., comparison over bags of words)‚Ää‚Äî‚Ääto then train only on one particular cluster. . With thanks¬†to: . Previous posts by samim. | The authors and contributors of char-nn on github | Kaggle, for the data, and for being a generally awesome place. Found here | . [*] This is not science. .",
            "url": "http://0.0.0.0:4000/machine-learning/tools/2015/12/14/Playing-with-char-rnn-and-the-NIPS-2015-data.html",
            "relUrl": "/machine-learning/tools/2015/12/14/Playing-with-char-rnn-and-the-NIPS-2015-data.html",
            "date": " ‚Ä¢ Dec 14, 2015"
        }
        
    
  
    
        ,"post29": {
            "title": "mHealth: evidence vs. agility",
            "content": "While mHealth apps are flooding the market, the majority of apps that are currently available lack scientific evidence about their efficacy. I have found that this point isn‚Äôt debated as much as it should: in research settings, it is taken for granted as a problem. . Why should this be debated more? I‚Äôve created two perspectives: . Group 1: Pro-Evidence . The pro-evidence camp are asking a number of very important questions. Do health apps really change behaviour? Are they safe, evidence-based tools that do what they claim? Arguments here always draw comparisons between software and medicine: . ‚ÄúWe need to do high-quality clinical studies and apply the same standards that you would to a new pharmaceutical.‚Äù (Torous, Scientific American) . This seems very sensible. Particularly if we want to reach a world where clinicians can prescribe software: they need to know that it works. . Instead, it seems that health apps have gone the other direction. A study of the content of 47 smoking cessation apps shows an inverse relation between app popularity and behavioural evidence; there are similar papers for chronic pain management apps; for melanoma diagnostic apps; for opioid conversion apps; and the content of cancer apps. . Group 2: Pro-Agility . The pro-agility camp, instead, are grounded in very practical matters. Most importantly, in practice, software is not medicine. Current clinical trial methods make testing health apps infeasible. The only way that I‚Äôve seen trials being conducted is by means of a linear, incremental pipeline of steps that takes a drug from the lab (idea/phase 1) to the market (product/phase 4). This makes sense for drugs‚Ää‚Äî‚Ääin software terms, this is (roughly speaking) the waterfall model of development. Ask your developer friends what they think about that. . Developing and evaluating software linearly is not only expensive and ineffective, it also ties your evaluation to ‚Äòbugs‚Äô that could easily be fixed along the way‚Ä¶ not to mention technology that will be outdated tomorrow: . ‚ÄúLocking down these interventions locks in defects and eliminates the opportunities for quality improvement and adaptation to the changing technological environment, often leading to validation of tools that are outdated by the time that trial results are published.‚Äù (Mohr et. al, JMIR). . Some thoughts . There is a problem with the word mHealth app‚Ää‚Äî‚Ääit is too broad. It currently includes health-tracking apps, health-calculator apps, health-information apps, health-diagnostic apps, and health-social networking apps. All of these are very different things. . There is a problem with the word evaluate ‚Äî‚Ääit is too broad. For example, this Twitter poll asked if robust user reviews were sufficient to evaluate a mental health app: the discussion includes (true) things like ‚Äúusers can‚Äôt evaluate the whether the app encrypts their data.‚Äù Determining that an app complies with certain engineering requirements such as encryption certainly doesn‚Äôt need a clinical trial. . One of the overriding problems here is that a health app‚Äôs outcomes fundamentally rely on its implementation. Usability matters; it may even trump evidence. Would you use an app that is based on the best evidence, but ugly as hell? Papers from clinicians (for example, this one) seem to recognise this, often claiming that their ‚Äòfuture work‚Äô (read: stuff that couldn‚Äôt work for this study) includes figuring out how to engage and retain users for a longer time. Are they best suited to be tackling design and engagement issues? . ###¬†Conclusion It sounds like there is a lot more to this debate to unpack and discuss. Yes, there is insufficient evidence for mHealth app effectiveness. Yet our methods for generating evidence are ineffective. Yes, we need some kind of framework to build better mHealth apps; yet, we shouldn‚Äôt build an insurmountable wall of requirements that will prevent any innovation. . A highly compelling resolution that I‚Äôve seen here advocates for a ‚Äúto each to their own approach:‚Äù research should focus on principles, and let technologists take care of implementing them into engaging services: . ‚ÄúWe argue that RCTs will have greater scientific and public health value if they focus on the evaluation of intervention principles‚Äù (Mohr et. al, JMIR). . This seems sound, but‚Ää‚Äî‚Ääas above‚Ää‚Äî‚Ääfails to recognise that the success of some intervention principles may rely on their design/implementation. . There is a final, broader, point: I have yet to meet a patient who really cares about statistical evidence and validation. Patients talk about ease of use, interactivity, engagement, and personal utility‚Ää‚Äî‚Ääthings that are not measured (or statistically significant) outcomes of clinical trials; things that are developed by iterating and running A/B tests. Are patients therefore leaning towards the pro-agility approach? Perhaps the best we can do, for now, is follow the advice of the NHS Choices webpage: . The only way to know if an app works for you is to download and try it.‚Äù (NHS Choices) .",
            "url": "http://0.0.0.0:4000/opinion/research/2015/12/04/mHealth-evidence-vs-agility.html",
            "relUrl": "/opinion/research/2015/12/04/mHealth-evidence-vs-agility.html",
            "date": " ‚Ä¢ Dec 4, 2015"
        }
        
    
  
    
        ,"post30": {
            "title": "Why I reject your data papers",
            "content": "No baseline. You present results of your wonderful new machine learning algorithm compared only to variations of itself (e.g., results with different parameters). No baseline or alternative approach results presented. Quantitative results on data only make relative (not absolute) sense: If your algorithmic magic is 99% accurate, is a simple linear regression baseline 98% or 58% accurate? There‚Äôs a huge difference here. . Excessive/unmotivated data pruning. You prune your dataset to fit your particular use case, and do not give any concrete motivation for doing so. The most common of these is the ‚Äúwe removed all of the users with fewer than X entries,‚Äù with no explanation as to why (overall), why X rather than some other value, what effect this had, or what would happen to users who do not have X amount of data. . Your metrics aren‚Äôt measuring your claims. You‚Äôre claiming an increase in user satisfaction (or long-term engagement, or recommendation quality), but are only measuring prediction accuracy. . You‚Äôve evaluated your algorithm manually. You evaluate your algorithm by manually inspecting test instances. ‚ÄúHey look! It works for these two cases, so surely this is cool!‚Äù I admit that I have sometimes done this while debugging algorithms, but (as far as I recall) I have never reported results in this way. If you‚Äôre going to do this, you need to (a) not be the one evaluating (i.e., run a user study), and (b) do this on way more than the handful of instances that you can do. . You do more than one thing on this list. . And some minor issues to keep in mind: . I haven‚Äôt read every paper out there, and assuming that I‚Äôm going to read all of the papers you cite (‚Äúwe do what [76] does.‚Äù) is a stretch. | Related work should discuss why work is related. It‚Äôs called ‚ÄúRelated‚Äù work, and not ‚ÄúHere‚Äôs a list of what others have done‚Äù | As a reader, I will skip your ‚Äúthe rest of this paper is organized as follows‚Äù paragraph. I don‚Äôt care much about knowing how your paper is structured; I would much rather you spend the time/space telling me about what you have found and its implications. | While I know that some people feel very strongly about this, I don‚Äôt care if your related work is Section 2, or Section [Conclusion -1] as long as it is readable where it is. Are you discussing related work results compared to yours? That should come after you have presented your results. Are you discussing related work as a motivation for yours? Perhaps that should appear after the Introduction. | . Update: here is an interesting suggested code of conduct for reviewing (and criticizing) research. .",
            "url": "http://0.0.0.0:4000/opinion/research/2015/12/02/Why-I-reject-your-data-papers.html",
            "relUrl": "/opinion/research/2015/12/02/Why-I-reject-your-data-papers.html",
            "date": " ‚Ä¢ Dec 2, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "üëã Hi, I‚Äôm Neal - I‚Äôm currently the Machine Learning Lead at Monzo in London, where we‚Äôre focusing on building machine learning systems that optimise the app and help the company scale. . ‚úàÔ∏è Before joining Monzo, I was a Data Scientist at Skyscanner, where I built recommender and ranking systems to improve travel information in the app. . üè´ Before Skyscanner, I was a Senior Research Associate in the Computer Lab at the University of Cambridge, working on healthcare mobile apps that use smartphone sensors. I spun out this research into a startup that was part of Accelerate Cambridge in the Judge Business School. . üéì I did my MSci, PhD, and first postdoctoral research position in the Department of Computer Science at University College London, where I am still an Honorary Research Associate. . My PhD focussed on methods for evaluating collaborative filtering algorithms over time. While at UCL, I also spent time as a visiting researcher at Telefonica Research, Barcelona and worked as a Data Science consultant. . My work has always focused on applications that use machine learning - this has taken me from recommender systems to urban computing and travel information systems, digital health monitoring, smartphone sensors, and banking. You can read more about my work and research in the Press &amp; Speaking and Research sections. .",
          "url": "http://0.0.0.0:4000/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Research",
          "content": "Upcoming Conferences . 14th ACM Conference on Web Search and Data Mining, Program Committee. 2021. . 14th ACM Conference on Recommender Systems, Senior Program Committee. 2020. . A full list is on this archive page. . PhD Research . Evaluating Collaborative Filtering Over Time . N. Lathia. Department of Computer Science, University College London. June 2010. . Supervisors: Prof. Stephen Hailes, Prof. Licia Capra . Research Publications . The Dark Triad of personality and momentary affective states: an experience sampling study . I. Pilch, N. Lathia, K. Wiesebach. In Current Issues in Personality Psychology. Volume 8(1), 2020. . Effectiveness of providing university students with a mindfulness-based intervention to increase resilience to stress: a pragmatic randomised controlled trial. . J. Galante, G. Dufour, M. Vainre, A. Wagner, J. Stochl, A. Benton, N. Lathia, E. Howarth, P. Jones. In The Lancet Public Health. Accepted November 2017. [The Guardian, The Daily Mail, Punjab Tribune] . When User Experience Designers Partner with Data Scientists . F. Girardin, N. Lathia. In AAAI Spring Symposium on Designing the User Experience of Machine Learning Systems. Palo Alto, CA, USA. March 2017. . Mobile Sensing at the Service of Mental Well-being: a Large-Scale Longitudinal Study . S. Servia-Rodr√≠guez, K. K. Rachuri, C. Mascolo, P. J. Rentfrow, N. Lathia, G. M. Sandstrom. In Proceedings of 26th International World Wide Web Conference. Computational Health Track. Perth, Australia. April 2017. . Happier People Live More Active Lives: Using Smartphones to Link Happiness and Physical Activity . N. Lathia, G. M. Sandstrom, C. Mascolo, P. J. Rentfrow. In PLoS ONE. January 2017. [Press release, New York Times] . Putting Mood in Context: Using Smartphones to Examine How People Feel in Different Locations . G. M. Sandstrom, N. Lathia, C. Mascolo, P. J. Rentfrow. Journal of Research in Personality. Accepted June 2016. . The Feasibility of a Context Sensing Smoking Cessation Smartphone Application (Q Sense): A Mixed Methods Study . F. Naughton, S. Hopewell, N. Lathia, R. Schalbroeck, C. Brown, C. Mascolo, S. Sutton. JMIR mHealth uHealth. Accepted May 2016 [MRC Network Magazine, Science Magazine]. . Mobile-Based Experience Sampling for Behaviour Research . V. Pejovic, N. Lathia, C. Mascolo, M. Musolesi. Chapter in Emotions and Personality in Personalized Systems. Accepted June 2015. . Opportunities for Smartphones in Clinical Care: The Future of Mobile Mood Monitoring . G. Sandstrom, N. Lathia, C. Mascolo, P. Rentfrow. The Journal of Clinical Psychiatry. May 2015. . Comparing Cities‚Äô Cycling Patterns Using Online Shared Bicycle Maps . A. Sarkar, N. Lathia, C. Mascolo. In Springer Transportation, Special Issue on Emerging, Passively Generated Datasets for Travel Behavior and Policy Analysis. April 2015. . Tube Star: Crowd-Sourced Experiences on Public Transport . N. Lathia, L. Capra. In the 11th International Conference on Mobile and Ubiquitous Systems. London, UK. December 2014. . Group Colocation Behavior in Technological Social Networks . C. Brown, N. Lathia, A. Noulas, C. Mascolo, V. Blondel. In PLoS ONE. August 2014. . Smartphones for Large-scale Behaviour Change Interventions . N. Lathia, V. Pejovic, K. Rachuri, C. Mascolo, M. Musolesi, P. Rentfrow. In IEEE Pervasive Computing, Special Issue on Understanding and Changing Behavior, Accepted May 2013. . Contextual Dissonance: Design Bias in Sensor-Based Experience Sampling Methods . N. Lathia, K. Rachuri, C. Mascolo, P. Rentfrow. In ACM International Joint Conference on Pervasive and Ubiquitous Computing. Zurich, Switzerland. September 8-12, 2013. . Open Source Smartphone Libraries for Computational Social Science . N. Lathia, K. Rachuri, C. Mascolo, G. Roussos. In 2nd ACM Workshop on Mobile Systems for Computational Social Science. Zurich, Switzerland. September 8, 2013. . Probabilistic Group Recommendation via Information Matching . J. Gorla, N. Lathia, S. Robertson, J. Wang. In 22nd International World Wide Web Conference. Rio de Janeiro, Brazil. May 13-17, 2013. . Individuals Among Commuters: Building Personalised Transport Information Services from Fare Collection Systems . N. Lathia, C. Smith, J. Froehlich, L. Capra. In Elsevier Pervasive and Mobile Computing, Special Issue on Pervasive Urban Applications, Accepted October 2012. . Measuring the Impact of Opening the London Shared Bicycle Scheme to Casual Users . N. Lathia, S. Ahmed, L. Capra. In Elsevier Transportation Research Part C, January 2012. [Salon] . Mining User Mobility Features for Next Place Prediction in Location-based Services . A. Noulas, S. Scellato, N. Lathia, C. Mascolo. In IEEE International Conference on Data Mining. Brussels, Belgium. December 10-13, 2012. . A Random Walk Around the City: New Venue Recommendation in Location-Based Social Networks . A. Noulas, S. Scellato, N. Lathia, C. Mascolo. In IEEE International Conference on Social Computing. Amsterdam, Netherlands. September 3-5, 2012. . The Hidden Image of the City: Sensing Community Well-Being from Urban Mobility . N. Lathia, D. Quercia, J. Crowcroft. In Pervasive 2012. Newcastle, UK. June 18-22, 2012. [New Scientist] . Using Control Theory for Stable and Efficient Recommender Systems . T. Jambor, J. Wang, N. Lathia. In 21st International World Wide Web Conference. Lyon, France. April 16-20, 2012. . Personalizing Mobile Travel Information Services . N. Lathia, L. Capra, D. Magliocchetti, F. De Vigili, G. Conti, R. De Amicis, T. Arentze, J. Zhang, D. Cal√¨, V. Alexa. In Transport Research Arena. Athens, Greece. April 23-26, 2012. . How Smart is Your Smartcard? Measuring Travel Behaviours, Perceptions, and Incentives . N. Lathia, L. Capra. In 13th ACM International Conference on Ubiquitous Computing. Beijing, China. September 17-21, 2011. . Mining Mobility Data to Minimise Travellers‚Äô Spending on Public Transport . N. Lathia, L. Capra. In ACM SIGKDD 2011 Conference on Knowledge Discovery and Data Mining. San Diego, California, USA. August 21-24, 2011. [BBC News] . Mining Public Transport Usage for Personalised Intelligent Transport Systems . N. Lathia, J. Froehlich, L. Capra. In IEEE International Conference on Data Mining. Sydney, Australia. December 14-17, 2010. . Recommending Social Events from Mobile Phone Location Data . D. Quercia, N. Lathia, F. Calabrese, G. Di Lorenzo, J. Crowcroft. In IEEE 2010 International Conference on Data Mining. Sydney, Australia. December 14-17, 2010. . Temporal Diversity in Recommender Systems . N. Lathia, S. Hailes, L. Capra, X. Amatriain. In ACM SIGIR 2010 Annual Conference on Research &amp; Development on Information Retrieval. Geneva, Switzerland. July 19-23, 2010. . Temporal Collaborative Filtering with Adaptive Neighbourhoods . N. Lathia, S. Hailes, L. Capra. In ACM SIGIR 2009 Annual Conference on Research &amp; Development on Information Retrieval. Boston, Massachusetts, USA. July 19-23, 2009. . The Wisdom of the Few: A Collaborative Filtering Approach Based on Expert Opinions from the Web . X. Amatriain, N. Lathia, J.M. Pujol, H. Kwak, N. Oliver. In ACM SIGIR 2009 Annual Conference on Research &amp; Development on Information Retrieval. Boston, Massachusetts, USA. July 19-23, 2009. . kNN CF: A Temporal Social Network . N. Lathia, S. Hailes, L. Capra. In ACM Conference on Recommender Systems 2008. Lausanne, Switzerland. October 23-25, 2008. . Trust-Based Collaborative Filtering . N. Lathia, S. Hailes, L. Capra. In Joint iTrust and PST Conferences on Privacy, Trust Management and Security (IFIPTM). Trondheim, Norway. June 18-20, 2008. . The Effect of Correlation Coefficients on Communities of Recommenders . N. Lathia, S. Hailes, L. Capra. In 23rd Annual ACM Symposium on Applied Computing. Trust, Recommendations, Evidence and other Collaboration Know-how (TRECK) Track. Fortaleza, Brazil. March 16-20, 2008. . Private Distributed Collaborative Filtering using Estimated Concordance Measures . N. Lathia, S. Hailes, L. Capra. In ACM Conference on Recommender Systems 2007. Minneapolis, Minnesota, USA. October 19-20, 2007. . Book Chapters . Location Recommendation with Social Media Data . C. Bothorel, N. Lathia, R. Picot-Clemente, A. Noulas. Chapter 16 in Brusilovsky, P., He, D. (eds.) Social Information Access. May 2018. . The Anatomy of Mobile Location-Based Recommender Systems . N. Lathia. In the Recommender Systems Handbook (2nd Edition). . The Human Sensor: Bridging Between Human Data and Services . N. Lathia. Chapter in the Handbook of Human Computation. Springer Publishing. October 2013. . Computing Recommendations With Collaborative Filtering . N. Lathia. Chapter 2 in Collaborative and Social Information Retrieval and Access: Techniques for Improved User Modeling. IGI Global Publishing. July 2008. . Workshop Publications . Using Idle Moments to Record Your Health via Mobile Applications . N. Lathia. In ACM Workshop on Mobile Systems for Computational Social Science. Low Wood Bay, Lake District, UK. June 25, 2012. . Temporal Defenses for Robust Recommendations . N. Lathia, S. Hailes, L. Capra. In ECML/PKDD Workshop on Privacy and Security Issues in Data Mining and Machine Learning. Barcelona, Spain. September 24, 2010. . Evaluating Collaborative Filtering Over Time . N. Lathia, S. Hailes, L. Capra. In ACM SIGIR 2009 Workshop on the Future of IR Evaluation. Boston, Massachusetts, USA. July 23, 2009. . Collaborative Filtering With Adaptive Information Sources . N. Lathia, X. Amatriain, J.M. Pujol. In IJCAI Workshop on Intelligent Techniques for Web Personalization and Recommender Systems. Pasadena, California, USA. July 11-17, 2009. . Posters, Abstracts, Demos, Proceedings . Bootstrapping a Destination Recommender System . N. Lathia. In ACM Conference on Recommender Systems 2017 (Industry Track). Como, Italy. August 27-31, 2017. . Assessing the Relationship Between Personality and Adaption to University Life Using Mobile Phones . S. Muller, G. Sandstrom, N. Lathia, C. Mascolo, J. Rentfrow. In European Conference on Personality. Timisoara, Romania. July 19-23, 2016. . Speed of engagement with support generated by a smoking cessation smartphone Just In Time Adaptive Intervention (JITAI) . F. Naughton, C. Brown, N. Lathia, S. Hopewell, S. Sutton, C. Mascolo. In 2nd Behaviour Change Conference: Digital Health and Wellbeing. London, UK. February 24-25, 2016. . Using Mobile Technology to Understand Student Adjustment . S. R. M√ºller, G. Sandstrom, N. Lathia, C. Mascolo, J. Rentfrow. In Society for Personality and Social Psychology. San Diego, California, USA. January 28-30, 2016. . Happy People have Active Lives: Sensing Happiness using Smartphone Accelerometers . N. Lathia, G. Sandstrom, J. Rentfrow and C. Mascolo. In International Conference on Computational Social Science. Helsinki, Finland. June 8-11, 2015. . The Development of a Novel Sensing Smoking Cessation App (Q Sense) . F. Naughton, N. Lathia, S. Hopewell, B. Brafman-Price, C. Mascolo, A. McEwen &amp; S. Sutton. In UK Society for Behavioural Medicine&lt;. Nottingham, UK. December 3-4, 2014. . The Development of a Context Aware Smoking Cessation app (QSense) . F. Naughton, N. Lathia, B. Brafman-Price, C. Mascolo, A. McEwen, S. Sutton. In European Health Psychology Society. Innsbruck, Austria. August 26-30, 2014. . Proceedings of the Workshop on Location-Aware Recommendations (LocalRec) . P. Bouros, N. Lathia, M. Renz, F. Ricci, D. Sacharidis. CEUR Workshop Proceedings 1405. co-located with the 9th ACM Conference on Recommender Systems (RecSys 2015). . Using Ratings to Profile Your Health . N. Lathia. In ACM Recommender Systems (Demo Track). Dublin, Ireland. September 9-13, 2012. .",
          "url": "http://0.0.0.0:4000/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "",
          "content": "2021 . 14th ACM Conference on Web Search and Data Mining | . 2020 . 14th ACM Conference on Recommender Systems (Senior Program Committee) | . 2019 . International Joint Conference on Artificial Intelligence | ACM Recommender Systems (Senior Programme Committee) | ACM RecSys Challenge | . 2018 . ACM Recommender Systems (Senior Programme Committee) | ACM Conference on Knowledge Discovery and Data Mining | ACM Conference on Research and Development in Information Retrieval | Conference on User Modeling, Adaptation and Personalization | World Wide Web Conference (Posters) | Workshop on Online Recommender Systems and User Modeling: continuous learning from web data | . 2017 . ACM Recommender Systems | ACM Conference on Knowledge Discovery and Data Mining | ACM Conference on Intelligent User Interfaces | ACM Symposium on Applied Computing (Recommender Systems: Theory and Applications) | ACM Conference on Human Factors in Computing Systems (External) | World Wide Web Conference (User Modeling Personalization and Experience, Posters) | Conference on User Modeling, Adaptation and Personalization | Florida Artificial Intelligence Research Society Conference (Special Track on Recommender Systems) | . 2016 . ACM Recommender Systems | ACM Conference on Pervasive and Ubiquitous Computing (External) | ACM Conference on Interactive Experiences for TV and Online Video (External) | World Wide Web Conference (Behavioral Analysis and Personalization, Crowdsourcing Systems and Social Media Tracks) | Conference on User Modeling, Adaptation and Personalization | Florida Artificial Intelligence Research Society (RecSys Track) | Nordic Conference on Human-Computer Interaction (Industry Experience Track) | Workshop on Engendering Health with Recommender Systems | Workshop on on Emotions and Personality in Personalized Systems | Workshop on Personalisation and Adaptation in Technology for Health | Workshop on Intelligent Personal Guidance of Human Behavior Utilizing Anticipatory Models | Workshop on Pervasive Urban Application | Workshop on Sensing Systems and Applications Using Wrist Worn Smart Devices | Workshop on the Impact of Human Mobility in Pervasive Systems and Applications | . 2015 . ACM Recommender Systems | ACM Conference on Intelligent User Interfaces | Springer Recommender Systems Handbook (2nd Edition) | Conference on User Modelling, Adaptation, and Personalization | Florida Artificial Intelligence Research Society (RecSys Track) | Innovative Technology in Mental Healthcare | Symposium on Pervasive Computing Paradigms for Mental Health | Workshop on Intelligent Personalization | Workshop on Location-aware Recommendations | Workshop on Emotions and Personality in Personalized Systems | Workshop on Pervasive Urban Applications | Workshop on the Impact of Human Mobility in Pervasive Systems and Applications | Workshop on Sensing Systems and Applications Using Wrist Worn Smart Devices | . 2014 . ACM Recommender Systems | ACM Multimedia | ACM Conference on Pervasive and Ubiquitous Computing (External) | IEEE Conference on Pervasive Computing and Communications | Conference on Pervasive Computing Technologies for Healthcare | Conference on Mobile Systems and Pervasive Computing | World Wide Web Conference (User Interfaces, Human Factors, and Mobile Devices) | City Labs Workshop | Workshop on Emotions and Personality in Personalized Services | Workshop on Context-Awareness in Retrieval and Recommendation | Workshop on Social Personalisation | . 2013 . ACM Conference on Information and Knowledge Management | ACM Conference on Pervasive and Ubiquitous Computing (Social Media Chair) | ACM Conference on Research and Development in Information Retrieval | AAAI Conference on Weblogs and Social Media | Workshop on Emotions and Personality in Personalized Services | Workshop on Benchmarking Adaptive Retrieval and Recommender Systems | Workshop on Personal Data Meets Distributed Multimedia | Workshop on When the City Meets the Citizen | Workshop on Pervasive Urban Applications (co-organiser) | Workshop on Mobile Systems for Computational Social Science | Workshop on SenCity: Uncovering the Hidden Pulse of a City | Workshop on Citizen Sensor Networks | . 2012 . ACM Conference on Information and Knowledge Management | ACM Conference on Research and Development in Information Retrieval (Posters) | ACM Conference on Pervasive and Ubiquitous Computing (External) | IEEE Conference on Pervasive Computing | Conference on Mobile and Ubiquitous Computing | Workshop on Personalization in Mobile Applications (co-organiser) | Workshop on Recommendation Utility Evaluation: Beyond RMSE | Workshop on Context-Aware Recommender Systems | Workshop on When the City Meets the Citizen | Workshop on Mobile Systems for Computational Social Science | Workshop on Pervasive Urban Applications | . 2011 . Joint Conference on Artificial Intelligence | ACM Conference on Research and Development in Information Retrieval (Posters) | ACM Conference on Pervasive and Ubiquitous Computing (External) | Workshop on Trust and Privacy in Distributed Information Sharing | Workshop on Novelty and Diversity in Recommender Systems | Workshop on Context-Awareness in Retrieval and Recommendation | Workshop on Information Heterogeneity and Fusion in Recommender Systems | Challenge on Context-Aware Movie Recommendation | Workshop on Personalization in Mobile Applications (co-organiser) | . 2010 . ACM Conference on Research and Development in Information Retrieval | ACM Symposium on Applied Computing | IEEE Conference on Social Computing | Workshop on Music Recommendation and Discovery | Workshop on Search and Mining User-Generated Contents | Workshop on the Mobile Social Web | . 2009 . ACM Conference on Web Search and Data Mining (Late-breaking) | ACM Symposium on Applied Computing | IEEE Conference on Social Computing | IEEE Conference on the Applications of Digital Information and Web Technologies | Workshop on Computational Trust for Self-Adaptive Systems | Workshop on Soft Computing for Recommender Systems | Workshop on Information Heterogeneity and Fusion in Recommender Systems | . 2009 . IEEE Conference on the Applications of Digital Information and Web Technologies | Workshop on Trust in Mobile Environments | Workshop on The Value of Security through Collaboration | . Journals . IEEE Transactions on Knowledge and Data Engineering | World Wide Web Journal | IEEE Pervasive Computing | Elsevier Pervasive and Mobile Computing Journal | .",
          "url": "http://0.0.0.0:4000/archive/reviewing.html",
          "relUrl": "/archive/reviewing.html",
          "date": ""
      }
      
  

  
  

  
  

  

  
  

  
  

  

  

}