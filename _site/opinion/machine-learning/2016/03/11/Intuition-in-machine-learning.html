<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Intuition in machine learning | Neal Lathia</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Intuition in machine learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Which perspective of an algorithm makes more sense?" />
<meta property="og:description" content="Which perspective of an algorithm makes more sense?" />
<link rel="canonical" href="http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html" />
<meta property="og:url" content="http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html" />
<meta property="og:site_name" content="Neal Lathia" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-03-11T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Which perspective of an algorithm makes more sense?","headline":"Intuition in machine learning","dateModified":"2016-03-11T00:00:00-06:00","datePublished":"2016-03-11T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html"},"url":"http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="Neal Lathia" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Intuition in machine learning | Neal Lathia</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Intuition in machine learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Which perspective of an algorithm makes more sense?" />
<meta property="og:description" content="Which perspective of an algorithm makes more sense?" />
<link rel="canonical" href="http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html" />
<meta property="og:url" content="http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html" />
<meta property="og:site_name" content="Neal Lathia" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-03-11T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Which perspective of an algorithm makes more sense?","headline":"Intuition in machine learning","dateModified":"2016-03-11T00:00:00-06:00","datePublished":"2016-03-11T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html"},"url":"http://0.0.0.0:4000/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="Neal Lathia" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Neal Lathia</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/research/">Research</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Intuition in machine learning</h1><p class="page-description">Which perspective of an algorithm makes more sense?</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2016-03-11T00:00:00-06:00" itemprop="datePublished">
        Mar 11, 2016
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#opinion">opinion</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#machine-learning">machine-learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>I’ve just finished Week 5 of the <a href="https://www.coursera.org/learn/machine-learning">Coursera/Stanford Machine Learning</a> course. It has been a mixture of refreshing, relearning, and new for me. I had already been using, building, and researching/evaluating machine learning algorithms for a number of years. I therefore felt like I ‘knew’ a lot of the concepts, particularly the introductory ones. I put ‘knew’ in quotes, however, since I’ve always had a feeling that I don’t <em>know</em> them <em>well enough</em>, no matter how many times I’ve used them.</p>

<p>It has therefore been particularly refreshing to hear Andrew Ng recognize that the inner workings of ML algorithms often don’t make intuitive sense. At the end of this week’s lecture videos, he talks about how, although he has been using neural networks for years (indeed, he is behind some of the <a href="https://www.technologyreview.com/s/600766/10-breakthrough-technologies-2016-conversational-interfaces/">biggest breakthrough technologies</a> in this domain!), he sometimes feels he doesn’t quite grasp how they work.</p>

<p>What struck me when going through this course, though, is that there doesn’t seem to be a single way to learn machine learning concepts— we create an intuitive understanding of algorithms by approaching them from different perspectives. I’ve put an example with four perspectives below. All of these approaches are used in the Coursera course, so why do feelings of inadequacy remain?</p>

<h2 id="example-gradientdescent">Example: gradient descent</h2>
<p>Here are four perspectives of the same concept, and some reasons why each of them is incomplete.</p>

<h3 id="perspective-1-motivations">Perspective #1: Motivations</h3>
<p>You have an ML algorithm that has certain parameters. You don’t know what those parameter values should be, and would like to learn them. What you do know is how <em>costly</em> (or how much error happens when) using certain values. The goal of gradient descent is to find the set of parameter values that minimise your cost function. To do that, gradient descent starts at an initial value, and then ‘steps’ the parameters towards values that reduce the cost. It keeps doing that until the improvement in those steps is very small.</p>

<p><strong>Pros:</strong> this perspective is a great place to start. It gives you a high-level view of what is supposed to happen. It allows you to see beyond the ML algorithm itself, and think about what you are trying to achieve.</p>

<p><strong>Cons:</strong> this perspective does not actually give us enough information to use this method. Furthermore, the high-level way that we talk about a problem constrains us to think about that problem in a particular way. For example, recommender system design is <a href="https://www.quora.com/What-is-the-next-big-thing-in-recommendation-systems/answer/Neal-Lathia">constrained by the way we think about it</a>, as a system of ‘users’ and ‘items’ (this metaphor does not fit many practical applications where we want to build a personalised system).</p>

<p><strong>Rating:</strong> essential, but insufficient.</p>

<h3 id="perspective-2-math-and-equations">Perspective #2: Math and Equations</h3>
<p>Here is the update rule for gradient descent (Credit: Week 1 coursework, Coursera/Stanford Machine Learning):</p>

<p><img data-width="1284" data-height="154" src="https://cdn-images-1.medium.com/max/800/1*CmOytMmmdbYEZL0v2-rIOw.png" /></p>

<p><strong>Pros:</strong> At its heart, machine learning is about understanding various mathematical concepts: from simple things like addition and multiplication to linear algebra, optimisation, and matrix operations. This is the ‘meat’ of the algorithm.</p>

<p><strong>Cons:</strong> If you’ve been away from these concepts for some time, then diving straight into machine learning from this perspective is not ideal. There are differences in how researchers write these equations and you need to be fully versed in the notation. For example, the snippet above uses <code class="highlighter-rouge">h(x)</code> to denote a regression model’s prediction, which I have often seen as <code class="highlighter-rouge">\hat{y}</code>. If you come from a Computer Science/programming background, some pseudocode may be much easier to read than an equation.</p>

<p><strong>Rating:</strong> important in theory, often difficult/inaccessible in practice.</p>

<h3 id="perspective-3pictures">Perspective #3: Pictures</h3>
<p>Here is what you get if you plot a simple linear regression’s parameters vs. the cost, as a surface plot (left) and contour plot (right). The goal of gradient descent is to get to the lowest vertical point in the surface plot (Credit: Week 1 coursework, Coursera/Stanford Machine Learning).</p>

<p><img data-width="1302" data-height="650" src="https://cdn-images-1.medium.com/max/800/1*Jh2dA0bPj_MatLKK5wcnFQ.png" /></p>

<p><strong>Pros:</strong> visualisations are the quickest way to grasp a simple concept. It is immediately clear where the ‘lowest’ point of the surface plot is. For non-convex problems, a similar plot will immediately show that there are multiple ‘low’ points, so understanding local minima is very easy as well.</p>

<p><strong>Cons:</strong> Visualisation does not scale to high-dimensional problems, so it is limited to toy/practice scenarios.</p>

<p><strong>Rating:</strong> good for simple concepts and educational examples, otherwise usually unsuitable.</p>

<h3 id="perspective-4-code-andpractice">Perspective #4: Code and Practice</h3>
<p>If putting machine learning theory into practice is your goal, you cannot get away from code. Here is how you fit a simple linear model in scikit-learn, which is what all of the work on gradient descent was leading up to.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">linear_model</span> <span class="n">import</span> <span class="n">LinearRegression</span>

<span class="k">model</span> <span class="p">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="k">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>


<span class="n">predictions</span> <span class="p">=</span> <span class="k">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Pros:</strong> A lot of imperative programming languages are very different from learning about the ‘maths’ of ML, or knowing how things work in theory. My confidence in ‘knowing’ how a particular algorithm works is usually proportional to how much of it I could implement from scratch. Reading others’ code is a great way to understand, step-by-step, what is happening under the hood.</p>

<p><strong>Cons:</strong> There are actually quite a few here.</p>
<ul>
  <li>The Coursera/Stanford course uses <a href="https://www.gnu.org/software/octave/">Octave</a>, which I have not had any reason to use before (and I do not know why I would use it again). I would imagine that it would take quite a bit of potentially duplicated work to design algorithms with Octave and then move them into production in some other language.</li>
  <li>The scikit-learn example above has not taught us anything: many machine learning packages are becoming so good that you don’t need to be a mechanic to drive the car anymore.</li>
  <li>When it comes to kaggle competitions, it is not practical to <em>both</em> implement algorithms from scratch (to learn) as well as actually compete (to perform)— so competing feels like more of an exercise in feature engineering and parameter tuning.</li>
</ul>

<p><strong>Rating:</strong> essential in practice, but there is a trade-off between coding to learn and coding to build a useful system.</p>

<h2 id="conclusion-keep-re-learning">Conclusion: keep (re-)learning</h2>
<p>All of the work that I’ve done that has used machine learning started during my PhD, so there while there are some concepts I have never used (e.g., backpropagation, this week’s Coursera topic) I realized that I have never <em>formally</em> been taught any of them — I learned by doing. It’s been great to re-learn so many things from different perspectives, and pay more attention to the details that I typically would overlook.</p>

<p>Quora has recently hosted a number of sessions with well-known machine learning (ML) researchers and practicioners. One of the questions that they all got was: “How should you start a career in Machine Learning?” <a href="https://www.quora.com/How-should-one-start-a-career-in-machine-learning-2">Xavier</a> (who is a great mentor, colleague, and friend!), <a href="https://www.quora.com/How-should-you-start-a-career-in-Machine-Learning">Andrew</a>, and <a href="https://www.quora.com/How-should-one-start-a-career-in-machine-learning-3">Ralf</a> all provided interesting insights, that range from the theory to the practice of machine learning.</p>

<p>They suggest learning about querying large datasets, linear algebra, optimization, code/software engineering, and using online courses and machine learning competitions to hone your skills. While there are differences in the details of what they suggest, there is one common theme: <strong>learning about machine learning does not end.</strong></p>

  </div><a class="u-url" href="/opinion/machine-learning/2016/03/11/Intuition-in-machine-learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nlathia" title="nlathia"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/neal_lathia" title="neal_lathia"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
