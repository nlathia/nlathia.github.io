		<h2>Temporal Diversity in Recommender Systems</h2>

			<div class="entry">
				<p>This year I&#8217;ll be presenting a paper &#8220;Temporal Diversity in Recommender Systems&#8221; at <a href="http://sigir2010.org/doku.php">SIGIR in Geneva</a>,  which I wrote with my supervisors (<a href="http://www.cs.ucl.ac.uk/staff/s.hailes/">Steve Hailes</a>, <a href="http://www.cs.ucl.ac.uk/staff/l.capra/">Licia Capra</a>) and <a href="http://research.tid.es/xar/">Xavier Amatriain</a> from Telefonica Research in Barcelona. As you can expect from the paper&#8217;s title, we&#8217;ll be appearing in the session on <a href="http://sigir2010.org/doku.php?id=program:sessions">Filtering and Recommendation</a>.<a id="session_3bfiltering_and_recommendation" name="session_3bfiltering_and_recommendation"></a></p>
<p>The paper is one of the latest publications based on the work I was doing for my PhD (by the way, I submitted my thesis earlier this month! You can see the abstract, and eventually download it, from <a href="http://www.cs.ucl.ac.uk/staff/n.lathia/publications/thesis.html">here</a>). The elevator pitch for nearly four year&#8217;s work would be something like this:</p>
<blockquote><p>Collaborative filtering has always been studied from a <em>static</em> viewpoint: by designing algorithms that can predict hidden test set ratings really well (the Netflix prize is the perfect example of that). However, when recommender systems are deployed, they will face a <em>dynamic</em> environment: users will keep rating things over time, and the system will need to be updated to take into account people&#8217;s latest ratings. My thesis examined this rift, by looking at how various dimensions of recommendations change as recommender systems are updated.</p>
</blockquote>
<p>When I give this sort of pitch to anyone who has an interest in recommender systems, they usually say: ah, but wait, what about that <a href="http://research.yahoo.com/pub/2824">awesome KDD paper</a> by Koren on the temporal dynamics of recommendations? And I say: yes, that is an awesome paper, and (from what I gathered) he is dealing with shifting <em>customer </em>preferences. In my work, I take a more <em>system</em>-oriented perspective; I look at how things change as you re-train your CF algorithm every week.</p>
<p>So, a bit of background. Leading up to the paper that this post is about, we had a few other contributions: at <a href="http://www.cs.ucl.ac.uk/staff/n.lathia/publications/recsys08.html">RecSys 2008</a> we looked at how similarity between users evolves over time, as they rate more stuff; at <a href="http://www.cs.ucl.ac.uk/staff/n.lathia/publications/sigir09.html">SIGIR 2009</a> we looked at the effect of updating a recommender system on temporal accuracy.</p>
<p>The next step in the story was to look at how ranking of recommendations changes over time. And here, we hit onto the question: what happens if, as users keep rating stuff, their recommendations do not change at all? We decided that the best way to find out was to run a user survey. Actually, we designed three. Each survey simulated a &#8220;popular&#8221; movie recommender system over five &#8220;weeks.&#8221; In other words, respondents would see week 1&#8242;s recommendations, rate how much they liked them, and then click through buffer screens to get to the following week&#8217;s recommendations. In the first survey, we gave them the most popular movies of all time, every week. Nothing changed. The second survey, instead, gave popular movies, but slightly changed them each week: it introduced different popular movies into the top list. The last survey just threw up random movies.</p>
<p>There were two interesting outcomes from the survey. The first, which we report in the <a href="http://www.cs.ucl.ac.uk/staff/n.lathia/publications/sigir09.html">paper</a>, relates to how people rated recommendations as they did (or did not change). Popular recommendations that changed were consistently rated highly. Random movies were consistently rated low. But popular movies that did not change were rated less and less each week; by week 5 they were rated as bad as the random movies. An interesting point about this is that when people rate, they aren&#8217;t just inputting their preference for a movie- their rating also reflects how much they like the recommendation that they are being served. The second interesting outcome was the angry emails I got: when recommendations didn&#8217;t change, people wrote to me to tell me that my system &#8220;sucked&#8221; or had a bug in it. At the broadest level, one of the conclusions we</p>
<div style="display:none"></div>
</p>
<p> drew from the survey is that temporal change to recommendations  is important: people don&#8217;t like being recommended the same things over and over again.</p>
<p>Based on this result, we performed a large scale analysis of state of the art algorithms: how much change do they offer? What impacts the amount of change? What can be done to promote diversity? All the results, and (hopefully) some food for thought, are in the paper <a href="http://www.cs.ucl.ac.uk/staff/n.lathia/publications/sigir10.html">here</a>.</p>
<p>Xavier and I will be at SIGIR 2010: we are looking forward to seeing you there too!</p>
<div id="_mcePaste" style="position: absolute; left: -10000px; top: 0px; width: 1px; height: 1px; overflow: hidden;">
<h5><a id="session_3bfiltering_and_recommendation" name="session_3bfiltering_and_recommendation">Session 3B: Filtering and  Recommendation</a></h5>
</div>
