---
layout: post
title: "Beyond Movie Recommendations: Solving the Continuous Cold Start Problem in E-commerce Recommendations"
tags: [recsys, paper]
excerpt_separator: <!--more-->
---

A review of a paper by Kiseleva, Tuzhilin, Kamps, Mueller, Bernardi, Davis, Kovacek, Einarsen, Hiemstra.

<!--more-->
<hr />

Available: [http://arxiv.org/abs/1607.07904](http://arxiv.org/abs/1607.07904)

### Background

The **standard cold start problem** emerges when a recommender system is unable to recommend anything until
 the new user/item gets 'warmed up' with data; e.g. what to recommend to a new user with no history?
 
Past research uses baselines, combines collaborative and content-based filtering, asks for ratings from the
 user, offers a wide range of (diverse) content, or uses social filtering.
 
These approaches rely on a 'short' warm-up period and assumes that data received remains relevant.

Context-Aware Recommendation augments 'traditional' recommendation with information about the current context.
It can be used to pre/post filter recommendations (when to select items that are relevant to current context), or 
when modeling preferences directly.

### Problem / Research Questions

The **continuous cold start problem**: where users remain cold for a long time, or 'cool down' as their
 needs change over time. Example: business vs leisure travel, sparse travel throughout the year, while
 many new items keep getting added to the system. Characterised by changes in data sparsity, volatility,
 identity, and user personas.

* How to characterize the continuous cold start problem in travel recommendation?
* How to define and discover contextual user profiles from multi-criteria ranking data in an unsupervised way?
* How to apply contextual user profiles for the ranking of travel destinations in a continuous cold start setting?
* How effective are contextual profiles for real-world users of the destination finder system in terms of engagement?

### Approach

Situational context provides 'powerful cues about user preferences that hold the promise to improve
 the quality of recommendations over the user of traditional long term interests.' However, context 'can be almost 
 anything'; will look at server logs as they are more widely available in practice. Overall, a very low amount of actual
 detail about the method, which can be summarised as: clustering to find profiles and a similarity metric to match a user
 to an existing profile.

#### Dataset

Data is based on 'endorsements' that users give for places after visiting a hotel in that area in the Booking.com
 [destination finder](http://www.booking.com/destinationfinder.html). There are 256 activities that they can select from;
 approximately 5M reviews, from which user agent (device, browser, etc.) and time (hour, day of week) data are derived.
 
Users interact with the service rarely or via anon devices; their holiday needs change over time. Places change, get
 added, or changed over time.
 
#### User Profiles: Creation and Mapping

Contextual information: place endorsements, user location, time, device type, referral (where the user comes from). From
 these they create an n-dimensional space (e.g., for 3 contextual features, a cube). A contextual user profile is a space
 or cell in this space: when a user visits the service they need to be mapped to this space.
 
Profiles are discovered offline--assuming that users will give similar endorsements in similar situations. After clustering,
the endorsements are removed from profiles, so that new users can be matched into a region. They don't initally specify what
kind of clustering ('it depends on the type of application') although later use k-means with k=20.

Each cluster centroid has a weight vector that shows how strong a contextual category is associated with a cluster. The
 weights are then pruned; any weights < 0.2 are redistributed across the profiles (this is meant to act as a denoising step).

When a new user arrives, she/he is mapped to one of the existing profiles using a similarity metric; Euclidean distance is
 mentioned at the end of Section 5.

#### Destination Ranking

Destinations are ranked using Naive Bayes, i.e. P(A, E) = P(A) x P(E|A) for destination A and endorsement (search term) E.
 Adding further terms updates the score: P(A, E1, E2) = P(A) x P(E1|A) x P(E2|A).

### Evaluation

Experiments run only with 27k users via an online experiment. The main metrics were clicks-per-user (C/U) and click-through rate (CTR),
since the aim is to increase customer engagement/exploration.

Not many details about the baseline except that it is 'the same non-contextualized ranker.' I didn't spot details given about how long the A/B
 test was run for.

The contextual ranker did not significantly increase conversation compared to the baseline, but CTR and C/U increase by 20% and 23%.

### Conclusions

Interesting usage of tailoring results based on device, browser, and other "contextual" information. Very light on details, and jargon
 heavy--arguably more about search ranking rather than recommendation, as domain still requires input.
