---
layout: post
title:  "Mining large streams of user data for personalized recommendations"
tags: [recsys, paper]
excerpt_separator: <!--more-->
---

A review of a paper by Amatriain, from the ACM SIGKDD Explorations Newsletter Volume 14 Issue 2, December 2012. 

<!--more-->
<hr />

### Introduction

A good recommender system goes beyond the algorithm - user interaction, data source(s), what error metric is optimized etc. can have as much of an impact on the business.

The main approach to recommendation systems is collaborative filtering, governed by the principle of like mindedness. Main alternative is content-based recommendation, based on item-feature similarities. Both are often combined into hybrid approaches.

Recommendation is a particular application of data mining: (a) preprocessing, (b) modeling, (c) result analysis. This opens up the recommendation problem to the entire family of supervised/unsupervised learning algorithms.

### The Netflix Prize

$1M prize to improve Cinematch's RMSE score by 10%. The first progress prize was won with 107 algorithms and over 2000 hours of work (for 8.43% improvement). The two best performing algorithms were matrix factorization and restricted Boltzmann machines. 

Scalability of MF was taken from Funk's incremental, iterative approximate SVD. SVD++ provided a means of taking implicit and explicit feedback into account. Restricted Boltzmann machines are 4th-gen ANNs after (1) perceptrons, (2) back propagation, (3) belief networks; they can be stacked to form deep belief networks. Using temporal dynamics and blending MF/neighborhood models worked. Final prize solutions were ensembles.

Limitations: competition algorithms on 100M ratings, production requires 5B ratings; algorithms not built to adapt with new ratings. Diminishing accuracy gains did not justify implementing all the algorithms.

### Beyond Rating Prediction

#### Consumer Data Science

Consumer (Data) Science: (1) start with a hypothesis, (2) design a test, (3) execute a test, (4) let data speak for itself. While many metrics are tracked, ultimately trust member engagement (e.g. viewing hours) and retention.

How to integrate machine learning approaches into data-driven culture? Try to combine 'best of both worlds' off/online experimentation. Offline experimentation is only used as an indication to make informed decisions on follow up tests.

#### Everything is a Recommendation

Groups: how to select them, what is in them, what order items are shown (all is personalized). These are optimized for both accuracy and diversity (household recommendation problem).

Awareness: users should know how we are adapting to their tastes. This is also done by providing explanations.

#### Ranking

The goal is to find a ranking function that is better than item popularity. For example, items can be scored on a linear combination of its popularity and predicted rating for the target user. Transforms the problem into a learning to rank scenario.

#### Data

Ratings, popularity scores, popularity over time/groups, movie queues, search terms, item metadata, impression data, implicit interactions, social data, external data, demographics, location, language.

#### Models

Linear regression, logistic regression, elastic nets, SVD, RBMs, markov chains, LDA, association rules, MF, gradient boosted decision trees, random forests, k-means, affinity propagation.

### More data or better models?

High variance models - overfit with given data. High bias models - adding more data will not help. 

### Research Directions

1. Beyond explicit ratings: in most real-world situations, implicit and binary feedback is much more readily available and requires no extra effort on the user side. Approaches like Bayesian Personalized Ranking 
2. Personalized learning to rank: e.g. pairwise ranking, which optimizes a loss function defined on pairwise preferences from the user. Must use related rank metrics, MAP/NDCG/MRR/FCP.
3. Context-awareness. Location/time, pre/post filtering vs contextual modeling. For example, representing the user as a hierarchy of micro profiles. Makes the dimensionality of the problem larger, but can prove effective for the business.
4. Unbalanced classes. In implicit feedback scenarios, all data is either positive, presented but not chosen, and not presented. Positional bias = an item's chances of being picked are proportional to their ranking.
5. Social recommendation: mixed evidence as to whether it is better than 'traditional' collaborative filtering. However, can be used to deal with cold start, or to pick expert users.

